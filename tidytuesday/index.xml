<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tidytuesdays on Sara A. Stoudt</title>
    <link>/tidytuesday/</link>
    <description>Recent content in Tidytuesdays on Sara A. Stoudt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Tue, 24 Dec 2019 10:42:13 -0700</lastBuildDate>
    
	<atom:link href="/tidytuesday/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Week 52, 2019: Christmas Songs</title>
      <link>/tidytuesday/holiday-song-sentiment/</link>
      <pubDate>Tue, 24 Dec 2019 10:42:13 -0700</pubDate>
      
      <guid>/tidytuesday/holiday-song-sentiment/</guid>
      <description>setwd(&amp;quot;~/Desktop/tidytuesday/data/2019/2019-12-24&amp;quot;) library(readr) library(dplyr) library(ggplot2) library(sentimentr) library(kableExtra) library(spotifyr) library(genius) library(purrr) songs &amp;lt;- read.csv(&amp;quot;christmas_songs.csv&amp;quot;, stringsAsFactors = F) lyrics &amp;lt;- read_tsv(&amp;quot;christmas_lyrics.tsv&amp;quot;) Let’s see how basic sentiment analysis classifies these Christmas hits. Luckily, I already have this code ready to go from my R Ladies Lightning Talk.
allSentiment &amp;lt;- sentiment(lyrics$lyric) lyrics$id &amp;lt;- 1:nrow(lyrics) lyrics2 &amp;lt;- merge(allSentiment, lyrics, by.x = &amp;quot;element_id&amp;quot;, by.y = &amp;quot;id&amp;quot;) Top 5 Most Positive Songs (on average across lyric lines) tt &amp;lt;- lyrics2 %&amp;gt;% group_by(track_title) %&amp;gt;% summarise(meanSentiment = mean(sentiment)) %&amp;gt;% arrange(desc(meanSentiment)) %&amp;gt;% head(5) kable(tt) %&amp;gt;% kable_styling()   track_title  meanSentiment      Silent Night  0.</description>
    </item>
    
    <item>
      <title>Week 44, 2019: Squirrel Census</title>
      <link>/tidytuesday/squirrel-census/</link>
      <pubDate>Wed, 30 Oct 2019 10:42:13 -0700</pubDate>
      
      <guid>/tidytuesday/squirrel-census/</guid>
      <description>It has been a LONG time since I last participated in Tidy Tuesday. Apologies #rstats world! It turns out getting a PhD is… alot. But I obviously had to return for the Squirrel Census.
library(dplyr) library(stringr) library(kableExtra) library(ggplot2) library(purrr) library(magrittr) setwd(&amp;quot;~/Desktop/tidytuesday/data/2019/2019-10-29&amp;quot;) sq &amp;lt;- read.csv(&amp;quot;nyc_squirrels.csv&amp;quot;,stringsAsFactors = F) names(sq) ## [1] &amp;quot;long&amp;quot; ## [2] &amp;quot;lat&amp;quot; ## [3] &amp;quot;unique_squirrel_id&amp;quot; ## [4] &amp;quot;hectare&amp;quot; ## [5] &amp;quot;shift&amp;quot; ## [6] &amp;quot;date&amp;quot; ## [7] &amp;quot;hectare_squirrel_number&amp;quot; ## [8] &amp;quot;age&amp;quot; ## [9] &amp;quot;primary_fur_color&amp;quot; ## [10] &amp;quot;highlight_fur_color&amp;quot; ## [11] &amp;quot;combination_of_primary_and_highlight_color&amp;quot; ## [12] &amp;quot;color_notes&amp;quot; ## [13] &amp;quot;location&amp;quot; ## [14] &amp;quot;above_ground_sighter_measurement&amp;quot; ## [15] &amp;quot;specific_location&amp;quot; ## [16] &amp;quot;running&amp;quot; ## [17] &amp;quot;chasing&amp;quot; ## [18] &amp;quot;climbing&amp;quot; ## [19] &amp;quot;eating&amp;quot; ## [20] &amp;quot;foraging&amp;quot; ## [21] &amp;quot;other_activities&amp;quot; ## [22] &amp;quot;kuks&amp;quot; ## [23] &amp;quot;quaas&amp;quot; ## [24] &amp;quot;moans&amp;quot; ## [25] &amp;quot;tail_flags&amp;quot; ## [26] &amp;quot;tail_twitches&amp;quot; ## [27] &amp;quot;approaches&amp;quot; ## [28] &amp;quot;indifferent&amp;quot; ## [29] &amp;quot;runs_from&amp;quot; ## [30] &amp;quot;other_interactions&amp;quot; ## [31] &amp;quot;lat_long&amp;quot; ## [32] &amp;quot;zip_codes&amp;quot; ## [33] &amp;quot;community_districts&amp;quot; ## [34] &amp;quot;borough_boundaries&amp;quot; ## [35] &amp;quot;city_council_districts&amp;quot; ## [36] &amp;quot;police_precincts&amp;quot; What weird stuff can we find?</description>
    </item>
    
    <item>
      <title>Week 34: Thanksgiving Dinner</title>
      <link>/tidytuesday/week34/</link>
      <pubDate>Sun, 30 Dec 2018 10:42:13 -0700</pubDate>
      
      <guid>/tidytuesday/week34/</guid>
      <description>.superbigimage{ overflow-x:scroll; white-space: nowrap; } .superbigimage img{ max-width: none; } /* https://stackoverflow.com/questions/52448104/how-to-add-horizontal-scroll-bar-for-a-ggplot-plot-in-rmarkdown-html*/  This is super tardy for Thanksgiving, but since Christmas is around the corner, and often there is a similar food vibe, here we go anyway…
library(dplyr) library(ggplot2) library(forcats) library(stringr) library(tidyr) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-11-20&amp;quot;) tg= read.csv(&amp;quot;thanksgiving_meals.csv&amp;quot;) Who travels most?
Trying to break down by community type, age, and gender, leaves bins too sparse.
look = tg %&amp;gt;% filter(celebrate==&amp;quot;Yes&amp;quot;) %&amp;gt;% group_by(travel, community_type,age,gender) %&amp;gt;% summarise(count=n()) summary(look$count) ## too sparse ## Min.</description>
    </item>
    
    <item>
      <title>Week 32: US Wind Farm Locations</title>
      <link>/tidytuesday/week32/</link>
      <pubDate>Mon, 12 Nov 2018 10:42:13 -0700</pubDate>
      
      <guid>/tidytuesday/week32/</guid>
      <description>.superbigimage{ overflow-x:scroll; white-space: nowrap; } .superbigimage img{ max-width: none; } /* https://stackoverflow.com/questions/52448104/how-to-add-horizontal-scroll-bar-for-a-ggplot-plot-in-rmarkdown-html*/  Tardy as usual…
 library(tidyr) library(dplyr) library(ggplot2) library(geofacet) library(RColorBrewer) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-11-06&amp;quot;) wind= read.csv(&amp;quot;us_wind.csv&amp;quot;) Deal with the missing data: I used na_if for the first time here.
wind=wind %&amp;gt;% mutate(faa_ors=na_if(faa_ors,&amp;quot;missing&amp;quot;),faa_asn=na_if(faa_asn,&amp;quot;missing&amp;quot;),usgs_pr_id=na_if(usgs_pr_id,-9999), t_state=na_if(t_state,&amp;quot;missing&amp;quot;),t_county=na_if(t_county,&amp;quot;missing&amp;quot;),t_fips=na_if(t_fips,&amp;quot;missing&amp;quot;),p_name=na_if(p_name,&amp;quot;missing&amp;quot;),p_year=na_if(p_year,-9999), p_tnum=na_if(p_tnum,-9999), p_cap=na_if(p_cap,-9999), t_manu=na_if(t_manu,&amp;quot;missing&amp;quot;), t_model=na_if(t_model,&amp;quot;missing&amp;quot;), t_cap=na_if(t_cap,-9999),t_hh=na_if(t_hh,-9999), t_rd=na_if(t_rd,-9999), t_rsa=na_if(t_rsa,-9999),t_ttlh=na_if(t_ttlh,-9999),t_img_date=na_if(t_img_date,&amp;quot;missing&amp;quot;), t_img_srce=na_if(t_img_srce,&amp;quot;missing&amp;quot;)) When windmills became operational by state over time:
toP=wind %&amp;gt;% group_by(t_state,p_year) %&amp;gt;% summarise(count=n()) ggplot(toP,aes(p_year,count))+geom_point()+geom_line()+facet_geo(~t_state)  It’s too hard to see what is going on because of big windmill states like CA.</description>
    </item>
    
    <item>
      <title>Week 31: R and R package downloads</title>
      <link>/tidytuesday/week31/</link>
      <pubDate>Tue, 30 Oct 2018 10:42:13 -0700</pubDate>
      
      <guid>/tidytuesday/week31/</guid>
      <description>.superbigimage{ overflow-x:scroll; white-space: nowrap; } .superbigimage img{ max-width: none; } /* https://stackoverflow.com/questions/52448104/how-to-add-horizontal-scroll-bar-for-a-ggplot-plot-in-rmarkdown-html*/  library(ggplot2) library(dplyr) library(tidyr) library(forcats) library(cranlogs) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-10-30&amp;quot;) rd=read.csv(&amp;quot;r_downloads_year.csv&amp;quot;) rd$date=as.Date(as.character(rd$date)) First download of each R version rd %&amp;gt;% drop_na(os) %&amp;gt;% group_by(version,os) %&amp;gt;% summarise(first=min(date)) %&amp;gt;% ggplot(.,aes(fct_rev(version),as.Date(first),col=os))+ geom_point()+coord_flip()+xlab(&amp;quot;date of first download&amp;quot;)+ylab(&amp;quot;R version&amp;quot;)  Take-Aways
 Windows often lags. I suspect “devel” and “latest” are relative to the current version since they appear early on.   Tidyverse and its components tidyverse=cran_downloads(package=&amp;quot;tidyverse&amp;quot;,from=min(rd$date),to=max(rd$date)) ggpl=cran_downloads(package=&amp;quot;ggplot2&amp;quot;,from=min(rd$date),to=max(rd$date)) dp=cran_downloads(package=&amp;quot;dplyr&amp;quot;,from=min(rd$date),to=max(rd$date)) tid=cran_downloads(package=&amp;quot;tidyr&amp;quot;,from=min(rd$date),to=max(rd$date)) re=cran_downloads(package=&amp;quot;readr&amp;quot;,from=min(rd$date),to=max(rd$date)) pr=cran_downloads(package=&amp;quot;purrr&amp;quot;,from=min(rd$date),to=max(rd$date)) tib=cran_downloads(package=&amp;quot;tibble&amp;quot;,from=min(rd$date),to=max(rd$date)) st=cran_downloads(package=&amp;quot;stringr&amp;quot;,from=min(rd$date),to=max(rd$date)) fc=cran_downloads(package=&amp;quot;forcats&amp;quot;,from=min(rd$date),to=max(rd$date)) allTy=rbind.</description>
    </item>
    
    <item>
      <title>Week 23: Fast Food Calories</title>
      <link>/tidytuesday/week23/</link>
      <pubDate>Tue, 04 Sep 2018 10:42:13 -0700</pubDate>
      
      <guid>/tidytuesday/week23/</guid>
      <description>Data
Article
Data Source
require(ggplot2) require(dplyr) require(gridExtra) require(tidyr) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-09-04&amp;quot;) ff = read.csv(&amp;quot;fastfood_calories.csv&amp;quot;, stringsAsFactors = F) ff = ff[,-c(1, ncol(ff))] ## remove salad (all the same) and X I wanted a simple way to group items across restaurants, so I’m going to follow this example and use hierarchical clustering via the Levenshtein Distance (typically used for string distances).
d &amp;lt;- adist(ff$item) rownames(d) &amp;lt;- ff$Item hc &amp;lt;- hclust(as.dist(d)) df &amp;lt;- data.frame(ff,cut = cutree(hc, k = 10)) ## 10 is a totally arbitrary choice Let’s see what groups we end up with.</description>
    </item>
    
    <item>
      <title>Week 20: Russian Troll Tweets</title>
      <link>/tidytuesday/week20/</link>
      <pubDate>Tue, 14 Aug 2018 10:39:54 -0700</pubDate>
      
      <guid>/tidytuesday/week20/</guid>
      <description>require(purrr) require(dplyr) require(ggplot2) require(gridExtra) require(stringr) require(readr) require(lubridate) require(data.table) require(tidyr) This week’s Tidy Tuesday uses data from 538 that shows tweets from Russian trolls. Read more about the data here.
setwd(&amp;quot;~/Desktop/russian-troll-tweets&amp;quot;) files &amp;lt;- list.files() files &amp;lt;- files[grepl(&amp;quot;.csv&amp;quot;, files)] getData &amp;lt;- lapply(files,fread) tweet &amp;lt;- do.call(&amp;quot;rbind&amp;quot;,getData) This analysis was inspired by Jennifer Golbeck’s “Benford’s Law Applies to Online Social Networks”. Benford’s Law provides the expected frequency (non-uniform) of numbers’ first digits. In this paper she finds that both the number of followers and the number of following per user on Twitter follow Benford’s Law.</description>
    </item>
    
    <item>
      <title>Week 16: Exercise USA</title>
      <link>/tidytuesday/week16/</link>
      <pubDate>Tue, 17 Jul 2018 10:38:18 -0700</pubDate>
      
      <guid>/tidytuesday/week16/</guid>
      <description>Week 16 CDC
CDC - National Health Statistics Reports|
require(readxl) require(dplyr) require(ggplot2) require(stringr) require(tidyr) require(geofacet) require(viridis) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-07-17&amp;quot;) exercise=read_excel(&amp;quot;week16_exercise.xlsx&amp;quot;,sheet=1) exercise=exercise[,-1] ## remove count exercise=exercise[-1,] ## remove &amp;quot;all states&amp;quot;&amp;quot; exerciseT=exercise %&amp;gt;% gather(type, value,-state) exerciseT$value=as.numeric(exerciseT$value) ## Warning: NAs introduced by coercion You Better Work I expected that working men and women would have less time to exercise, but it looks like those who work meet the federal guidelines for exercise more across the US.</description>
    </item>
    
    <item>
      <title>Week 15: Craft Beer USA</title>
      <link>/tidytuesday/week15/</link>
      <pubDate>Tue, 10 Jul 2018 10:36:44 -0700</pubDate>
      
      <guid>/tidytuesday/week15/</guid>
      <description>Week 15 Craft Beer USA
data.world
thrillist.com
require(readxl) require(dplyr) require(ggplot2) require(stringr) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-07-10&amp;quot;) beers=read_excel(&amp;quot;week15_beers.xlsx&amp;quot;,sheet=1) brewer=read_excel(&amp;quot;week15_beers.xlsx&amp;quot;,sheet=2) beer=inner_join(beers,brewer,by =c(&amp;quot;brewery_id&amp;quot;=&amp;quot;id&amp;quot;)) byState=beer %&amp;gt;% group_by(state) %&amp;gt;% summarise(numBrewer=length(unique(brewery_id)),count=n(),mabv=mean(abv,na.rm=T)) counties= map_data(&amp;quot;county&amp;quot;) state=map_data(&amp;quot;state&amp;quot;) stateInfo=cbind.data.frame(abb=state.abb,name=tolower(state.name)) state=inner_join(state,stateInfo,by=c(&amp;quot;region&amp;quot;=&amp;quot;name&amp;quot;)) ## Warning: Column `region`/`name` joining character vector and factor, ## coercing into character vector all_state=inner_join(state,byState,by=c(&amp;quot;abb&amp;quot;=&amp;quot;state&amp;quot;)) ## Warning: Column `abb`/`state` joining factor and character vector, coercing ## into character vector This palette isn’t very visually appealing, but in the spirit of beer, I’ll use it anyway.</description>
    </item>
    
    <item>
      <title>Week 10: Biketown Bikeshare</title>
      <link>/tidytuesday/week10/</link>
      <pubDate>Tue, 05 Jun 2018 10:24:24 -0700</pubDate>
      
      <guid>/tidytuesday/week10/</guid>
      <description>require(dplyr) require(readr) require(lubridate) require(ggplot2) Tidy the Raw Data. Luckily, each file has the same header, so we can easily stack them.
setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-06-05/PublicTripData&amp;quot;) toRead=list.files() individ=lapply(toRead,read.csv) full=do.call(&amp;quot;rbind&amp;quot;,individ) head(full,1) ## RouteID PaymentPlan StartHub StartLatitude StartLongitude ## 1 1282087 Casual NE Sandy at 16th 45.52441 -122.6498 ## StartDate StartTime EndHub EndLatitude EndLongitude EndDate EndTime ## 1 7/19/2016 10:22 45.53506 -122.6546 7/19/2016 10:48 ## TripType BikeID BikeName Distance_Miles Duration RentalAccessPath ## 1 6083 0468 BIKETOWN 1.</description>
    </item>
    
    <item>
      <title>Week 9: Comic Book Characters</title>
      <link>/tidytuesday/week9/</link>
      <pubDate>Tue, 29 May 2018 10:22:43 -0700</pubDate>
      
      <guid>/tidytuesday/week9/</guid>
      <description>Data: Comic book characters
Data Source: FiveThirtyEight package
Article: FiveThirtyEight.com
require(dplyr) require(ggplot2) require(tidyr) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-05-29/&amp;quot;) cb=read.csv(&amp;quot;week9_comic_characters.csv&amp;quot;) Names: Boy v. Man, Girl v. Woman cb$isBoy=unlist(lapply(cb$name,function(x){grepl(&amp;quot;boy\\&amp;gt;&amp;quot;,x,ignore.case=T)})) ## nothing after boy cb$isGirl=unlist(lapply(cb$name,function(x){grepl(&amp;quot;girl&amp;quot;,x,ignore.case=T)}))## cb$isMan=unlist(lapply(cb$name,function(x){grepl(&amp;quot;man\\&amp;gt;&amp;quot;,x,ignore.case=T)})) ## nothing after man cb$isWoman=unlist(lapply(cb$name,function(x){grepl(&amp;quot;woman&amp;quot;,x,ignore.case=T)}))## cb$isMan[which(cb$isMan==1 &amp;amp; cb$isWoman==1)]=0 ## don&amp;#39;t want to double count woman byYear=cb %&amp;gt;% group_by(year)%&amp;gt;%summarise(isGirl=sum(isGirl),count=n(),isWoman=sum(isWoman),isBoy=sum(isBoy),isMan=sum(isMan)) %&amp;gt;% mutate(percentG=isGirl/count,percentW=isWoman/count) Tangent: Just for the record: characters identified as another’s girlfriend exist, but no boyfriends.
gf=cb[which(unlist(lapply(cb$name,function(x){grepl(&amp;quot;girlfriend&amp;quot;,x,ignore.case=T)}))==T),] gf$name ## [1] Ruby (Thug&amp;#39;s girlfriend) (Earth-616) ## [2] Annie (Noh-Varr&amp;#39;s Girlfriend) (Earth-616) ## [3] Karen (Hijack&amp;#39;s girlfriend) (Earth-616) ## 23272 Levels: &amp;#39;Spinner (Earth-616) .</description>
    </item>
    
    <item>
      <title>Week 8: US Honey Production</title>
      <link>/tidytuesday/week8/</link>
      <pubDate>Mon, 21 May 2018 10:06:52 -0700</pubDate>
      
      <guid>/tidytuesday/week8/</guid>
      <description>Data: US Honey Production
Data Source: USDA
Data Source:Kaggle.com
Article: Bee Culture
Find my cleaning process for going from the three raw data files to my clean version here.
It’s a bit like… require(dplyr) require(ggplot2) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-05-21/week8_honey_production&amp;quot;) honey=read.csv(&amp;quot;honeyDataNice.csv&amp;quot;,stringsAsFactors=F) names(honey) ## [1] &amp;quot;state&amp;quot; &amp;quot;numColonies&amp;quot; &amp;quot;yieldPerColony&amp;quot; &amp;quot;production&amp;quot; ## [5] &amp;quot;stocks&amp;quot; &amp;quot;avgPricePerLb&amp;quot; &amp;quot;valProd&amp;quot; &amp;quot;year&amp;quot; By Year byYear=honey %&amp;gt;% group_by(year)%&amp;gt;% summarise(numColoniesT=sum(numColonies),productionT=sum(production),avgPrice=mean(avgPricePerLb),sdPrice=sd(avgPricePerLb),avgYieldPerCol=mean(yieldPerColony),sdYieldPerCol=sd(yieldPerColony),mnumColonies=mean(numColonies),mproduction=mean(production),sdnumColonies=sd(numColonies),sdproduction=sd(production)) Supply and Demand
ggplot(byYear,aes(x=year,y=mnumColonies))+geom_point() ## no real difference, scale is narrow ggplot(byYear,aes(x=year,y=sdnumColonies))+geom_point() ## increasing variability ggplot(byYear,aes(x=year,y=mproduction))+geom_point() ## decline ggplot(byYear,aes(x=year,y=sdproduction))+geom_point() ## decline ggplot(byYear,aes(x=year,avgPrice))+geom_point() ## increase ggplot(byYear,aes(x=year,sdPrice))+geom_point() ## increase Efficiency</description>
    </item>
    
    <item>
      <title>Week 7: Star Wars Survey</title>
      <link>/tidytuesday/week7/</link>
      <pubDate>Mon, 14 May 2018 10:05:21 -0700</pubDate>
      
      <guid>/tidytuesday/week7/</guid>
      <description>Week 7 - Star Wars Survey (2014) RAW DATA
Article
DataSource fivethirtyeight (fivethirtyeight package)
 How do perceptions of female Star Wars characters differ across age and gender? require(data.table) require(dplyr) require(ggplot2) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-05-14&amp;quot;) sw=fread(&amp;quot;week7_starwars.csv&amp;quot;) ## read.csv didn&amp;#39;t work for me Brute force manipulation.
realHeader=sw[1,] sw=sw[-1,] names(sw)[c(36,38)]=c(&amp;quot;householdIncome&amp;quot;,&amp;quot;location&amp;quot;) names(sw)[2]=&amp;quot;seenStarWars&amp;quot; Let’s focus on those who have actually seen Star Wars.
swYes=subset(sw,seenStarWars==&amp;quot;Yes&amp;quot;) Padme ## complete data only toPlot=swYes[-which(swYes$Gender==&amp;quot;&amp;quot;),c(&amp;quot;V28&amp;quot;,&amp;quot;Gender&amp;quot;,&amp;quot;Age&amp;quot;)] toPlot=toPlot[-which(toPlot$V28==&amp;quot;&amp;quot;),] toPlot$V28=factor(toPlot$V28) toPlot$V28=factor(toPlot$V28,levels=levels(toPlot$V28)[c(4,6,3,1,2,5)]) ## GROSS! byCatGen=toPlot%&amp;gt;%group_by(V28,Gender)%&amp;gt;%summarise(count=n()) byGen=toPlot%&amp;gt;%group_by(Gender)%&amp;gt;%summarise(count=n()) toPlot=byCatGen%&amp;gt;% inner_join(byGen,by=c(&amp;quot;Gender&amp;quot;=&amp;quot;Gender&amp;quot;))%&amp;gt;%mutate(percent=count.</description>
    </item>
    
    <item>
      <title>Week 6: Global Coffee Chains</title>
      <link>/tidytuesday/week6/</link>
      <pubDate>Mon, 07 May 2018 09:53:05 -0700</pubDate>
      
      <guid>/tidytuesday/week6/</guid>
      <description>Fueled By Dunkin Week 6 - Global coffee-chain locations (as of 2017 or 2018) RAW DATA
Article
DataSource (Starbucks): kaggle.com
DataSource (Tim Horton): timhortons.com
DataSource (Dunkin Donuts): odditysoftware.com
require(readxl) require(dplyr) require(maps) require(ggmap) require(fields) require(sf) setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-05-07&amp;quot;) coffee1=read_excel(&amp;quot;week6_coffee_chains.xlsx&amp;quot;,sheet=1) coffee2=read_excel(&amp;quot;week6_coffee_chains.xlsx&amp;quot;,sheet=2) coffee3=read_excel(&amp;quot;week6_coffee_chains.xlsx&amp;quot;,sheet=3) ## all have different columns starbucksUS=subset(coffee1,Country==&amp;quot;US&amp;quot;) dunkinUS=subset(coffee3,e_country==&amp;quot;USA&amp;quot;) ## all in US so not really needed How far do I need to walk/drive to get my fix? I’m putting my personal biases into this, so I“m only looking at Dunkin and Starbucks within the US.</description>
    </item>
    
    <item>
      <title>Week 5: ACS Census Data (2015)</title>
      <link>/tidytuesday/week5/</link>
      <pubDate>Mon, 30 Apr 2018 09:45:50 -0700</pubDate>
      
      <guid>/tidytuesday/week5/</guid>
      <description>Setup require(ggplot2) require(maps) require(dplyr) require(plotly) require(spotifyr)  Week 5 - County-level American Community Survey (5-year estimates) 2015 RAW DATA
DataSource: census.gov
Kaggle source
This week I am taking inspiration from the Tidy Tuesday submissions of @AidoBo and @jakekaupp.
I’m slightly tweaking @AidoBo’s function to plot continuous variables on a map to help me explore.
For #TidyTuesday I created simple function which allows you to plot any continuous variable in the data on a map #rstats #r4ds pic.</description>
    </item>
    
    <item>
      <title>Week 4: Australian Salaries by Gender</title>
      <link>/tidytuesday/week4/</link>
      <pubDate>Tue, 24 Apr 2018 09:40:44 -0700</pubDate>
      
      <guid>/tidytuesday/week4/</guid>
      <description>Week 4 - Gender differences in Australian Average Taxable Income RAW DATA
Article
DataSource: data.gov.au
Disparities in STEM Take-aways
About equal number of indivuals in scientist jobs. Many more males in engineering jobs.  (to be fair, should look into proportion of work force)
Rough OLS interpretation: For every dollar a woman makes in science, a man makes $1.52. Rough OLS interpretation: For every dollar a woman makes in engineering, a man makes $1.</description>
    </item>
    
    <item>
      <title>Week 3: Global Mortality</title>
      <link>/tidytuesday/week3/</link>
      <pubDate>Mon, 16 Apr 2018 09:38:17 -0700</pubDate>
      
      <guid>/tidytuesday/week3/</guid>
      <description>Setup require(readxl) require(dplyr) require(ggplot2) require(gridExtra) require(tidyr) require(RColorBrewer)  Week 3 - Global causes of mortality RAW DATA
Article
DatSource: ourworldindata.org
Original Graphic
 Read and Clean Data setwd(&amp;quot;~/Desktop/tidytuesday/data/2018-04-16&amp;quot;) gm=read_excel(&amp;quot;global_mortality.xlsx&amp;quot;) gm.gathered=gather(gm,cause,percent,-country,-country_code,-year) ## want a single column for cause of death gm.gathered$cause=as.vector(gsub(&amp;quot; \\(\\%\\)&amp;quot;,&amp;quot;&amp;quot;,gm.gathered$cause)) ## remove (%) in causes of death  Get Colors Ready I will want the color per cause to be the same across plots.
The colors I use are still not perfectly distinguishable.</description>
    </item>
    
  </channel>
</rss>