---
layout: default
title: Research
---

		<div class="about">
			<h1>Research</h1>


<h1>Sara Stoudt - Research</h1>

<h2>Graduate Research (ongoing)</h2> 
<h3> Identifiability in Species Distribution Modeling</h3>
<p><i>Advisors: Will Fithian and Perry de Valpine </i></p>



<ul>
</ul>

<h2>DS421 Research </h2> 

<h3> Streamlining Climate Model Accessibility for Integration into Site-Specific Climate Research</h3>
<p><i>Project for Capstone Project, with Jenna Baughman</i></p>

Climate change is tracked and measured on the global scale, making it hard to incorporate for more localized analysis. There are many different climate products that give different predictions for future climate under different scenarios. This data is stored at the global level per time step, so if you are interested in a time series for one location, you would have to download and process data for the whole globe at each time step, taking computational time and space that may be prohibitive. We aim to streamline this process by allowing users to explore different climate products before deciding on the ones of interest, and then helping users access the time series in their location of interest more easily.

See our work <a href="https://github.com/sastoudt/UCB_DS421_NEX_partnerProject"> here</a>.



<h3> US Drought Vulnerability</h3>
<p><i>Project for Class Taught by Gaston Sanchez, with Daniel Blaustein-Rejto, Ian Bolliger, Hal Gordon, Andrew Hultgren, Yang Ju, and Kate Pennington </i></p>

Our goal was to assess variation in vulnerability to drought across counties and census blocks in the continental United States using health, social, and agricultural indicators.

See our work <a href="https://github.com/bolliger32/US_drought_vulnerability"> here</a> and check out our Shiny app <a href="https://sastoudt.shinyapps.io/US_Drought_Vulnerability/"> here </a>.


<h3>Generalized Additive Models (GAMs) for Understanding Chlorophyll in the Bay Delta, Comparison of GAMs and Weighted Regression </h3>
<p><i>Advisors: Perry de Valpine, David Senn, Erica Spotswood, Collaborator (comparisons) Marcus Beck</p></i>

This was a Summer '16 project in collaboration with the San Francisco Estuary Institute. Track my progress <a href="https://github.com/sastoudt/DS421_summerProject" > here</a>. A Shiny application for the GAM/weighted regression portion can be found <a href="https://sastoudt.shinyapp.io/GAM_Delta" > here </a>.

</ul>

<h2>NIST Research</h2>


<h3> Implementations for Easy Use by Scientists: Shiny Applications</h3>
<p><i>Advisors: Antonio Possolo and Tom Bartel (EIV), Collaborator (nano) Bryan Jimenez</i></p>

I built Shiny applications (web applications based in R) to implement the Errors in Variables force calibrations calculations and calculations for the size measurement of nanoparticles. Both allow scientists to harness the power of R (as opposed to something like Excel) to analyze their data without having to learn R themselves. I also worked to finesse the optimization procedure for the EIV approach. This is challenging because we often are pushing the boundaries of how many parameters we need to estimate given the number of data points.

<h3>Errors in Variables Modeling in Force Calibrations</h3>
<p><i>Advisors: Antonio Possolo and Tom Bartel</i></p>

I worked on an alternative method for determining the calibration function for force measuring instruments as well as a Monte Carlo uncertainty evaluation for the calibration function. I implemented these methods and am helping to integrate them into the procedure employed at NIST for force calibration reporting.  Force measuring instruments are calibrated by recording the readings of each instrument when "known" masses are applied. I use an errors in variables regression method instead of the ordinary least squares method in order to take into consideration the uncertainty in the forces applied. The Monte Carlo method gives a more accurate representation of the calibration uncertainty throughout the transducerâ€™s force range than a single conservative value given by the current approach. 
Read our paper <a href="http://iopscience.iop.org/article/10.1088/0026-1394/53/3/965/meta;jsessionid=2B7013C6CBEADB69E9A44542A78B8E5B.ip-10-40-2-73" > here </a>. 

<h3>Homogenization of Surface Temperature Records</h3>
<p><i>Advisors: Antonio Possolo and Adam Pintar</i></p>
I worked on improving and making available a homogenization algorithm developed by NIST statisticians. The process of homogenization finds and adjusts for biases unrelated to the climate in temperature records. I worked to improve the uncertainty quantification of the algorithm and to provide a way for the method to handle missing values. I attended the Statistical and Applied Mathematical Sciences Institute's workshop on international surface temperatures to work on this homogenization algorithm. I am now working with another attendee to create an R package that includes the homogenization algorithm as well as resources for accessing and formatting portions of interest in the International Surface Temperature Initiative databank for use with the algorithm.


<h3>Measuring Optical Apertures for Solar Irradiance Monitoring</h3>
<p><i>Advisors: Antonio Possolo and Maritoni Litorja</i></p>

I worked to improve the preciseness and accuracy of aperture measurements used in solar irradiance monitoring. My job was to check for possible biases in the data collection and analysis as well as test different methods and assess their degree of uncertainty. I implemented and compared various algorithms for fitting circles on both simulated data and data collected from my proposed sampling method experiments. My goal was to see which combination of algorithm and sampling method yielded the least uncertainty. I found a combination of fitting algorithm and sampling method that was more accurate than the pair that scientists at NIST were using. My recommendations are currently being implemented at NIST.

<h2>Undergraduate Research</h2>
<h3>  Geostatistical Models for the Spatial Variability of the Abundance of Uranium in Soils and Sediments of the Continental United States  </h3>
<p><i> <a href="https://dspace.smith.edu/handle/11020/24600">Undergraduate Honors Thesis </a>, Advisor: Ben Baumer</i></p>

In my thesis I compared several different models for the spatial distribution of the mass fraction of uranium in soils and sediments across the continental United States, aiming to identify the model that predicts this quantity with smallest uncertainty. I am explored local regression, generalized additive models, and Gaussian processes to interpolate maps of uranium and used cross validation to pick the model that predicts uranium both accurately and precisely. I made interpolated maps and maps characterizing the uncertainty of the estimates that are compatible with Google Earth so that a user can interact with the data, "flying over" the US or zooming in to a region of interest.
<br>
Read an extended abstract <a href="http://www.geocomputation.org/2015/papers/GC15_78.pdf"> here </a>.



<h3>Modeling Internet Traffic Generation for Telecommunication Applications </h3>
<p><i>Industrial Careers in Mathematical Sciences Program (PIC Math) 

<br> With: Pamela Badian-Pessot, Vera Bao, Erika Earley, Yadira Flores, Liza Maharjan,

Blanche Ngo Mahop, Jordan Menter, Van Nguyen, Laura Rosenbauer, Danielle Williams, and Weijia Zhang
<br> Advisors: Nessy Tania & Veena Mendiratta (Bell Labs)</i></p>

The goal of this project was to develop a stochastic model that could predict and simulate traffic load on an internet network. The model takes as input number of users and proportion of internet application being used at a given time - namely web surfing, video  streaming, online gaming - and outputs simulated traffic data. Using individual user data, we produced models for web surfing, video streaming, and gaming which were combined to form the simulator. The first method fit known theoretical distributions to the data to simulate individual packets; the second used an empirical copula to simulate packets per second. Read our paper <a href="http://www.ajuronline.org/uploads/Volume_13_3/AJUR%20Vol%2013%20Issue%203%2008.25.16%20with%20covers%20interactive.pdf"> here</a>.

<h3>March Machine Learning Madness </h3>
<p><i>With: Lauren Santana, Advisor: Ben Baumer</i></p>

In pursuit of the perfect March Madness bracket we aimed to determine the most effective model and the most relevant data to predict match-up probabilities. We decided to use an ensemble method of machine learning techniques. The predictions made by a support vector machine, a Naive Bayes classifier, a k nearest neighbors method, a decision tree, random forests, and an artificial neural network were combined using logistic regression to determine final matchup probabilities. These machine learning techniques were trained on historical team and tournament data. We tested the performance of our method in the two stages of Kaggle's March Machine Learning Madness competition. After the 2014 tournament, we assessed our performance using a generalizable simulation-based technique. 

<br>
Read more <a href="http://www.math.smith.edu/~bbaumer/pub/jsm2014_perfect_bracket.pdf"> here </a>.

<h3>Roadless America: An Activity for Introduction to Sample Proportions and Their Confidence Intervals </h3>
<p><i>With: Yue Cao and Dana Udwin, Advisor: Nicholas J. Horton</i></p>


 Our goal was to have an accessible classroom activity that uses random sampling of locations in coordination with Google Maps to determine what proportion of the United States is within one mile of a road and visualize where roadless areas tend to be. The challenge was to implement a sophisticated geographic sampling that could be kept invisible to the user so that students could focus on the big picture ideas. My job involved brainstorming ways to appropriately collect and display this data as well as conveying these complex ideas in a more straightforward way to both students, who were seeing the material for the first time, and to instructors, who were seeing this technology for the first time. We worked on different sets of code and accompanying instructions for different thresholds of experience with the technology. We tested our most simplistic activity on an introductory statistics class at Smith, and used the feedback from the experience to motivate supplementary versions. 
<br>
You can find the classroom activity <a href="https://www.amstat.org/education/stew/pdfs/PercentWithinMileofRoad.pdf"> here </a>.

<h3>Factors Associated With Changes in Academic Performance During the Transition from Elementary to Middle School  </h3>
<p><i>With: Dana Hsu and Anna Rockower , Advisor: Katherine Halvorsen</i></p>

 We were asked by a local school district to look at student, teacher, and school variables to try to explain the drop in standardized test scores on the math section between 5th and 6th grade using de-identified data. My job was to clean the data as well as perform univariate and bivariate analysis to see what associations occur with the change in standardized testing score between 5th and 6th grade. I then worked to model the change in scores. 





<!--
			<ul class="contacts">
				<li><a href="#"></a></li>
				<li><a href="#"></a></li>
				<li><a href="#"></a></li>
				</ul>
		</div>
-->