[
  {
    "path": "posts/2021-06-05-learnr-tutorials-intro-stat/",
    "title": "Using learnr Tutorials in an Intro Stats Class as Pre-Labs",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\nTLDR\nI adapted the intro stat labs from Open Intro (with randomization and simulation) into learnr tutorials and had students complete them prior to coming to lab. This alleviated many pain points for both me and my students. 10/10 would do again. (Now skip to the “Resources” section for the key links.)\nWhat I did in the fall\nIn the fall, I taught intro stats remotely to about 45 students. In addition to the lecture, these students were split across 2 labs that met for 1 hour and 15 minutes each once a week. Students worked in pairs to complete a lab report syncronously (with a few exceptions).\nI tweaked slightly and rearranged the labs that come with Open Intro (with randomization and simulation), but I leaned pretty hard on these pre-existing materials because this was my first time teaching this course. Thank you Open Intro!\nThis intro stats class also had a final data analysis project. In consultation with Will Hopper and Scott LaCombe who were also teaching intro stat that semester, we narrowed the scope of this project a bit given the pandemic circumstances. In teams of three, students were asked to explore a dataset chosen from a pre-established list of datasets that Will, Scott, and I compiled. We let students find their own data if they wished, but most of my students took me up on the offer to streamline the searching process. After they individually explored the data, they were to come together and pick two quantitative variables and one qualitative variable to further investigate.\nAs the semester went on and they learned about linear models, one member of the team was to fit two univariate models (quant1 ~ quant2, quant1 ~ qual), one member was to fit the parallel slopes model (quant1 ~ quant2 + qual), and one member was to fit the interaction model (quant1 ~ quant2 * qual). Each team would then pick a final “best” model and write a report on the model’s findings while defending their model choice.\nPainpoints\nThe labs brought me and my students plenty of angst. The main issue was that the lab assignments ended up being too long to get through in one lab period, at least in the remote conditions. Different lab duos were placed in different breakout rooms, and I’d jump around answering questions. Students often ended up waiting on me to debug a problem which stalled their progress.\nAs a consequence of the time pressure, I suspected that students weren’t always reading through the whole lab document that explained new concepts or R functions and were skipping around to the exercises. This meant they weren’t getting the full explanation of new ideas and code but were “plugging and chugging”. Not ideal!\nAs an aside, some of the Shiny elements broke for certain students, and then they couldn’t easily move past this. This was a me problem, not an Open Intro problem, but I note it because it was hard to bounce back from quickly in the remote format (and learnr has Shiny capabilities too!).\nSince the logistics of getting students to work together outside of lab to finish up each lab was daunting due to different time zones, I ended up cutting a lot of the more critical thinking, open-ended questions that came at the end of the labs. However, these were often the questions touching on the content I most wanted to evaluate. Even when I shortened the lab each week, the lab reports took me a long time to grade, and I was mostly looking at code chunks, not written text.\nTo make sure students were making progress on their projects and that they could get feedback from me along the way, I had several project checkpoints throughout the semester. These were in addition to a weekly lab and homework, which I recognize was a lot. These had to be reviewed quickly because students needed my feedback before going on to the next stage. If they came in late due to the added flexibility of remote learning, the time crunch for me was even worse.\nlearnr prep\nI was teaching intro stats again in the spring, and I knew some things had to change.\nI had been to Mine Çetinkaya-Rundel and Colin Rundel’s workshop on building interactive tutorials in R which taught me all about learnr, gradethis, and learnrhash and seen Mine’s rstudio::global talk about feedback at scale. I had also talked to Marney Pratt about using interactive tutorials in the classroom since she had been created and used swirl tutorials before. In the interterm, I decided to try to use learnr to fix some of my lab painpoints, and Marney worked on some learnr tutorials for her biology students. Having an accountability and troubleshooting buddy was REALLY helpful.\nWait, what are all of these packages?\nDefinitely go through the materials mentioned above for more details but here is the gist.\nThe learnr package allows you to make interactive tutorials that students can step through, at their own pace. These tutorials are made up of a combination of text, code chunks that can run as they are, and code chunks to be completed and run by students. These tutorials pop up as a stand-alone window when launched from RStudio and can be viewed in the browser.\nThe gradethis package provides some automatic feedback as it checks students’ answers in real time. You can use multiple choice questions and even check code or code output. Sometimes the automatic feedback really is enough to go on and get students back on the right track (a nudge, if you will). Sometimes the feedback, like R error messages, is a bit cryptic. When students are correct, they get some automatic praise (thanks praise package!).\nThe learnrhash package keeps track of which chunks are engaged with by the students as they work through the tutorial. Magic happens at the end of a tutorial where a hash is produced and students just have to copy and paste it into a Google Form for me to grab later. The learnrhash package then has functions to “decode” this garbled mess for the instructor as well.\nGo on with the prep…\nI took each of my labs from the fall and turned them into their own learnr tutorial. The main text could just be copied over. All I had to do was rewrite questions into those that could be auto-graded by gradethis (typically multiple choice and fill in the blank style code chunks) and transfer example code chunks into the right chunk format expected so that they would become runnable within the tutorial. The addins included in the gradethis package were really helfpul for this.\nI made a package, to be installed from GitHub, to distribute the tutorials to my students. Thanks to Desirée De Leon for writing a clear and comprehensive blog post to walk me through that process.\nWhat I did in the spring\nI used the learnr tutorials as a pre-lab, to be completed individually before each lab. I had students submit their hash from learnrhash to make sure that students were at the bare-minimum running each code chunk, another nudge. A completion grade based on this provided just enough accountability to ensure everyone came to lab with at least some familiarity with the material in the tutorial.\nHowever, I couldn’t rely soley on learnr tutorials because a major goal of the course is to teach students how to work reproducibly in an R Markdown document. Therefore, I had students create a lab report during lab each week with their answers to the more open-ended questions. These questions focused on the interpretation, communication, and deeper understanding that is hard to auto-check.\nAs the semester went on, students started to work in their project teams during lab. I turned the project checkpoints into lab report questions. For example, for the simple linear regression lab, the lab report questions had each team fit the two univariate regression models for their project dataset instead of a dataset common to the whole class. I was not able to fit the initial exploration into a lab session, but I did want students to explore on their own before consulting with their group so there wouldn’t be too much mind-meld too early on in the process of choosing a question to investigate, so this ended up working out.\nComparing to the fall\nThere were a lot of wins.\nStudents could learn the new material at their own pace and all come in with the same background to approach the lab report questions. This to some extent ameliorated the different backgrounds of randomly assigned lab trios where some had seen R before and others hadn’t.\nLab report questions were the open-ended, writing-focused prompts that I really wanted to evaluate. Grading was SO MUCH FASTER, and the students could get more meaningful feedback from me on their reasoning rather than me getting bogged down checking their code for mistakes.\nAnecdotally, students seemed to finish lab questions during lab, although many took some time to polish their reports afterwards. I didn’t have to cut any problems. Hooray!\nDuring lab I still got questions on code, but students didn’t get as hung up as before and had the tutorials to fall back on while they were waiting for me or a TA to answer their questions.\nSome of the weeds\nThere were also some hiccups.\nInstall day was gnarly, but honestly it always is with just R and RStudio alone. However, because I was leveraging capabilities from development versions of gradethis and learnrhash, things were slightlly more complicated. Plus, I didn’t leave myself a lot of time to hard-core user test beyond me and Marney before the semester started.\nStudents had to install my package on their own computers, not on Smith’s RStudio server. Again, this was more on me not giving enough lead time to really get that working well on the whole server system. We were able to get the package installed on individual student’s server accounts on a case by case basis if using RStudio on a personal laptop wasn’t feasible.\nOccassionally students would find a bug or have trouble submitting the Google Form. To avoid the need to reinstall the package just to fix the bug, I just noted them to fix at the end of the semester and had students not worry about that question/part that was glitchy. Since the submission was really just an accountability check, I often just accepted a screenshot of the hash in a pinch.\nThe tutorials do not reliably save a student’s progress, so the tutorial would need to be done in one sitting. This was not ideal. You might consider breaking your tutorials into smaller chunks to get around this, or at least give students an estimated time for completion so that they could better plan. Again, user-testing would have been helpful here.\nMy materials\nFeel free to adapt my materials for your own use. I would just love to hear how it goes for you.\nmy package nudgeStatLabs\nmy “autograde” gist Note: This could get much fancier. I still did a lot of things manually, but there is room for more automation (e.g. using the googlesheets package to read the data automatically, writing tests to make sure number of chunks run v. total is equal for each student instead of a visual inspection)\nYou’ll note that there are two extra tutorials beyond the Open Intro derived labs. One provides data collection for the “Roadless America” activity, and one provides a brief glimpse of scenarios when the bootstrap fails.\nWhat you would need to change when adapting these to your own setting\nCreate your own GitHub repo. If you want to start with mine, you can fork it and go from there.\nYou will need to create your own Google Form for collecting the hashes and replace my link with yours in the tutorials. You can restrict submission to those with your institiution mailing domain, which is handy. Note some students had problems seeing the Google Form embedded in the tutorial, so I gave them the stand-alone link for reference as well.\nYou will likely want to update the lab questions, especially those in the back half of the tutorials as mine are tailored to our projects.\nYou might want to refine the questions in the pre-lab and/or decide not to give the solution and instead provide more hints.\nI am on the learnr bandwagon!\nIf I were to teach intro stats in R again, I would definitely use this learnr setup. I would probably spend a little time revising the questions and hints though. I’m happy to chat if anything above was unclear or if you have any questions.\nHowever, my next learnr frontier is to think about how these kinds of tutorials can be used in upper-level classes, most immediately mathematical statistics/statistical inference/whatever you call that class at your institution.\nI have some really half-baked ideas, but I’m thinking broadly about investigation via simulation studies to pair with the more theoretical proofs and derivations. This is in the spirit of an approach described in Chelsea Parlett-Pelleriti’s rstudio::global talk.\nAre you teaching math-stat in the fall and want to think with me about this over the summer? Let me know!\nResources\nMine Çetinkaya-Rundel and Colin Rundel’s workshop on building interactive tutorials in R\nMine’s rstudio::global talk\nDesirée De Leon blog post on making a package out of tutorials.\nmy package nudgeStatLabs\nmy “autograde” gist\nShoutouts\nThanks to Marney for being my learnr buddy, Mine, Colin, and Desirée for their materials, Scott and Will for intro stat project solidarity, my students for bearing with the ups and downs in both semesters, and my spring lab assistants Audrey and Amrita who were a HUGE help.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:21:26-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "I'm remaking my website, trying out distill.",
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2021-06-05",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T13:26:01-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-parallelization-pitfalls/",
    "title": "Parallelization Pitfalls",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\nJust run it in parallel, they said. It’ll be easy, they said.\n\nIn reality, I ran into many pitfalls as I tried to setup my simulation study on the department cluster. This blog post will outline my missteps and lessons learned in hopes that it will save someone else some time. Some of the information, but not all, is specific to Berkeley’s computing setup.\n\nMotivating Problem\nI have a simulation study that requires fitting models of different complexity to a variety of scenarios. Each scenario is replicated many times. It looks a bit like this (go ahead, judge that gnarly nested loop).\n\n\n\nI have too many scenarios and each model fit takes too long to run this overnight on my laptop which is my usual M.O. Plus, my computer fan isn’t super conducive to a good night’s sleep (#StudioApartmentLife).\nOrganizing files\nI realized I had to run this on the statistics department cluster which means I had to get my input files organized, my R script cleaned of any absolute paths, figure out how to get output files back to my personal computer, and write a bash script to launch the R script on the cluster.\nTo transfer files, I use sftp and put in the terminal. More info on file transfer here.\n\n\n\nFor Berkeley people, you can find the appropriate computername here. Also, after running the sftp line you’ll have to enter your password, and FYI, as you type, it won’t show up on the command line.\nLater on, when I want to get my output files back to my computer, I’ll have to use get in the terminal.\n\n\n\nNote, I named all of my results files with this beginning prefix results_ so that I could use the * to just get them all at once without typing a bunch.\nMaking it happen\nHere is a sample bash script that is named r_scenario1.sh\n\n\n\nThe first cp line moves the input data to the cluster.\nThe R CMD BATCH line runs the R script on the cluster and saves anything that is written to the console in the .Rout file. This will be useful to help see why things don’t work.\nThe last cp copies all of the results files (I used csv files) off of the cluster back to my own directory on the school machine.\nTo launch this bash script I first ssh into a school computer using the terminal (more info here):\n\n\n\n(Again you’ll have to enter your password.)\nThen I run the following line in the terminal:\n\n\n\nTo see what is happening on the cluster, run this in the terminal:\n\n\n\nFind more information about how to monitor progress here.\nPotential Pitfall: R packages not installed/forget to load them in script\nLogin to a Berkeley computer, launch R via the command line, and just make sure you can load all the packages you want. If they live there, they should live on the cluster. Inevitably, I forgot to load certain packages in certain R scripts. The script would fail, and I figured out why from the .Rout file.\nPotential Pitfall: not respecting the cluster scheduler\nThere is a lot going on in cluster_scenario1.R where all my actual code goes, but the important part is:\n\n\n\nA helper function does all of the heavy lifting, reading in input data, fitting models, and writing out results files. The key here is that the number we use for mc.cores needs to match --cpus-per-task in the bash script. The cluster scheduler uses this information when assigning tasks to parts of the cluster. I had initially forgotten to specify --cpus-per-task which messed things up. Thank you to Chris Paciorek for kindly explaining what I was doing wrong.\nPotential Pitfall: time/memory constraints\nThere are time and memory constraints that your code has to follow to be run on the Berkeley cluster. In practice this means that you have to be reasonably sure that each R script will finish in the allotted time and not require too much memory.\nI timed a few model fits locally and scaled that time up to estimate how long a certain chunk of simulations would take.\nThe memory constraints were much more trial and error. I would run a chunk, get a memory error, and then try to further partition the scenarios into different runs until I no longer got a memory error. Sorry that I can’t be more helpful here.\nPotential Pitfall: what should I be saving, more memory considerations\nThese are computationally expensive simulations, so it was important that I saved every possible output I would need to do my analysis. However, because there were so many simulations, the memory used really adds up. I went through a few trial and error iterations here too. I would run some simulations, realize I needed some extra information, and then have to adjust what results were saved for the next batch. Prior planning would be nice, but you just can’t always anticipate what you might need.\nPotential Pitfall: threads, tasks, cores, nodes, CPUs\nAt this point I had things working, but I’m impatient, and it seemed like things were taking much longer than I had expected. Luckily, the Berkeley Research Computing program was having a workshop on parallel computing, so I tuned in. I learned that there are different ways to partition tasks across cores/threads/cpus which live on nodes. Understanding the terminology and the hierarchy was helpful for me. Long story short, if you are using mclapply the relevant bash flag is in fact --cpus-per-task. My choice of 5 CPUs was to ensure that I wasn’t hogging too much of the statistics cluster which is used by the whole department. Note: on the Berkeley Stats cluster you can use a fraction of the total CPUs on a particular node whereas on other clusters you would want to use all the CPUs on one node or you are effectively wasting the others.\nPotential Pitfall: load balancing\nThis one turned out to be the real kicker.\n\nIf you recall, I had a setup like this:\n\n\n\nand I was parallelizing over k. This turned out to the absolute worst place to introduce parallelization. Go figure!\nFor example, when k=5, the model takes WAY LONGER to fit than when k=1. This meant that four cores were waiting around for the fifth task to be done before restarting with the next chunk of five models.\nAgain, the Berkeley Research Computing program came to my rescue. They had virtual office hours where Nicolas Chan and Christopher Hann-Soden explained that to “balance the computational load” I should parallelize over chunks of tasks that should take about the same order of magnitude of time to run. Note: tasks should also not be too fast because the overhead of moving everything on and off the cluster adds up. That luckily wasn’t my problem. So at first, I thought I should just parallelize over the replicates since each replicate of the same scenario should take about the same amount of time to fit. But then Christopher offered a game-changing suggestion.\n\n\n\nIf I pre-generated all of the combinations, then none of the cores would be waiting on anything else to finish. When a core finished, it would just take the next combination. Anecdotally, once I implemented this, things run MUCH faster (probably at least 2 times faster if not more).\nIn conclusion, my simulations are still running, but I feel better about them being as fast as possible. It only took weeks of trial and error to get the whole process streamlined.\n\nHopefully this helps you avoid some pitfalls. Good luck parallelizing! As always, feedback appreciated: [@sastoudt](https://twitter.com/sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:13:57-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-presidential-names/",
    "title": "What's in a (presidential) name?",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2020-03-03",
    "categories": [],
    "contents": "\nWe are hearing these names a lot, and it’s only going to continue (p.s. VOTE). But don’t these names seem a bit… ordinary to you?\n\n\nlibrary(babynames)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nHow popular are these Democratic candidate names throughout history? The babynames dataset can help shed some light on this question. Let’s look at the proportion of babies with these particular names over time.\n\n\ncandidates <- c(\"Elizabeth\", \"Amy\", \"Joseph\", \"Peter\", \"Bernard\", \"Michael\", \"Thomas\")\ncandidate_status <- c(T, F, T, F, T, T, F)\n\nyearTotals <- babynames %>%\n  group_by(year) %>%\n  summarise(yearTotal = sum(n))\n\ncandidateData <- babynames %>%\n  filter(name %in% candidates) %>%\n  group_by(year, name) %>%\n  summarise(count = sum(n))\n\ncandidateData <- merge(candidateData, yearTotals, by.x = \"year\", by.y = \"year\")\ncandidateData$prop <- candidateData$count / candidateData$yearTotal\n\n\nggplot(candidateData, aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\n\nFad names (with peaks): Amy, Thomas, Michael\nMore stable names: Bernard, Elizabeth, Joseph, Peter\nFor the sticklers, we can just look at the ones moving forward.\n\n\nggplot(subset(candidateData, name %in% candidates[candidate_status]), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\n\nBut this is just proportion of babies over time. What about the ranking of names? (Insert ranked choice voting joke here.)\n\n\ntest <- babynames %>%\n  group_by(year, name) %>%\n  summarise(count = sum(n))\n\nallYears <- unique(test$year)\n\n# this is gross, but bear with me\nhelper <- function(x) {\n  dat <- subset(test, year == x)\n  dat <- dat %>% arrange(desc(count))\n  dat$ranking <- 1:nrow(dat)\n\n  return(dat)\n}\n\nbyYearRanking <- lapply(allYears, helper)\n\nallRanked <- do.call(\"rbind\", byYearRanking)\n\nallRanked %>%\n  filter(name %in% candidates) %>%\n  ggplot(., aes(year, ranking, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal()\n\n\n\n\nMany of these names are ranked pretty high fairly consistently.\n\n\nallRanked %>%\n  filter(name %in% candidates[candidate_status]) %>%\n  ggplot(., aes(year, ranking, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal()\n\n\n\n\nAlas, poor Bernard, let’s zoom in a bit.\n\n\nallRanked %>%\n  filter(name %in% candidates) %>%\n  ggplot(., aes(year, ranking, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylim(c(0, 750))\n\n\n\n\nIt appears that these names often make the top 200 of baby names each year.\nWhat are the average and best ranks across time?\n\n\nallRanked %>%\n  filter(name %in% candidates) %>%\n  group_by(name) %>%\n  summarise(avgRank = mean(ranking), bestRank = min(ranking)) %>%\n  arrange(avgRank)\n\n\n# A tibble: 7 x 3\n  name      avgRank bestRank\n  <chr>       <dbl>    <int>\n1 Joseph       15.9        7\n2 Thomas       30.4       10\n3 Elizabeth    30.9        8\n4 Michael      60.3        1\n5 Peter       152.        67\n6 Amy         288.         9\n7 Bernard     594.       107\n\nJoseph has the best average rank, but only Michael reached the top spot at one point in history.\nHow consistently were these names in the top 200, 100, or 50 spot over time?\n\n\nallRanked %>%\n  filter(ranking <= 200) %>%\n  group_by(name) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count)) %>%\n  filter(name %in% candidates)\n\n\n# A tibble: 7 x 2\n  name      count\n  <chr>     <int>\n1 Elizabeth   138\n2 Joseph      138\n3 Michael     138\n4 Thomas      138\n5 Peter       112\n6 Amy          48\n7 Bernard      32\n\nallRanked %>%\n  filter(ranking <= 100) %>%\n  group_by(name) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count)) %>%\n  filter(name %in% candidates)\n\n\n# A tibble: 6 x 2\n  name      count\n  <chr>     <int>\n1 Elizabeth   138\n2 Joseph      138\n3 Thomas      138\n4 Michael      89\n5 Peter        43\n6 Amy          28\n\nallRanked %>%\n  filter(ranking <= 50) %>%\n  group_by(name) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count)) %>%\n  filter(name %in% candidates)\n\n\n# A tibble: 5 x 2\n  name      count\n  <chr>     <int>\n1 Joseph      138\n2 Elizabeth   137\n3 Thomas      120\n4 Michael      80\n5 Amy          19\n\nOut of 138 years of data, Joseph has always been in the top 50 of baby names. Elizabeth misses perfection by one year. Peter doesn’t crack the top 50 ceiling, but makes it into the top 100. Bernard manages to be in the top 200 for 32 years. It seems like these candidates are taking “name recognition” benefits a bit too far.\nWhat does this mean for the presidential hopefuls? We love a good George, John, or James for president, but we’ve also had a Millard and Rutherford, so it’s anyone’s guess.\n\n\npresidents <- c(\"George\", \"John\", \"Thomas\", \"James\", \"Andrew\", \"Martin\", \"William\", \"Zachary\", \"Millard\", \"Franklin\", \"Abraham\", \"Ulysses\", \"Rutherford\", \"Chester\", \"Grover\", \"Benjamin\", \"Theodore\", \"Woodrow\", \"Warren\", \"Calvin\", \"Herbert\", \"Harry\", \"Dwight\", \"Lyndon\", \"Richard\", \"Gerald\", \"Ronald\", \"Barack\", \"Donald\")\n\npresidentData <- babynames %>%\n  filter(name %in% presidents) %>%\n  group_by(year, name) %>%\n  summarise(count = sum(n))\n\npresidentData <- merge(presidentData, yearTotals, by.x = \"year\", by.y = \"year\")\npresidentData$prop <- presidentData$count / presidentData$yearTotal\n\npresidentPop <- presidentData %>%\n  group_by(name) %>%\n  summarise(averageProp = mean(prop)) %>%\n  arrange(desc(averageProp))\n\npresidentPop$id <- 1:nrow(presidentPop)\n\npresidentData <- merge(presidentData, presidentPop, by.x = \"name\", by.y = \"name\")\n\n\nggplot(subset(presidentData, id %in% 1:5), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\nggplot(subset(presidentData, id %in% 6:10), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\nggplot(subset(presidentData, id %in% 11:15), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\nggplot(subset(presidentData, id %in% 16:20), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\nggplot(subset(presidentData, id %in% 21:25), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\nggplot(subset(presidentData, id %in% 26:30), aes(year, prop, col = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"proportion of babies in year\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-06-05-presidential-names/presidential-names_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-06-05T14:54:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-bay-area-sound/",
    "title": "The Bay Area's Five Star Sound",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2019-10-29",
    "categories": [],
    "contents": "\nI signed up to get Dan Kopf’s “Golden Stats Warrior” newsletter, a newsletter that provides data driven insight into the Bay Area. You can sign up too here.\nThe last edition looked at the greatest albums from the Bay Area. Of albums by Bay Area natives, which ones got five stars on AllMusic? Being a bit of a music nerd, I wanted to dig in further. Kopf was kind enough to share the data with me so that I could do some more exploring.\nI decided to use the Spotify API to get audio features for all of the songs on these albums. Then we can see if there is a quintessential Bay Area sound.\n\n\nlibrary(dplyr)\nlibrary(spotifyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(ggradar)\nlibrary(kableExtra)\nlibrary(scales)\n\n\n\n\n\nspotify_client_id=\"\" ## put yours here\nspotify_client_secret=\"\" ## put yours here\naccess_token <- get_spotify_access_token(client_id=spotify_client_id,client_secret=spotify_client_secret)\n\n\n\n\n\nsetwd(\"~/Desktop/freelance/goldenStatWarrior\")\ndata <- read.csv(\"BayAreaAlbums-datafromletter3.csv\", stringsAsFactors = F)\ngreatest <- subset(data, Score == 5)\n\n\n\nFirst we grab the audio features for any album by the Bay Area artists who scored a five on at least one album.\n\n\nhelper <- function(x) {\n  get_artist_audio_features(x, include_groups = \"album\", authorization = access_token)\n}\n\n\n\nsafe_spotify <- safely(helper) ## don't want it to crash if there is a bad request\n\nallInfoGreatest <- unique(greatest$Artist.Name) %>% map(safe_spotify)\nsave(allInfoGreatest, file = \"allInfoGreatest.RData\")\n\ntestSly <- helper(\"family stone\") ## Sly & the Family Stone gets weird, so do them seperately\n\n\n\nThen we clean that up a bit.\n\n\nload(file = \"allInfoGreatest.RData\")\nclean <- lapply(allInfoGreatest, function(x) {\n  x$result\n})\nclean2 <- do.call(\"rbind\", clean)\n\nclean2$album_release_date <- as.Date(clean2$album_release_date)\nclean2$year <- year(clean2$album_release_date)\ntestSly$album_release_date <- as.Date(testSly$album_release_date)\ntestSly$year <- year(testSly$album_release_date)\nclean3 <- rbind(clean2, testSly)\n\nsave(clean3, file = \"clean3.RData\")\n\n\n\nNow we get things prepped for merging based on names. This could get ugly.\n\n\nsetwd(\"~/Desktop/freelance/goldenStatWarrior\")\nload(file=\"clean3.RData\")\n\ngreatest$Artist.Name=tolower(greatest$Artist.Name)\ngreatest$Album.Name=tolower(greatest$Album.Name)\nclean3$album_name=tolower(clean3$album_name)\nclean3$artist_name=tolower(clean3$artist_name)\n\n\n\nInevitably, it did. These were the ones where I had to do some manual fudging to get things right.\n\n\ngreatest$Album.Name[which(greatest$Album.Name == \"handel: messiah [2008 recording]\")] <- \"handel: messiah\"\ngreatest$Artist.Name[which(greatest$Artist.Name == \"dave brubeck\")] <- \"the dave brubeck quartet\"\n\ngreatest$Album.Name[which(greatest$Album.Name == \"lorraine at emmanuel\")] <- \"lorraine hunt lieberson at emmanuel\"\n\ngreatest$Album.Name[which(greatest$Album.Name == \"recital: lorraine hunt lieberson at ravinia\")] <- \"recital at ravinia\"\n\ngreatest$Album.Name[which(greatest$Album.Name == \"red house painters (roller-coaster)\")] <- \"red house painters i\"\n\ngreatest$Album.Name[which(greatest$Album.Name == \"john adams: harmonielehre; short ride in a fast machine\")] <- \"adams: harmonielehre - short ride in a fast machine\"\n\ngreatest$Artist.Name[which(greatest$Artist.Name == \"vince guaraldi\")] <- \"vince guaraldi trio\"\n\ngreatest$Album.Name[which(greatest$Album.Name == \"cast your fate to the wind: jazz impressions of black orpheus\")] <- \"cast your fate to the wind\"\n\ngreatest$Album.Name[which(greatest$Album.Name == \"a boy named charlie brown [original soundtrack]\")] <- \"a boy named charlie brown\"\n\ngreatest2 <- rbind.data.frame(greatest, greatest[which(greatest$Album.Name == \"cast your fate to the wind\"), ])\n\ngreatest2$Album.Name[nrow(greatest2)] <- \"jazz impressions of black orpheus\" ## split into two on Spotify\n\n\n\nActually do the matching.\n\n\nhelper <- function(x) {\n  use <- greatest2[x, c(\"Album.Name\", \"Artist.Name\", \"Year\")]\n\n  test <- subset(clean3, artist_name == use$Artist.Name)\n  toReturn <- test[agrep(use$Album.Name, test$album_name), ] ## sort of fuzzy match\n  return(toReturn)\n}\n\n\ngetData <- lapply(1:nrow(greatest2), helper)\ntest <- lapply(getData, nrow)\n\n\n\nWhich ones didn’t work out?\n\n\ntt = greatest[which(unlist(test)==0),] \nkable(tt) %>% kable_styling()\n\n\n\n\n\nArtist.URL\n\n\nYear\n\n\nCity\n\n\nArtist.Name\n\n\nScore\n\n\nAlbum.Name\n\n\n658\n\n\n/artist/marty-paich-mn0000858709\n\n\n1956\n\n\nOakland\n\n\nmarty paich\n\n\n5\n\n\nsings fred astaire\n\n\n933\n\n\n/artist/russell-garcia-mn0000808436\n\n\n1958\n\n\nOakland\n\n\nrussell garcia\n\n\n5\n\n\nfantastica: music from outer space\n\n\n1032\n\n\n/artist/souls-of-mischief-mn0000041857\n\n\n1993\n\n\nOakland\n\n\nsouls of mischief\n\n\n5\n\n\n93 ’til infinity\n\n\n1225\n\n\n/artist/tony%21-toni%21-ton%C3%A9%21-mn0000790667\n\n\n1990\n\n\nOakland\n\n\ntony! toni! toné!\n\n\n5\n\n\nthe revival\n\n\n1457\n\n\n/artist/david-murray-mn0000182855\n\n\n1982\n\n\nBerkeley\n\n\ndavid murray\n\n\n5\n\n\nmurray’s steps\n\n\n2043\n\n\n/artist/los-tigres-del-norte-mn0000806336\n\n\n1989\n\n\nSan Jose\n\n\nlos tigres del norte\n\n\n5\n\n\ntriunfo solido\n\n\n2294\n\n\n/artist/andra%C3%A9-crouch-mn0000031263\n\n\n1978\n\n\nSan Francisco\n\n\nandraé crouch\n\n\n5\n\n\nandrae crouch & the disciples\n\n\n2453\n\n\n/artist/big-brother-the-holding-company-mn0000758943\n\n\n1968\n\n\nSan Francisco\n\n\nbig brother & the holding company\n\n\n5\n\n\ncheap thrills\n\n\n2791\n\n\n/artist/constantine-orbelian-mn0000937363\n\n\n2017\n\n\nSan Francisco\n\n\nconstantine orbelian\n\n\n5\n\n\ngeorgy sviridov: russia cast adrift\n\n\n2910\n\n\n/artist/del-sol-string-quartet-mn0000335603\n\n\n2009\n\n\nSan Francisco\n\n\ndel sol string quartet\n\n\n5\n\n\nmarc blitzstein: first life - rare early works\n\n\n3518\n\n\n/artist/jeannette-sorrell-mn0001286718\n\n\n2011\n\n\nSan Francisco\n\n\njeannette sorrell\n\n\n5\n\n\ncome to the river: an early american gathering\n\n\n4520\n\n\n/artist/leon-fleisher-mn0001209232\n\n\n2009\n\n\nSan Francisco\n\n\nleon fleisher\n\n\n5\n\n\nhindemith: klaviermusik mit orchester; dvorák: symphony no. 9 “from the new world”\n\n\n4588\n\n\n/artist/linda-tillery-mn0000594698\n\n\n1993\n\n\nSan Francisco\n\n\nlinda tillery\n\n\n5\n\n\nsecrets\n\n\n4635\n\n\n/artist/lorraine-hunt-lieberson-mn0001472476\n\n\n2007\n\n\nSan Francisco\n\n\nlorraine hunt lieberson\n\n\n5\n\n\nsongs by mahler, handel & peter lieberson\n\n\n5492\n\n\n/artist/ruggiero-ricci-mn0000249876\n\n\n1985\n\n\nSan Francisco\n\n\nruggiero ricci\n\n\n5\n\n\nfranck: violin sonata; prokofiev: violin sonata op. 94a\n\n\n6281\n\n\n/artist/vince-guaraldi-mn0000201678\n\n\n1966\n\n\nSan Francisco\n\n\nvince guaraldi trio\n\n\n5\n\n\nit’s the great pumpkin, charlie brown\n\n\nI checked, and this wasn’t just an issue of merging on slightly different names. I couldn’t find the analogue in the albums pulled from Spotify based on the artist names. We only lost 16 out of 56, not bad!\n\n\ndataINeed=do.call(\"rbind\",getData)\nsave(dataINeed,file = \"fullData2018-10-28.RData\")\n\n\n\nNow we get to the fun stuff. What kind of features do we have? We want to get the average and variability across songs, per album.\n\n\ncharacteristics <- c(\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\")\n\ncharSum <- dataINeed[, c(characteristics, \"album_name\")] %>%\n  group_by(album_name) %>%\n  summarise(\n    meanDanceability = mean(danceability), meanEnergy = mean(energy),\n    meanLoudness = mean(loudness), meanSpeechiness = mean(speechiness),\n    meanAcousticness = mean(acousticness), meanInstrumentalness = mean(instrumentalness), meanLiveness = mean(liveness), meanValence = mean(valence),\n    varDanceability = var(danceability), varEnergy = var(energy),\n    varLoudness = var(loudness), varSpeechiness = var(speechiness),\n    varAcousticness = var(acousticness), varInstrumentalness = var(instrumentalness), varLiveness = var(liveness), varValence = var(valence)\n  )\n\n\n## add mean of everything in black\nadd <- cbind.data.frame(album_name = \"avg\", t(apply(dataINeed[, c(characteristics)], 2, mean)))\n\n\nmeanStuff <- charSum[, 1:9]\nnames(add) <- names(meanStuff)\nmeanStuff <- rbind.data.frame(meanStuff, add)\nvarStuff <- charSum[, c(1, 10:17)]\n\n\n\nNow, I wanted to make some cool radar plots, inspired by music analysis like this. You can also find descriptions of the features there.\nOf course there is a gg version, but I had to tweak the internal functionality to get the plots to look like what I wanted them to. This gets a bit gnarly, so I’m compartmentalizing that code over here.\n\n\nsetwd(\"~/Desktop/freelance/goldenStatWarrior\")\nsource(\"fix_ggradar.R\")\n\n\n\nI also snagged some code from the internal ggradar so that I could add a line representing the average across all albums in black. That stuff lives in the following helper function.\n\n\nhelperPlot <- function(base) {\n  grid.min <- 0 # , # 10,\n  grid.mid <- 0.5 # , # 50,\n  grid.max <- 1 # , # 100,\n  centre.y <- grid.min - ((1 / 9) * (grid.max - grid.min))\n  plot.data <- as.data.frame(toP[nrow(toP), ])\n  names(plot.data)[1] <- \"group\"\n\n\n  if (!is.factor(plot.data[, 1])) {\n    plot.data[, 1] <- as.factor(as.character(plot.data[, 1]))\n  }\n\n  plot.data.offset <- plot.data\n  plot.data.offset[, 2:ncol(plot.data)] <- plot.data[, 2:ncol(plot.data)] + abs(centre.y)\n  # print(plot.data.offset)\n  # (b) convert into radial coords\n  group <- NULL\n  group$path <- CalculateGroupPath(plot.data.offset)\n\n  group.line.width <- 1.5\n  group.point.size <- 6\n  # ... + group (cluster) 'paths'\n  base <- base + geom_path(\n    data = group$path, aes(x = x, y = y, group = group),\n    size = group.line.width, lty = 2, col = \"black\"\n  )\n\n  # ... + group points (cluster data)\n\n  base <- base + geom_point(data = group$path, aes(x = x, y = y, group = group), size = group.point.size, alpha = 0.5, col = \"black\")\n\n  return(base)\n}\n\n\n\nAcousticness for the win! However, there are quite a few albums that hit high instrumentalness (makes sense, lots of jazz and classical music), high energy, danceability (they aren’t called Funky Divas for nothing), and loudness (Green Day, anyone?)\n\n\ntoP <- meanStuff %>% mutate_at(vars(-album_name), rescale)\n\nbase <- ggradar2(toP[-nrow(toP), ])\n\nhelperPlot(base)\n\n\n\n\nWhich album is most like the average five-star Bay Area album? Dave Brubeck’s “Time Out”\nThis isn’t particularly suprising has Brubeck has the third highest number of albums on the five-star list.\n\n\ntest=as.matrix(dist(toP[,-1]))\n\n#toP[which.min(test[nrow(toP),-ncol(test)]),]\n\ndataINeed %>% group_by(album_id) %>% summarise(count=n(),artist=artist_name[1])  %>% group_by(artist) %>% summarise(count=n()) %>% arrange(desc(count))\n\n\n# A tibble: 24 x 2\n   artist                   count\n   <chr>                    <int>\n 1 vince guaraldi trio          9\n 2 lorraine hunt lieberson      8\n 3 the dave brubeck quartet     6\n 4 leon fleisher                5\n 5 moby grape                   5\n 6 david murray                 3\n 7 san francisco symphony       3\n 8 chanticleer                  2\n 9 grateful dead                2\n10 green day                    2\n# … with 14 more rows\n\nThe Bay Area likes variability in valence (positivity of sound).\n\n\ntoP = varStuff %>% mutate_at(vars(-album_name), rescale) \n\nbase = ggradar2(toP[-nrow(toP),])\n\nhelperPlot(base)\n\n\n\n\nWhich album has the variability most like the average five-star Bay Area album? The Grateful Dead’s “American Beauty” (from SF)\n\n\ntest=as.matrix(dist(toP[,-1]))\n\n#toP[which.min(test[nrow(toP),-ncol(test)]),]\n\n\n\nWhat about the Bay Area pace? Let’s look at tempo.\n\n\nggplot(dataINeed, aes(x=tempo)) + geom_histogram()+xlab(\"beats per minute\")\n\n\n\n\nWhat song is most like the average tempo? Ironically, Brubeck’s St. Louis Blues. Brubeck is a Concord original.\n\n\nmean(dataINeed$tempo)\n\n\n[1] 111.9667\n\n#dataINeed[which.min(abs(dataINeed$tempo-mean(dataINeed$tempo))),]\n\n#blogdown::shortcode(\"youtube\", \"tq2YENV_Q9s\")\n\n\n\n\n\nBut there is a peak in the tempo distribution that is slower than the mean. What song is most like this mode? Gotta love a good cover of The Beatles’ “Yesterday” by Oakland’s En Vogue.\n\n\n#which.max(table(round(dataINeed$tempo)))\n\n# mode\n#dataINeed[which.min(abs(dataINeed$tempo-80)),]\n\n#blogdown::shortcode(\"youtube\", \"k1PiJAeydLs\")\n\n\n\n\n\nWhat about those few faster paced jams? Here is an inspirational song from San Francisco’s Moby Grape.\n\n\n#dataINeed[which.min(abs(dataINeed$tempo-200)),]\n\n#blogdown::shortcode(\"youtube\", \"9-RKXCvb5E\")\n\n\n\n\n\nWhat is an example of a song that is in the most prominent key? Oakland’s Digital Underground gives us “Doowutchyalike.”\n\n\n#which.max(table(dataINeed$key_mode))\n\n#dataINeed[which(dataINeed$key_mode==\"A major\")[1],]\n\n#blogdown::shortcode(\"youtube\", \"5P4WZHlHsyk\")\n\n\n\n\n\nThere is plenty more to dig into here.\nWhat about all of the other star-levels?\nAre the 5-star albums from another area noticeably different than the Bay Area ones?\nDo these Bay Area albums call out their roots in the song lyrics?\nBut those are for another day…\nThanks again to Dan Kopf (@dkopf) for sharing the data! Thoughts, comments, suggestions, etc. welcome –> @sastoudt.\n\n\n\n",
    "preview": "posts/2021-06-05-bay-area-sound/bay-area-sound_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-06-05T14:34:29-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-songs-to-strut-to/",
    "title": "Songs to Strut To",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2019-10-11",
    "categories": [],
    "contents": "\n\n\n\nI watched Saturday Night Fever in preparation for my blog post about its soundtrack on CoverMeSongs. The opening scene shows John Travolta marching down the street to the Bee Gees’s “Stayin Alive.’”\n\nI learned that this song is also used to help people practice CPR because it has the correct tempo for recommended compressions. This made me wonder more about the tempo of the song. Would a New Yorker really walk along at that pace? Relatedly, as an East Coaster transplanted to the West Coast for graduate school, I am certainly aware of differences in walking speed norms by geography. I wondered if there was a perfect strutting song per region.\nApparently, there once was a study that tried to quantify differences in walking speeds across many cities. The researchers went to various cities and timed how long people took to walk 60 feet. I entered that data for the 32 countries across the globe into a spreadsheet.\n\n\n\nThe top five fastest walking countries:\n\n\ncity\n\n\ncountry\n\n\ntime\n\n\nSingapore\n\n\nSingapore\n\n\n10.55\n\n\nCopenhagen\n\n\nDenmark\n\n\n10.82\n\n\nMadrid\n\n\nSpain\n\n\n10.89\n\n\nGuangzhou\n\n\nChina\n\n\n10.94\n\n\nDublin\n\n\nIreland\n\n\n11.03\n\n\nThe top five slowest walking countries:\n\n\n\n\ncity\n\n\ncountry\n\n\ntime\n\n\n28\n\n\nDamascus\n\n\nSyria\n\n\n14.94\n\n\n29\n\n\nAmman\n\n\nJordan\n\n\n14.95\n\n\n30\n\n\nBern\n\n\nSwitzerland\n\n\n17.37\n\n\n31\n\n\nManama\n\n\nBahrain\n\n\n17.69\n\n\n32\n\n\nBlantyre\n\n\nMalawi\n\n\n31.60\n\n\nNow we need to get this on a beats per minute scale. I’m going to treat one stride as a beat. According to LiveStrong, one stride is about 2.6 feet.\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.82   97.46  109.03  106.30  115.10  131.24 \n\n“Stayin’ Alive” comes in at 103 beats per minute, a little under the average across these countries. Notably, this tempo is too slow for John Travolta’s character, Tony Manero, to be strutting down the streets of New York to. That is more of a between Japan and Canada tempo.\n\n\n\n\ncity\n\n\ncountry\n\n\ntime\n\n\nstepsPerMin\n\n\n19\n\n\nTokyo\n\n\nJapan\n\n\n12.83\n\n\n107.9201\n\n\n20\n\n\nOttawa\n\n\nCanada\n\n\n13.72\n\n\n100.9195\n\n\nWhat song would be more appropriate tempo-wise? For that, we look at the songs on the top 100 billboard charts from 1960 to 2015 (handily compiled by the billboard package).\n\n\nyear\n\n\nartist_name\n\n\ntrack_name\n\n\n1984\n\n\nRay Parker, Jr.\n\n\nGhostbusters\n\n\n\nBut this song wasn’t out yet, so let’s limit to hits prior to 1977, when Saturday Night Fever was released.\n\n\nyear\n\n\nartist_name\n\n\ntrack_name\n\n\n1972\n\n\nDennis Coffey & The Detroit Guitar Band\n\n\nScorpio\n\n\n\n{{% youtube \"DyuS16P911g\" %}}\n\nNow that is more like it!\nWhat is the perfect strutting song for these other countries (granted that the US billboard charts may not accurately reflect “jams” in other countries)?\n\n\ncity\n\n\ncountry\n\n\nstepsPerMin\n\n\nyear\n\n\nartist_name\n\n\ntrack_name\n\n\nSingapore\n\n\nSingapore\n\n\n131.24316\n\n\n1986\n\n\nWhitney Houston\n\n\nGreatest Love Of All\n\n\nCopenhagen\n\n\nDenmark\n\n\n127.96815\n\n\n2012\n\n\nChris Brown\n\n\nDon’t Wake Me Up\n\n\nMadrid\n\n\nSpain\n\n\n127.14558\n\n\n1979\n\n\nLeif Garrett\n\n\nI Was Made for Dancin’\n\n\nGuangzhou\n\n\nChina\n\n\n126.56448\n\n\n1966\n\n\nAllen Reynolds\n\n\nFive O’Clock World\n\n\nDublin\n\n\nIreland\n\n\n125.53177\n\n\n1982\n\n\nEarth, Wind & Fire\n\n\nLet’s Groove\n\n\nCuitiba\n\n\nBrazil\n\n\n124.40390\n\n\n1985\n\n\nSurvivor\n\n\nThe Search Is Over\n\n\nBerlin\n\n\nGermany\n\n\n124.06948\n\n\n2011\n\n\nKaty Perry\n\n\nFirework\n\n\nNew York\n\n\nUSA\n\n\n115.38462\n\n\n1984\n\n\nRay Parker, Jr.\n\n\nGhostbusters\n\n\nUtrecht\n\n\nNetherlands\n\n\n115.00128\n\n\n2015\n\n\nMark Ronson\n\n\nUptown Funk\n\n\nVienna\n\n\nAustria\n\n\n114.81056\n\n\n1987\n\n\nThe Georgia Satellites\n\n\nKeep Your Hands To Yourself\n\n\nWarsaw\n\n\nPoland\n\n\n114.71544\n\n\n1977\n\n\nKC & The Sunshine Band\n\n\nI’m Your Boogie Man\n\n\nLondon\n\n\nUnited Kingdom\n\n\n113.77283\n\n\n1978\n\n\nGerry Rafferty\n\n\nBaker Street\n\n\nZagreb\n\n\nCroatia\n\n\n113.49306\n\n\n1968\n\n\nClarence Carter\n\n\nSlip Away\n\n\nPrague\n\n\nCzech Republic\n\n\n112.11461\n\n\n1986\n\n\nZZ Top\n\n\nSleeping Bag\n\n\nWellington\n\n\nNew Zealand\n\n\n109.71596\n\n\n1968\n\n\nThe Lettermen\n\n\nGoin’ Out Of My Head/Can’t Take My Eyes Off You (Medley) - Live;1987 Digital Remaster\n\n\nParis\n\n\nFrance\n\n\n109.45576\n\n\n1960\n\n\nConnie Francis\n\n\nMy Heart Has A Mind Of Its Own\n\n\nStockholm\n\n\nSweden\n\n\n108.59729\n\n\n1976\n\n\nDavid Bowie\n\n\nGolden Years - 1999 Remastered Version\n\n\nLjubljana\n\n\nSlovenia\n\n\n108.51218\n\n\n1980\n\n\nSam Morrison and Turn The Page\n\n\nAgainst the Wind\n\n\nTokyo\n\n\nJapan\n\n\n107.92014\n\n\n1984\n\n\nThompson Twins\n\n\nHold Me Now\n\n\nOttawa\n\n\nCanada\n\n\n100.91949\n\n\n1999\n\n\nFaith Evans\n\n\nLove Like This\n\n\nHarare\n\n\nZimbabwe\n\n\n99.46950\n\n\n1998\n\n\nDiddy\n\n\nBeen Around The World (feat. The Notorious B.I.G. & Mase)\n\n\nSofia\n\n\nBulgaria\n\n\n99.18448\n\n\n1964\n\n\nThe Trashmen\n\n\nSurfin’ Bird\n\n\nTaipei\n\n\nTaiwan\n\n\n98.90110\n\n\n1967\n\n\nBuffalo Springfield\n\n\nFor What It’s Worth\n\n\nCairo\n\n\nEgypt\n\n\n97.64565\n\n\n1975\n\n\nSugarloaf\n\n\nDon’t Call Us, We’ll Call You\n\n\nSanaa\n\n\nYemen\n\n\n96.89401\n\n\n1996\n\n\nEric Clapton\n\n\nChange The World\n\n\nBucharest\n\n\nRomania\n\n\n96.42168\n\n\n1962\n\n\nThe Shirelles\n\n\nSoldier Boy\n\n\nDubai\n\n\nUnited Arab Emirates\n\n\n94.57755\n\n\n1987\n\n\nGregory Abbott\n\n\nShake You Down - Single Version\n\n\nDamascus\n\n\nSyria\n\n\n92.67841\n\n\n1970\n\n\nLas Clasicas de Universal Stereo\n\n\nReflections of My Life\n\n\nAmman\n\n\nJordan\n\n\n92.61641\n\n\n1960\n\n\nPercy Faith & His Orchestra\n\n\nThe Theme From “A Summer Place” - Single Version\n\n\nBern\n\n\nSwitzerland\n\n\n79.71303\n\n\n2007\n\n\nGym Class Heroes\n\n\nCupid’s Chokehold / Breakfast In America - Radio Mix\n\n\nManama\n\n\nBahrain\n\n\n78.27108\n\n\n1993\n\n\nDef Leppard\n\n\nTwo Steps Behind - Live\n\n\nBlantyre\n\n\nMalawi\n\n\n43.81694\n\n\n1979\n\n\nBarbra Streisand\n\n\nYou Don’t Bring Me Flowers\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:22:23-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-wealth-tax-app-making/",
    "title": "The Making of the Wealth Tax App: Shiny Lessons Learned",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2019-03-18",
    "categories": [],
    "contents": "\nUC Berkeley economists Emmanuel Saez and Gabriel Zucman analyzed Senator Elizabeth Warren’s proposal for a wealth tax, and Fernando Hoces de la Guardia from the Berkeley Initiative for Transparency in the Social Sciences (BITSS) wanted to turn their work into an open policy analysis (OPA). I got involved because they needed someone to make an interactive visualization that would allow users to explore different wealth tax proposals. Check it out here.\nI learned a lot from the experience and wanted to share some lessons learned.\nMaking a Shiny app for use by the public\nUp until this point I had only made Shiny apps for my individual use or for the use of a few collaborators who were carefully “trained” on the expected inputs. These apps were functional, but brittle. If a user provided unexpected input, the app had no capability to save face; the app would just crash. The apps were also were run locally, or on the free version of shinyapps.io with no real worries about heavy traffic. Making a Shiny app that was going to be more publically facing came with its own set of challenges.\nUnexpected User Behavior\nIt was fairly straightforward to create an app that had the desired functionality as long as users behaved as expected. However what if the tax brackets were entered in the wrong order? What if there were duplicates in the brackets? What if nonsensical values were entered such as a tax evasion rate below zero or above 100 or a tax rate below zero? It’s not ideal for the app to crash if given unexpected input while giving users no feedback about what went wrong.\n\nMy first instinct was just to throw a bunch of checks into every main function (LOTS of if, else if, else chunks). However this strategy at one point led to a visible lag in the app’s reactivity. For the final version of the app, we saved time by allowing the user to enter whatever they wanted and postpone dealing with issues until we needed to update the plot and calculations (signaled by a click of a button). At this point we reordered the brackets, broke ties in bracket values, and updated tax rates and evasion to be within the realm of plausibility when the user clicks “update”. We also updated the displayed values to match our fixes, so the user sees that the calculated values and plots are made under conditions different from what they entered.\nThere are some unexpected user behaviors that we still don’t react to. We allow users to enter non-monotonic tax rates, and the plot and calculations respect this (even though in practice, this would be a weird proposed tax scheme). The app does not immediately crash if you enter a non-numeric entry, but if you don’t catch your mistake before clicking the “update” button, the app will crash. This was mainly a decision based on time constraints rather than being something truly un-fixable.\nDeployment\nI didn’t do much on the actual deployment side. Katie Donnelly Moran, Clancy Green, and Akcan Balkir worked to make this happen through the use of AWS to allow for some control in the case of high traffic to the app. This guide was helpful. However, this was the first time I used Binder. Binder “allows you to create custom computing environments that can be shared and used by many remote users” and can handle moderate traffic. For the purposes of this project, this means that a user can go straight to the Shiny app or step through the dynamic document without installing R and RStudio on their own computers. This was much easier to set up than I anticipated. I just followed this example. Also, shout out to Lindsey Heagy who gave me a crash course in how Binder, BinderHub, and JupyterHub work together behind the scenes.\nTesting and breaking things\nBecause I built the app and knew the internal structure, I would, without thinking, avoid doing things that would cause the app to crash, thereby making my testing ineffectual. However, Katie, Emmanuel, and Gabriel were great at finding bugs. I cannot count the number of times that I thought the app was ready and then they would within ten minutes have a list of things that needed to be fixed.\n\nSome of these fixes involved logical errors on my part that they were able to easily identify by using their domain knowledge. I was learning the economics on the fly, and sometimes I misinterpreted how certian values should be calculated.\nMost fixes involved anticipating and being resilient to unexpected user behavior. Sometimes unexpected user behavior would crash the app, but other times, unexpected user behavior would violate the assumptions I was making in order to make the required calculations. Nothing would officially crash, but the returned calculations would not be accurate. These errors were more pernicious and harder for me to detect since my expectations for what the output would look like were not appropriately tailored to the underlying economics. A number that would surprise Katie, would not jump out at me as obviously wrong.\nAnother change that I would not necessarily have thought of on my own was making the switch from using sliders to text boxes. Sliders allow more control of the inputs (users can only pick from allowable options), so I started with those to make my life easier. However, it became clear in testing that the sliders can get annoying if you have a very specific and detailed wealth tax plan in mind to test out. Text boxes allow for more freedom.\nMajor take-away: It takes a village to really put a Shiny app through its paces.\nReactivity is a double-edged sword\nAnd now for some technical bits…\nEven though reactivity is often what you want in an interactive visualization, there were times where reactivity caused some headaches.\nWhen the reactivity is too fast\nSuppose a user is typing in a value: “.” on their way to “.5” or “50” on their way to “500”. If the app tried to start calculating right away, it could run into issues with “.” not having a numeric interpretation or “50” being tied with another bracket. Even if there were reactive fixes to ties, this could be a problem. We might bump “50” up to “60” to break a tie, but the user really just wants to be allowed to finish typing to “500”. As users typed it was also distracting to see the plot keep jumping around, recalculating after every keystroke. Requiring a button click before starting calculations and plots (eventReative) was a good fix.\nreq() was also a lifesaver! This wrapper makes sure that the particular value is available before continuing with the calculation. This was handy when we created new brackets on the fly. It would take a split second to create the new object needed for the calculation, so the app needed to know to pause in the calculation until the value was populated.\nWhen the reactivity is too slow\nWhen we updateded the user interface using things like updateTextInput() and renderUI, it would take some time for the new values to kick in, affecting the calculations and plots. req() was helpful for some of this, but there were still some sticking points. For example, one thing that I couldn’t easily get around was that the plot does not automatically reflect the tie updates. Instead, I had to add a conditional warning so that the user would click “update” again to reflect these changes (I’m open to less clunkier ways to do this. Share your wisdom!).\nTimeline\nI severely underestimated how much time this project would take. Getting the original mock up with four fixed brackets took a few hours. The rest took weeks. The majority of the extra time spent on the app didn’t even come from adding features to allow users to tune more parameters of a tax plan.\nThe biggest time commitment came from unexpected user and reactivity behavior. These changes were hardest to make because I had to completely reorganize the inner workings of the app to accomodate certain behaviors.\n\nDealing with these structural changes was definitely one of those times where I would stare at something for hours, give up and take a break, then come back the next day and fix it in ten minutes. Moral: take breaks!\nThe good news is that I effectively used git branches to work on these major overhauls which was a good skill to practice. By using branches, others on the team could use the current version of the app to do other tests while I was breaking things on another branch. This also made it easier to start over again if an approach in a different branch wasn’t working out.\n\nThe making of this app was certainly an adventure! I learned a lot about my Shiny instincts (the good and the bad) and how to better foritfy an app to withstand unexpected user behavior (plus some economics along the way).\nThank you again to the whole team.\nFeedback, questions, comments, etc. are welcome (@sastoudt).\nIf you are a Shiny afficianado, feel free to dig into the source code and streamline things. Pull requests are welcome. Similarly, if you find a bug, please file an issue.\nP.S. Alvin Chang (@alv9n) also made a cool, interactive exploration of the wealth tax.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:22:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-2018-highlights-2019-goals/",
    "title": "2018 Highlights, 2019 Goals",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-12-31",
    "categories": [],
    "contents": "\nIt’s that time of year, time to reflect on our little victories and make plans of attack for the future. I love reading about everyone’s accomplishments and goals, so I figured I would add mine to the mix.\n\nI definitely felt what I’m calling the fourth year funk this semester and didn’t feel like I was quite in the zone nor at my very best. More on this later? But perseverance is the name of the grad school game, so luckily there are still some things I accomplished.\nThings I’m proud of from 2018\nI passed my qualifying exam! I’m officially a PhD Candidate. 👩‍🎓\nI helped @DebAtStat draft a book about writing as statisticians. 📖\nI had my first taste of industry by interning as a Data Scientist at @FBNFarmers. 🌽\nI blogged my way through the tidyverse and participated in some #TidyTuesdays. 👩‍💻\nI became a data science fellow at @UCBIDS. 🎉 Stay tuned for information about the progress of the Diversity and Inclusion Working Group in 2019.\nGoals for 2019\nPapers! Seriously, this needs to happen so I can be a Dr. 📝 The first paper of my dissertation on identifiability controversies in species distribution and abundance models is so so close to being ready. Ideas for the second paper of my dissertation are becoming more concrete.\nWrite at least one blog post a month and participate in #TidyTuesday at least twice a month. I was pretty good at doing this throughout the summer, but the semester took its toll on my writing for fun time. I aim to set some time aside this year to keep writing.\nGet some breadth by getting involved in a side project collaboration outside my main focus area.\nBecome more involved in open source development, whether this is an R package that accompanies my dissertation, or something on a smaller scale, like working on GitHub issues for others’ packages.\nKeep thinking about how to teach writing for statisticians and data scientists with @DebAtStat.\nShoutouts\nThank you to everyone who supported me through this year!\nMy family: thank you for telling me not to take on too much and then patiently listening to me complain when I ignore your advice and take on too much. 🤓\nMy friends: thank you for commiserating through the tough times and for indulging me when I procrastinate by searching for dream jobs.\nMy advisors and mentors: thank you for helping me carve my own path (please ignore the above admission of procrastination 😳).\nMy Berkeley Stat and BIDS peers: I’m proud of the communities we are fostering.\nThe #rstats, #RLadies, and @WiMLDS_BayArea communities: you all are so kind and helpful, and everyone’s experiences and paths through statistics and data science are inspiring!\nIn conclusion, bring it on 2019.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:24:10-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-base-r-new-tricks/",
    "title": "You Can Teach a Base R Dog Tidy Tricks",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-09-01",
    "categories": [],
    "contents": "\nThis summer I aimed to update my R skills via the tidyverse and blog about my experience. See here for my motivations.\nAlthough I didn’t make it through all of the tidyverse affiliated packages, I did finish blog posts for all of the core packages: ggplot took two parts, dplyr, tidyr, readr, tibble, stringr, purrr joint with @kellieotto, and forcats.\nThis felt like a milestone worth briefly reflecting on, so here goes.\nTake-Aways\nWhat I learned has stuck much more than I thought it would. I didn’t expect my habits to change so noticeably. I’m mainly taking advantage of purrr, tidyr, ggplot2, and new dplyr tricks in my everyday work. Using these packages now comes pretty naturally. I’m not consciously saying to myself “oh, I should switch this to be tidy”, I’m just doing it. It helped that this summer I was interning, so I had my hands in data more than I usually do (muscle memory!). Commuting via public transportation also gave me blocks of time to focus on this. ggplot2 is still a stumbling block for me if I’m trying to do something too fancy, but I am faceting and coloring by a variable like a champ. Also, shout out to Tidy Tuesday for giving me a chance to continually practice making a wide range of plots.\nWorking on this took longer than I thought. Thinking of motivating scenarios so that I wasn’t just going through the documentation examples was challenging. As optimistic as I was about a summer of getting things done while working, apparently I have limits. Who knew? There were some tricky aspects to the tibble, stringR, and readr posts that took me awhile to figure out, but because I had a specific goal within the blog’s scenario, I was motivated to persevere.\nThis exercise made me more patient and brave. I am much more likely now to spend some extra time making code nice instead of just slapping together something that works and moving on. Investing in tools and best practices has paid off. I am also less worried about making my code available to others. I hope that by me being open about my stumbling blocks, others will feel more comfortable sharing theirs. The #rstats community is kind and constructive. Plus every “like” on Twitter from one of my posts gives me a little boost #sorrynotsorry.\nWhat’s Next?\nThe elephant in the room is that my actual blog setup is horrendous. When I first started out, I couldn’t figure out blogdown and I didn’t want to lose my momentum to get the blog going. So first up is to improve my website and blog workflow. I am definitely open to suggestions and resources.\nI also want to continue with the other tidyverse affiliated packages (and try to keep up with Tidy Tuesday). There is plenty more to learn.\nLonger term, I want to start getting involved in the open source community. I plan to tackle some beginner pull requests this year (thanks to @dpseidel and @alexpghayes for pointing out places to start) to get some experience.\nThank you to everyone who has kept up with my journey, given my posts a signal boost, and provided suggestions along the way!\nI started this whole thing with an Usher reference, so I have to bookend it here.Feedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:34:00-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-cursing-with-factors/",
    "title": "Cursing with Instead of at Factors",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-08-26",
    "categories": [],
    "contents": "\n\n\nlibrary(forcats)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nlibrary(sp)\nlibrary(maps)\nlibrary(maptools)\n## Thank you to Chris Kennedy for kindly telling me I should be using library instead of require on my posts.\n\n\n\nForever ago @dpseidel drew my attention to an awesome dataset collected by @jimwebb about tweets that cursed being cold/hot. This reminded me of a project that @danascientist (so many Dana’s!) and I did in @BaumerBen’s class where we tried to assess how cold it had to be for people to talk about being cold on Twitter. I was curious whether differences in region would impact the overall median per phrase since we expect those in warmer regions to have less of a tolerance to the cold.\nThis seemed like a perfect dataset to work through the forcats package and tackle some factors. Be warned, curse words are involved.\n\n\nsetwd(\"~/Desktop/cold_as_f-ck-master\")\ndata <- read.csv(\"data/collected-tweets.csv\")\n\n\n\nFirst I translate lat/long to state.\n\n\n# https://stackoverflow.com/questions/8751497/latitude-longitude-coordinates-to-state-code-in-r\nlatlong2state <- function(pointsDF) {\n  # Prepare SpatialPolygons object with one SpatialPolygon\n  # per state (plus DC, minus HI & AK)\n  states <- map(\"state\", fill = TRUE, col = \"transparent\", plot = FALSE)\n  IDs <- sapply(strsplit(states$names, \":\"), function(x) x[1])\n  states_sp <- map2SpatialPolygons(states,\n    IDs = IDs,\n    proj4string = CRS(\"+proj=longlat +datum=WGS84\")\n  )\n\n  # Convert pointsDF to a SpatialPoints object\n  pointsSP <- SpatialPoints(pointsDF,\n    proj4string = CRS(\"+proj=longlat +datum=WGS84\")\n  )\n\n  # Use 'over' to get _indices_ of the Polygons object containing each point\n  indices <- over(pointsSP, states_sp)\n\n  # Return the state names of the Polygons object containing each point\n  stateNames <- sapply(states_sp@polygons, function(x) x@ID)\n  stateNames[indices]\n}\n\ndata$state <- latlong2state(data[, c(\"long\", \"lat\")])\n\n\n\nThen I break down the states into regions using fct_collapse since we don’t have enough tweets per state.\n\n\ndata$division <- fct_collapse(data$state,\n  newengland = c(\"connecticut\", \"maine\", \"massachusetts\", \"new hampshire\", \"rhode island\", \"vermont\"),\n  midatlantic = c(\"new jersey\", \"new york\", \"pennsylvania\"),\n  eastnorthcentral = c(\"illinois\", \"indiana\", \"michigan\", \"ohio\", \"wisconsin\"),\n  westnorthcentral = c(\"iowa\", \"kansas\", \"minnesota\", \"missouri\", \"nebraska\", \"north dakota\", \"south dakota\"),\n  southatlantic = c(\"delaware\", \"florida\", \"georgia\", \"maryland\", \"north carolina\", \"south carolina\", \"district of columbia\", \"west virginia\", \"virginia\"),\n  eastsouthcentral = c(\"alabama\", \"kentucky\", \"mississippi\", \"tennessee\"),\n  westsouthcentral = c(\"arkansas\", \"louisiana\", \"oklahoma\", \"texas\"),\n  mountain = c(\"arizona\", \"colorado\", \"idaho\", \"montana\", \"nevada\", \"new mexico\", \"utah\", \"wyoming\"),\n  pacific = c(\"alaska\", \"california\", \"hawaii\", \"oregon\", \"washington\")\n)\n\ndata$region <- fct_collapse(data$division,\n  northeast = c(\"newengland\", \"midatlantic\"),\n  midwest = c(\"eastnorthcentral\", \"westnorthcentral\"),\n  south = c(\"southatlantic\", \"eastsouthcentral\", \"westsouthcentral\"),\n  west = c(\"mountain\", \"pacific\")\n)\n\n\n\nNo Hawaii or Alaska in our dataset, but that’s fine. Also, am I the only one who thinks it is weird that Delaware is considered the south?\nWe can use fct_count to easily count how many tweets fall in each category.\n\n\nfct_count(data$division)\n\n\n# A tibble: 10 x 2\n   f                    n\n   <fct>            <int>\n 1 eastsouthcentral   313\n 2 mountain           224\n 3 westsouthcentral   709\n 4 pacific            911\n 5 newengland         108\n 6 southatlantic     1043\n 7 eastnorthcentral   697\n 8 westnorthcentral   172\n 9 midatlantic        500\n10 <NA>               698\n\nfct_count(data$region)\n\n\n# A tibble: 5 x 2\n  f             n\n  <fct>     <int>\n1 south      2065\n2 west       1135\n3 northeast   608\n4 midwest     869\n5 <NA>        698\n\nEven these are a bit sparse, so we’ll stick with region to try to get a reasonable number of tweets per phrase.\nFor reference I want to easily be able to tell which phrase contains “hot” or “cold”.\n\n\ndata$type <- ifelse(grepl(\"hot\", data$phrase, ignore.case = T), \"H\", ifelse(grepl(\"cold\", data$phrase, ignore.case = T), \"C\", \"O\"))\n## I should be using stringr, but forgive me\n\n\n\nNow I want to pick the phrases that are displayed in Jim’s plots to make comparisons easier.\n\n\ntoCompare <- c(\"colder than mars\", \"colder than a witch's tit\", \"cold as heck\", \"colder than a mf\", \"cold as a bitch\", \"cold as balls\", \"cold as tits\", \"cold as fuck\", \"colder than my heart\", \"colder than a bitch\", \"cold as a mf\", \"cold as hell\", \"cold as ice\", \"hot as heck\", \"hotter than two rats\", \"hot as balls\", \"hotter than hell\", \"hot as tits\", \"hot as hell\", \"hot as fuck\", \"hot as shit\", \"hotter than satan's asshole\", \"hot as a mf\", \"hot as a bitch\", \"hotter than a mf\", \"hot as hades\", \"hot as dick\")\n\n\n\nWe can use fct_other to lump all the other phrases into an “Other” category.\n\n\ndata$phrase <- fct_other(data$phrase, keep = toCompare)\n\n\n\nSome regions don’t have every phrase, but I want that to be explicit in the data, so we break out our new friend tidyr. You can read more about my adventures with tidyr here.\n\n\ndata <- left_join(expand(data, phrase, region), data, c(\"phrase\" = \"phrase\", \"region\" = \"region\"))\n\nres <- data %>%\n  group_by(phrase, region) %>%\n  summarise(medAppT = median(apparentTemperature, na.rm = T), count = n())\n\ndata %>%\n  group_by(phrase) %>%\n  summarise(medAppT = median(apparentTemperature, na.rm = T), count = n()) %>%\n  arrange(medAppT)\n\n\n# A tibble: 28 x 3\n   phrase                    medAppT count\n   <fct>                       <dbl> <int>\n 1 colder than mars             13.4     8\n 2 colder than a witch's tit    24.0    28\n 3 cold as heck                 24.8    35\n 4 colder than a mf             33.2    15\n 5 cold as balls                35.4    62\n 6 cold as a bitch              37.6    41\n 7 cold as fuck                 40.3   545\n 8 cold as tits                 41.4    23\n 9 cold as a mf                 43.1    23\n10 colder than my heart         43.2    16\n# … with 18 more rows\n\nSuccess! We match Jim’s values. I can try to make a quick plot to assess differences per region.\n\n\nres2 <- subset(res, !is.na(region) & phrase != \"Other\")\nggplot(data = res2, aes(x = phrase, y = medAppT, col = region)) +\n  geom_point() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nBut my ordering doesn’t match Jim’s so it’s hard to compare. He ordered them by overall median, so I can compute that and then use fct_reorder to rearrange. But first I need to get rid of the pesky “Other” category using fct_drop.\n\n\ndata2 <- data %>% filter(phrase != \"Other\")\ndata2$phrase <- fct_drop(data2$phrase) ## defaults to drop what isn't present\n\ntoOrder <- data2 %>%\n  group_by(phrase) %>%\n  summarise(medAppT = median(apparentTemperature, na.rm = T), count = n()) %>%\n  arrange(medAppT) %>%\n  mutate(order = 1:nrow(.))\n\nres2$phrase <- fct_drop(res2$phrase) ## drop before merging so levels match\n\nres2 <- left_join(x = res2, y = toOrder, by = c(\"phrase\" = \"phrase\"))\n\nres2$phrase <- fct_reorder(res2$phrase, res2$order)\n\nggplot(data = res2, aes(x = phrase, y = medAppT.x, col = region)) +\n  geom_point() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  ylab(\"median apparent temp (F)\") +\n  xlab(\"\")\n\n\n\n\nThere we go! But it is still hard to see what is going on. The south and west have larger temperatures for cold and hot categories which makes sense. But how much do the differences in number of tweets by region affect the median? Regional differences in phrase usage could potentially skew even a median.\n\n\n## drop levels\ndata2 <- data %>% filter(phrase != \"Other\")\ndata2$phrase <- fct_drop(data2$phrase, only = \"Other\")\n## I would like to be able to do this in one step?\n\n## reorder\ndata2 <- left_join(x = data2, y = toOrder, by = c(\"phrase\" = \"phrase\"))\ndata2$phrase <- fct_reorder(data2$phrase, data2$order)\n\n## add on medians in black\nggplot(data = filter(data2, !is.na(region)), aes(x = phrase, y = apparentTemperature, col = region)) +\n  geom_point(alpha = .5) +\n  geom_point(aes(x = phrase, y = medAppT), cex = 2, col = \"black\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ylab(\"apparent temperature (F)\")\n\n\n\n\nEven with some transparency in the points, we can’t see what may be affecting the medians (shown in black). Jittering on the categorical scale to the rescue!\n\n\nggplot(data = filter(data2, !is.na(region)), aes(x = phrase, y = apparentTemperature, col = region)) +\n  geom_point(alpha = .5, position = position_jitter(w = .25, h = 0)) +\n  geom_point(aes(x = phrase, y = medAppT), cex = 2, col = \"black\") +\n  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ylab(\"apparent temperature (F)\")\n\n\n\n\nThere are no immediately obvious clusters by region in the temperature direction per phrase, so the median seems like a reasonable choice. However, we can see how the mean could be affected by values that differ from the majority per phrase.\nActually drawing conclusions is beyond the scope of this post but questions remain:\nDo particular regions use certain phrases more than we would expect by proportion of people/Twitter users?\nWithin phrases that are common across all regions, are there statistically and practically significant differences between the average temperature when a phrase is used?\nI almost made it through this post without a gif (the horror!).\n\nFeedback, questions, comments, etc. are welcome (@sastoudt).\nP.S. @AmeliaMN and @askdrstats have a great paper about Wrangling Categorical Data in R if you want some serious guidance on factors.\n\n\n\n",
    "preview": "posts/2021-06-05-cursing-with-factors/cursing-with-factors_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-06-05T14:36:54-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-apply-users-to-purrr/",
    "title": "A Tale of Two Kitties: Two apply Users Convert to purrr",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-08-15",
    "categories": [],
    "contents": "\nFor this post about purrr, I had help from Kellie Ottoboni (@kellieotto) since we both wanted to update our skills. We both took some code from our own research and converted it from its original form (usually with variants of apply) to variants of map. This post talks about that experience, pitfalls we ran into, and cool tricks we learned.\nSara’s Adventure in the Tidyverse\nThe goal for the code I’m revamping is to simulate data under a variety of scenarios where various parameters change. Within each set of parameters, we want to repeat the simulation multiple times (here KK) so that we can quantify our uncertainty. changeHelper simulates the data and analyzes it.\nIt’s easier to see what is going on if I start with the purrr version because the original code was A MESS.\nNote: I’m showing pseudocode because this is part of a collaborative project that hasn’t been released yet.\n\n\nrequire(abind)\n\nchangeSim <- function(params, KK){\n\n  ## for each set of parameters, repeat the simulation KK times\n  replicateR <- map(rep(list(params), times = KK), changeHelper)\n  \n  ## changeHelper returns a list with two elements: betaStore and lambdaStore\n  replicateRb <- map(replicateR, ~.x$betaStore)\n  replicateRl <- map(replicateR, ~.x$lambdaStore)\n\n  ## or use transpose instead\n  #output <- replicateR %>% transpose\n  \n  ## organize the output so I can easily process it to make plots\n  return(list(betas = abind(replicateRb, along = 0), lambdas = abind(replicateRl, along = 0)))\n  #return(list(betas = abind(output$betaStore, along = 0), lambdas = abind(output$lambdaStore, along = 0)))\n}\n\n\n\nNot only does purrr help simplify running the simulations, it also helps me set up the possible values of parameters. I want to keep most aspects of the simulation the same but change one aspect at a time.\n\n\nbetaOpt <- seq(0.1,3,by=.3)\nN <- 100\n\n## get list of parameter sets\n## the variables in toChange are the things I'm varying in this case study\ninputChangeBetaSim <- map(betaOpt,\n                          ~ list(toChange = list(v = rep(0.2, times = N), lambda = 0.5, beta =.x),    \n                              predVT= T, respVT = T, N = N, mu = 5, var = 1.3, alpha = -0.2, n = rpois(N,10), sigma = 1))\n\n## apply list of parameter sets to the simulation function\nchangeBetaR <- map(inputChangeBetaSimLL, changeSim, KK)\n\n## get the separate pieces that I need out of the output\nbetas <- map(changeBetaR, ~ .x$betas)\nlambdas <- map(changeBetaR, ~ .x$lambdas)\n\n## or we could use transpose\noutput <- changeBetaR %>% transpose\n\n## transpose goes from this:\nchangeBetaR[[1]]$betas\nchangeBetaR[[1]]$lambdas\n\n## to this:\noutput$betas[[1]]\noutput$lambda[[1]]\n\n\n\nIt is possible to get a simulated data set that give the analysis method trouble. To avoid having one error ruin a whole map statement, we can use possibly to fill in blank output if we run into an error in the analyzeData function. This is an alternative to tryCatch and (in my opinion) is easier to remember.\n\n\ncarefulAnalyzeData=possibly(analyzeData,list(betaStore=NA,lambdaStore=NA)) ## an alternative to tryCatch\n\nchangeHelper <- function(params){\n## simulate data\n  data=simData(params)\n## analyze simulated data\n  #results = analyzeData(data)\n  \n  #results=tryCatch(analyzeData(data),error=function(e){list(betaStore=NA,lambdaStore=NA)})\n  results=carefulAnalyzeData(data)\n  return(results)\n}\n\n\n\nTo improve performance, I can choose the slowest map statement and run it in parallel using furrr (akin to mclapply).\nNote: I’m doing this on a Mac, so the plan code may differ on a PC.\n\n\nrequire(furrr) # future purrr\nplan(cluster,workers=makeCluster(2))\n#plan(multiprocess) # will use all available cores\n\nchangeBetaR <- future_map(inputChangeBetaSimLL, changeSim, KK)\n\n\n\nNow on to the original version. Brace yourself!\nOriginally, all but the parameter I was changing was hard coded into a file that I sourced in changeHelper. This required separate functions depending on which parameter I was changing, and if I wanted to change the other aspects of the simulation, I had to make sure I changed the value everywhere so the results would be comparable. You can imagine how this worked out.\n\n\n\nchangeSim <- function(betaOpt, KK){\n    betaOptR <- lapply(rep(betaOpt, times = KK), changeBetaHelper)\n    betaOptRb <- lapply(betaOptR, function(x){x$betaStore})\n    betaOptRl <- lapply(betaOptR, function(x){x$lambdaStore})\n    return(list(betas = abind(betaOptRb, along=0),lambdas = abind(betaOptRl, along = 0)))\n}\n\n\nchangeBetaR <- lapply(betaOpt, changeBeta, KK)\nbetas <- lapply(changeBetaR, function(x){x$betas})\nlambdas <- lapply(changeBetaR, function(x){x$lambdas})\n\n\n\nTo be fair this used to all be loops, so making the step towards lapply was progress.\nRecapping what changed:\nlapply –> map\nfunction(x){x$var} –> ~.x$var\nhard coded aspects requiring many different source files –> use map to generate sets of parameters where only a few things change\ntranspose to re-organize the two outputs\nVerdict\nOverall, I found the transition to purrr more straightforward than the transition to other packages in the tidyverse that I’ve learned so far. It definitely helped that I had forced myself to use the apply family of functions more often leading up to this. I wasn’t expecting the extra benefit of possibly and transpose but I can definitely see myself using them into my work moving forward.\nOne annoying thing I ran into is that my R session got confused between map in the maps package and map in purrr. I ended up just using purrr::map to avoid any issues, but if I would have just required purrr after maps, the default map would be the purrr one.\nKellie’s Adventure in the Tidyverse\nI’ve rewritten some code that I used to analyze a dataset from a clinical trial. The data needed to be reshaped before it could be pumped into the usual linear model functions. There were seven clinical endpoints, each of which we analyzed individually to determine whether two treatments differed in their effect on patients’ GERD symptoms. Originally, I looped over each variable using apply.\nThe original code is here if you want to take a look. First, let’s read in the data and have a look at it.\n\n\nrequire(grid)\nrequire(gridExtra)\nlibrary(tidyverse)\ndatafile <- \"https://raw.githubusercontent.com/kellieotto/ancova-permutations/master/data/clinical_cleaned.csv\"\nclinical <- read.csv(datafile, header = TRUE, stringsAsFactors = FALSE)\nhead(clinical)\n\n\n  SUBJID SITEID VISITNUM tr country heart_sev regurg_sev dysp_sev\n1      1      1        1  A       1  1.428571   1.428571 1.571429\n2      1      1        2  A       1  1.428571   1.285714 2.142857\n3      2      1        1  B       1  2.714286   2.571429 2.000000\n4      2      1        2  B       1  2.285714   2.285714 1.857143\n5      3      1        1  A       1  2.000000   1.857143 1.714286\n6      3      1        2  A       1  1.857143   1.285714 1.142857\n  heart_freq regurg_freq dysp_freq daily_heart daily_regurg\n1   3.642857    4.285714  3.928571   0.8000000    0.8149357\n2   3.642857    3.428571  5.571429   0.7831171    0.6688314\n3   8.142857    7.500000  5.071429   1.9732257    1.7942057\n4   8.571429    8.071429  5.642857   1.6983043    1.6619400\n5   5.500000    1.285714  3.785714   1.2353657    0.5285714\n6   3.428571    1.142857  2.214286   1.0000000    0.3785714\n  daily_hrdq daily_dysp\n1   1.614936  0.9000000\n2   1.451949  1.3883129\n3   3.767431  1.2201314\n4   3.360239  1.2595414\n5   1.763937  0.8441571\n6   1.378571  0.5428571\n\nWe have measurements at two timepoints for each patient, and we want to control for the first measurement as a covariate in the regression model that estimates the effect of treatment on the outcome. To do this in R, the values need to be in separate columns. We need to reshape the data, hooray! For whatever reason, this is something I have a lot of trouble with conceptually. This step took me longer than anything involving purrr…\nYou get a bonus tidyverse example: in this function, I changed old reshape2 code to use tidyr instead. The function turns the data from long into wide format and selects only the relevant columns. The result is a dataframe for the clinical endpoint of interest, with one baseline column and an outcome column, in addition to other variables to put in the model. Previously, this function did something hacky to pick out the variable of interest, then used the dcast function to reshape. The reshape2 package used formula syntax to specify keys and values; I find the syntax of tidyr::spread to be more straightforward.\n\n\n# Group the data to treat person (SUBJID) and first/second visit (VISITNUM) as a single observation\ndata_by_subjid_visitnum <- clinical %>% group_by(SUBJID, VISITNUM)\n\nreshape_data <- function(variable){\n  # Reshape data to be analyzed with regression\n  # Inputs:\n  # variable = the clinical endpoint of interest, input as a string\n  # Output:\n  # A dataframe with a single row per subject and columns for treatment, site ID, baseline + outcome measures\n  cleaned <- data_by_subjid_visitnum %>%\n    select_(\"VISITNUM\", \"SUBJID\", \"tr\", \"SITEID\", variable) %>% # Pull out the columns we want\n    tidyr::spread_(key = \"VISITNUM\", value = variable) # Turn VISITNUM into columns, with value equal to measure\n  colnames(cleaned) <- c(\"SUBJID\", \"tr\", \"SITEID\", \"Baseline\", \"Outcome\")\n  cleaned <- ungroup(cleaned) %>% \n    mutate(difference = Outcome - Baseline) # Ungroup and create a difference column\n  return(cleaned)\n}\n\n\n\nNow, instead of applying this function to each of the seven clinical endpoints, I used purrr::map. So clean!\n\n\n# Reshape each variable, store in a list\ncontinuous_vars <- c(\"daily_heart\", \"daily_regurg\", \"daily_dysp\", \n                     \"daily_hrdq\", \"heart_freq\", \"regurg_freq\", \"dysp_freq\")\nreshaped_data <- continuous_vars %>% map(reshape_data)\nhead(reshaped_data[[1]])\n\n\n# A tibble: 6 x 6\n  SUBJID tr    SITEID Baseline Outcome difference\n   <int> <chr>  <int>    <dbl>   <dbl>      <dbl>\n1      1 A          1     0.8    0.783    -0.0169\n2      2 B          1     1.97   1.70     -0.275 \n3      3 A          1     1.24   1        -0.235 \n4      4 B          1     1.54   1.54      0     \n5      5 A          1     1.21   1.11     -0.0935\n6      6 B          1     1.16   0.913    -0.243 \n\nNow I want to visualize the distribution of these variables in each treatment group. Same idea: I’m going to make the same kind of plot for each clinical endpoint by writing a plot function that I run for each variable. Previously, I used apply. Now, I want to loop over the reshaped data and the variable names (so each plot is labeled appropriately). I used map2 for this: it just maps a function over two vectors of paired arguments. The first vector is the datasets and the second vector is the variable names.\n\n\n# Plot the distribution of Outcome for each variable\n\nplot_distributions <- function(dataset, varname){\n  p <- dataset %>% \n    mutate(tr = factor(tr)) %>%\n    ggplot(aes(Outcome)) +\n    geom_density(alpha = 0.6, aes(fill = tr))+ \n    labs(x = varname, fill = \"Treatment\") +\n    theme_bw() +\n    theme(\n      axis.text.x = element_text(size = 12),\n      axis.text.y = element_text(size = 12),\n      axis.title = element_text(size = 16),\n      title = element_text(size = 16),\n      legend.title = element_text(size = 12),\n      legend.text = element_text(size = 14),\n      strip.text.x = element_text(size = 12)\n    ) \n  return(p)\n}\n\n# Create a list containing a plot for each variable\nall_plots <- map2(reshaped_data, continuous_vars, plot_distributions)\n\n\n\nOk, now a tricky step. Each plot has its own legend. I want to put all seven plots in one figure and just use one legend. Basically, you can grab just the legend element from one of the plots, remove it from all of the plots, then slap that saved legend wherever you want. That’s what I do below, again using map to set legend.position=\"none\" on each of the plots. I put the seven plots in a 2x4 grid, then put the legend in the open 8th spot.\n\n\n# Move the legend\ntmp <- ggplot_gtable(ggplot_build(all_plots[[1]]))\nleg <- which(sapply(tmp$grobs, function(x) x$name) == \"guide-box\")\nlegend <- tmp$grobs[[leg]]\n\n# Remove the legend from each plot, then stick it at the end of the plot list\nall_plots <- all_plots %>% map(function(x) x + theme(legend.position=\"none\"))\nall_plots[[length(all_plots)+1]] <- legend\ndo.call(grid.arrange, c(all_plots, nrow = 2))\n\n\n\n\nVerdict\nIncorporating pipes in my code has made things infinitely more legible, and purrr::map just makes things even clearer. Throwing an apply in the middle of a pipe sequence kind of breaks the flow of reading from left to right. I think I will keep using map going forward!\npurrr has so many more capabilities than just map and map2, though. I didn’t have the opportunity to explore all those other functions in this code, and to be frank I can’t think of examples from my work that would require using more complicated mapping functions. However I’m sure they are super useful.\nWe did it!\n\nFeedback, questions, comments, etc. are welcome (@sastoudt @kellieotto). Tell us about your own purrr conversion!\n\n\n\n",
    "preview": "posts/2021-06-05-apply-users-to-purrr/apply-users-to-purrr_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-06-05T14:33:33-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-gender-neutral-letters-stringr/",
    "title": "Gender Neutral Letters of Recommendation with stringR",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-06-16",
    "categories": [],
    "contents": "\n\n\nrequire(reticulate)\nrequire(stringr)\nrequire(babynames)\n\n\n\nThere are lots of functions in stringR that improve upon base R equivalents for string processing. I’m not going to go through all the functionality, but at the end of the post, after the main attraction, I’ll go through examples in the stringR documentation and pick out the ones that seem handiest for scenarios I have run into where base R has been found wanting.\nGender Neutral Letters of Rec.\nNow for the star of the show. I want to be able to read in the text of a letter of recommendation and make all the pronouns gender neutral. I found a template here for a letter of recommendation that we will use. Since names are often a give-away of gender, I also would like to replace any names with “Student”. I’m going to change the generic name to “Sara” to test this out. Note that originally I wanted to replace with the less awkward “my student”, but then I would have to worry about whether “my” should be capitalized or not depending on the position of the name in the sentence.\n\n\nletter=[1238 chars quoted with '\"']\n\n\n\nNames\nIf the candidate has a name that has at least 5 uses in the United States, we can use the babynames package to locate it and replace it. This approach has limitations for international names. str_detect only searches for one pattern. We don’t want to search for every single name one at a time. Instead, I’m going to find each capitalized word in the letter of recommendation and use it as the pattern to look for in the babynames. This is somewhat wasteful because the first word of every sentence is capitalized, but for now, I don’t want to have to deal with deciding whether or not each word comes directly after a punctuation mark.\n\n\nhead(unique(babynames$name))\n\n\n[1] \"Mary\"      \"Anna\"      \"Emma\"      \"Elizabeth\" \"Minnie\"   \n[6] \"Margaret\" \n\n#https://stackoverflow.com/questions/21781014/remove-all-line-breaks-enter-symbols-from-the-string-using-r\nnolines=str_replace_all(letter,\"[\\n]\",\" \")\nnolines=str_replace_all(nolines,fixed(\"[\"),\"\\\\[\")\nnolines=str_replace_all(nolines,fixed(\"]\"),\"\\\\]\")\nwords=str_split(nolines,\" \")\nwords=words[[1]]\ncapitalized=unique(words[str_detect(words,\"[A-Z]\")])\n\n\n\nThe following is not restrictive enough since the words can be part of a name.\n\n\nisName=lapply(capitalized,function(x){sum(str_detect(unique(babynames$name),x))})\ncapitalized[which(unlist(isName)>0)] ## not restrictive enough\n\n\n[1] \"Dear\" \"Sara\" \"I\"    \"She\"  \"Her\"  \"As\"   \"Best\"\n\nThe following is too restrictive:\n\n\nisName=lapply(capitalized,function(x){sum(str_detect(unique(babynames$name),paste(x,\"\\\\>\",sep=\"\")))})\ncapitalized[unlist(isName)]\n\n\ncharacter(0)\n\nBut this is weird. This tells me that “\\>” is the regular expression for the pattern being found at the end of the word.\n\n\nsum(str_detect(unique(babynames$name),\"Sara\\\\>\")) ## default is regex so not looking for that actually\n\n\n[1] 0\n\nsum(str_detect(unique(babynames$name),regex(\"Sara\\\\>\"))) ## default is regex so not looking for that actually\n\n\n[1] 0\n\nThis works using grepl. Can someone please explain this to me? I thought it might have to do with default engines, but I couldn’t find much information on the base R default beyond a note in the “performance consideration section” here.\n\n\nsum(grepl(\"Sara\\\\>\",unique(babynames$name)))\n\n\n[1] 1\n\nHere is a hack:\n\n\nallnames=str_flatten(unique(babynames$name),\" \") \nisName=lapply(capitalized,function(x){str_detect(allnames,paste(\" \",x,\" \",sep=\"\"))})\ncapitalized[unlist(isName)]\n\n\n[1] \"Sara\" \"She\"  \"Her\" \n\nThis is annoying. These are actually names.\n\n\nunique(babynames$name)[which(grepl(\"She\\\\>\",unique(babynames$name)))]\n\n\n[1] \"She\"\n\nunique(babynames$name)[which(grepl(\"Her\\\\>\",unique(babynames$name)))]\n\n\n[1] \"Her\"\n\nI’ll just create an exception list.\n\n\nexception=c(\"She\",\"Her\") ## may need to add more as we experience more weird things\n\n\n\nPick names to replace. Note we don’t have to worry about “Sara’s” because we will still replace the “Sara” portion with “Student”.\n\n\nnamesToReplace=setdiff(capitalized[unlist(isName)],exception)\n\n\n\nDo the replacing. I don’t want to use a loop but I need to continually update words. Any suggestions? Will walk in purrr do this?\n\n\n### need to go through everything in namesToReplace but resave every time\n\nfor(i in 1:length(namesToReplace)){\n  words=str_replace_all(words,namesToReplace[i],\"Student\")\n}\n\n\n\nPronouns\nIdeally, we could just change everything to the gender neutral singular they/them. However this would require us to change the verbs. Instead we will use “s/he”, while recognizing that this binary is not fully inclusive.\nAgain the mystery of different syntax for anchors comes up:\n\n\nwhich(str_detect(words,\"^She\\\\>\")>0)\n\n\ninteger(0)\n\ngrep(\"^She\\\\>\",words)\n\n\n[1]  51  89 116\n\ngrep(\"^She$\",words)\n\n\n[1]  51  89 116\n\n\n\n#words=  str_replace_all(words,\"^She\\\\>\",\"S/He\") ## doesn't work\nwords=  str_replace_all(words,\"^She$\",\"S/He\")\n\n\n\n\n\nwords=  str_replace_all(words,\"^she$\",\"s/he\")\nwords= str_replace_all(words,\"^he$\",\"s/he\")\nwords=  str_replace_all(words,\"^He$\",\"S/He\")\n\n## need possessives\nwords=  str_replace_all(words,\"^She's$\",\"S/He's\")\nwords=  str_replace_all(words,\"^she's$\",\"s/he's\")\nwords= str_replace_all(words,\"^he's$\",\"s/he's\")\nwords=  str_replace_all(words,\"^He's$\",\"S/He's\")\n \n\n words=  str_replace_all(words,\"^hers\\\\>\",\"theirs\") ## shouldn't be first so no capitalization\n words= str_replace_all(words,\"^him\\\\>\",\"them\") ## shouldn't be first\n \n str_flatten(words,\" \")\n\n\n[1] \"Dear Mr./Mrs./Ms. \\\\[Last Name\\\\],  It’s my absolute pleasure to recommend Student for \\\\[position\\\\] with \\\\[Company\\\\].  Student and I \\\\[relationship\\\\] at \\\\[Company\\\\] for \\\\[length of time\\\\].  I thoroughly enjoyed my time working with Student, and came to know her as a truly valuable asset to absolutely any team. S/He is honest, dependable, and incredibly hard-working. Beyond that, s/he is an impressive \\\\[soft skill\\\\] who is always \\\\[result\\\\].  Her knowledge of \\\\[specific subject\\\\] and expertise in \\\\[specific subject\\\\] was a huge advantage to our entire office. S/He put this skillset to work in order to \\\\[specific achievement\\\\].  Along with her undeniable talent, Student has always been an absolute joy to work with. S/He is a true team player, and always manages to foster positive discussions and bring the best out of other employees.  Without a doubt, I confidently recommend Student to join your team at \\\\[Company\\\\]. As a dedicated and knowledgeable employee and an all-around great person, I know that s/he will be a beneficial addition to your organization.  Please feel free to contact me at \\\\[your contact information\\\\] should you like to discuss Student’s qualifications and experience further. I’d be happy to expand on my recommendation.  Best wishes, \\\\[Your Name\\\\]  \"\n\nNow because English is weird we have a problem. How do we distinguish between the following examples?\nThat is hers. –> theirs\nThat is his. —> theirs\nThat is his experience. —> their\nThat is her experience. —> their\nGet to know her. —> them\nGet to know him. —> them\nNumbers 1 and 6 are not ambigous, so we can fix those.\n\n\n words= str_replace_all(words,\"^hers$\",\"theirs\") \n words= str_replace_all(words,\"^him$\",\"them\") \n \ntoParse=  str_flatten(words,\" \")\n#toParse=r_to_py(toParse)\n\n\n\nTo distinguish between 2 and 3 and 4 and 5, we need to automatically determine what part of speech the words are.\n\n\n his=which(str_detect(words,\"^his$\"))\n\n His=which(str_detect(words,\"^His$\"))\n\n her=which(str_detect(words,\"^her$\"))\n\n Her=which(str_detect(words,\"^Her$\"))\n\n \n toChange=c(his,His,her,Her)\n\n\n\nNatural Language Processing\n\nBet you didn’t expect to see NLP when you clicked on this post. Apparently we need a part of speech (POS) tagger to tell us what type of word each is in a sentence.\nBoth the R packages I found to do this had rJava issues.\n\n\n## not run\nrequire(openNLP)\ndevtools::install_github(\"bnosac/RDRPOSTagger\")\n\n\n\nI guess now is the time to learn some reticulate basics.\n\nPython package that I need\nTell R where my Python is\nTell R where my Python is Pt 2\nMissing some downloads Pt 2\nMissing some downloads Pt 3\nHow to see Python output in RMarkdown\nHow to see Python output in RMarkdown Pt 2\nI tried to use r_to_py to pass in toParse, but was having trouble (see commented out code), so for now, I’m just copying the contents of toParse into this chunk. Can somebody please point me to an example of getting an R object to Python in Markdown?\n\nimport nltk\ntext = nltk.word_tokenize(\"Dear Mr./Mrs./Ms. [Last Name],  It’s my absolute pleasure to recommend Student for [position] with [Company].  Student and I [relationship] at [Company] for [length of time].  I thoroughly enjoyed my time working with Student, and came to know her as a truly valuable asset to absolutely any team. S/He is honest, dependable, and incredibly hard-working. Beyond that, s/he is an impressive [soft skill] who is always [result].  Her knowledge of [specific subject] and expertise in [specific subject] was a huge advantage to our entire office. S/He put this skillset to work in order to [specific achievement].  Along with her undeniable talent, Student has always been an absolute joy to work with. S/He is a true team player, and always manages to foster positive discussions and bring the best out of other employees.  Without a doubt, I confidently recommend Student to join your team at [Company]. As a dedicated and knowledgeable employee and an all-around great person, I know that s/he will be a beneficial addition to your organization.  Please feel free to contact me at [your contact information] should you like to discuss Student’s qualifications and experience further. I’d be happy to expand on my recommendation.  Best wishes, [Your Name]\")\ntest=nltk.pos_tag(text)\n#test=nltk.pos_tag(toParse)\n\n\n\nprint(py$test[[1]])\n\n\n[[1]]\n[1] \"Dear\"\n\n[[2]]\n[1] \"NNP\"\n\nwordsPy=unlist(lapply(py$test,function(x){x[[1]]}))\n\nhisPy=which(str_detect(wordsPy,\"^his$\"))\nHisPy=which(str_detect(wordsPy,\"^His$\"))\n\nherPy=which(str_detect(wordsPy,\"^her$\"))\nHerPy=which(str_detect(wordsPy,\"^Her$\"))\n\ntoGet=c(hisPy,HisPy,herPy,HerPy)\n\npos=unlist(lapply(toGet,function(x){py$test[[x]][[2]]}))\npos\n\n\n[1] \"PRP\"  \"PRP$\" \"PRP$\"\n\nAccording to the key here:\nPRP: pronoun, personal (case 5)\nPRP$: pronoun, possessive (case 4)\nSo now we can determine what to replace them with. Bear with this loop please.\n\n\nfor(i in 1:length(pos)){\n  if(pos[i]==\"PRP\"){\n     words[toChange[i]]=\"them\"\n     \n  }else if(pos[i]==\"PRP$\"&str_detect(words[toChange[i]],\"[A-Z]\")){\n     words[toChange[i]]=\"Their\"\n  }else if(pos[i]==\"PRP$\"&!str_detect(words[toChange[i]],\"[A-Z]\")){\n  words[toChange[i]]=\"their\"\n  }\n}\n\n\n\nFinally, we can take away the extra escape characters to get back to the original.\n\n\n words= str_replace_all(words,fixed(\"\\\\[\"),\"[\") \n words= str_replace_all(words,fixed(\"\\\\]\"),\"]\") \n\n\nstr_flatten(words,\" \")\n\n\n[1] \"Dear Mr./Mrs./Ms. [Last Name],  It’s my absolute pleasure to recommend Student for [position] with [Company].  Student and I [relationship] at [Company] for [length of time].  I thoroughly enjoyed my time working with Student, and came to know them as a truly valuable asset to absolutely any team. S/He is honest, dependable, and incredibly hard-working. Beyond that, s/he is an impressive [soft skill] who is always [result].  Their knowledge of [specific subject] and expertise in [specific subject] was a huge advantage to our entire office. S/He put this skillset to work in order to [specific achievement].  Along with their undeniable talent, Student has always been an absolute joy to work with. S/He is a true team player, and always manages to foster positive discussions and bring the best out of other employees.  Without a doubt, I confidently recommend Student to join your team at [Company]. As a dedicated and knowledgeable employee and an all-around great person, I know that s/he will be a beneficial addition to your organization.  Please feel free to contact me at [your contact information] should you like to discuss Student’s qualifications and experience further. I’d be happy to expand on my recommendation.  Best wishes, [Your Name]  \"\n\nI thought this would be a quick, cute thing, but I was SO wrong; it turned into a mess. But it finally works!!\n\nMiscellaneous stringR Capabilities\ntoupper and tolower have equivalents in stringR, but stringR also has a function to make things like a title. This can come in handy for example, when you need state names to start with a capital later for facet_geo.\n\n\nstates<-c(\"pennsylvania\",\"massachusetts\",\"maryland\",\"california\")\n\n#str_to_upper ## toupper\n#str_to_lower ## tolower\nstr_to_title(states) ## this format needed for geofacet\n\n\n[1] \"Pennsylvania\"  \"Massachusetts\" \"Maryland\"      \"California\"   \n\nA period matches any character in a regular expression, but sometimes you want to search for acutal periods. You can use fixed in stringR functions to do this without having to remember escape characters. Apparently, base R string functions have a fixed parameter as well, but I wasn’t aware of it before now.\n\n\npattern<-\"a.b\" \nstrings<-c(\"abb\",\"a.b\")\nstr_detect(strings,pattern)\n\n\n[1] TRUE TRUE\n\nstr_detect(strings,fixed(pattern)) \n\n\n[1] FALSE  TRUE\n\nUsing boundary you can split on words and allow for inconsistent spacing.\n\n\nwords<-c(\"These are   some words.\")\nstr_split(words,boundary(\"word\"))[[1]] ## character, line_break, sentence, word\n\n\n[1] \"These\" \"are\"   \"some\"  \"words\"\n\nI always put the wrong argument first in grep and grepl, but the stringR packages have the order of parameters that fit my expectation.\n\n\nstr_detect(fruit,\"a\") ## grepl(\"a\",fruit) \n\n\n\nI always forget how to concatenate a vector with a particular separation using paste.\n\n\nstr_flatten(letters,\"-\") \n\n\n[1] \"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o-p-q-r-s-t-u-v-w-x-y-z\"\n\npaste(letters,collapse=\"-\")\n\n\n[1] \"a-b-c-d-e-f-g-h-i-j-k-l-m-n-o-p-q-r-s-t-u-v-w-x-y-z\"\n\nglue related functions seem handy. This could be a whole other post, so I’ll save the details for later\n\n\nname <- \"Fred\"\nstr_glue(\"My name is {name}, not {{name}}.\")\n\n\nMy name is Fred, not {name}.\n\nmtcars %>% str_glue_data(\"{rownames(.)} has {hp} hp\") %>% head()\n\n\nMazda RX4 has 110 hp\nMazda RX4 Wag has 110 hp\nDatsun 710 has 93 hp\nHornet 4 Drive has 110 hp\nHornet Sportabout has 175 hp\nValiant has 105 hp\n\nstringR has fancier trimming functions.\n\n\nstr_trim(\" test \",side=\"both\") ## trimws\n\n\n[1] \"test\"\n\nstr_squish(\"\\n\\nString with excess,  trailing and leading white  space\\n\\n\")\n\n\n[1] \"String with excess, trailing and leading white space\"\n\nIf you have any insight into my remaining mysteries, please let me know!\nFeedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:33:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-tibble-troubles/",
    "title": "Troubles with Tibble",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-06-03",
    "categories": [],
    "contents": "\n\n\nrequire(readr)\nrequire(dplyr)\nrequire(ggplot2)\nrequire(tibble)\nrequire(data.table)\nrequire(gridExtra)\n\n\n\nNote: I’m skipping over purrr until @kellieotto returns from her travels, so we can write our joint post.\nThis week’s post is on tibbles. This actually came at a perfect time since recently I’ve run into a few mysteries where I get unexpected errors or output after a data frame gets turned into a tibble at some point during my workflow (like when I use functions from the tidyverse). This is actually a good sign because it means I’m using the tidyverse more in my day to day work. So now to solve some tibble troubles.\n\nMystery #1\nThis mystery comes from Week 7 of Tidy Tuesday, the dreaded barplots. I’m hiding the process code to streamline the post, but if you want to see it, go here. The gist of it, is that I want the order of the side by side bars to be consistent within category.\n\n\n\nThe key is to pre-arrange the data to follow the order that we want to plot it in.\ng1 doesn’t work, but g2, g3, and g4 do.\nKey: It turns out that the tibble format isn’t the issue per se, it is tibble’s lazy evaluation (and ggplot’s) that is the real issue. This is analogous to why you need aes_string instead of aes when you are passing in a variable name to ggplot in a custom made function. The rearranging doesn’t actually happen until wrapped in another function that evaulates it.\n\n\ntoPlot\n\n\n# A tibble: 47 x 7\n# Groups:   Gender, Age [8]\n   Gender Age   V28                 count.x count.y percent genderAge \n   <fct>  <fct> <fct>                 <int>   <int>   <dbl> <fct>     \n 1 Female > 60  Unfamiliar (N/A)         37      94  0.394  Female > …\n 2 Female > 60  Very unfavorably          2      94  0.0213 Female > …\n 3 Female > 60  Somewhat unfavorab…       1      94  0.0106 Female > …\n 4 Female > 60  Neither favorably …      24      94  0.255  Female > …\n 5 Female > 60  Somewhat favorably       16      94  0.170  Female > …\n 6 Female > 60  Very favorably           14      94  0.149  Female > …\n 7 Female 18-29 Unfamiliar (N/A)         10      85  0.118  Female 18…\n 8 Female 18-29 Very unfavorably          3      85  0.0353 Female 18…\n 9 Female 18-29 Somewhat unfavorab…       6      85  0.0706 Female 18…\n10 Female 18-29 Neither favorably …      18      85  0.212  Female 18…\n# … with 37 more rows\n\ntest <- toPlot %>% arrange(Age, V28)\nis.tibble(test)\n\n\n[1] TRUE\n\ntest2 <- as.data.frame(toPlot %>% arrange(Age, V28))\ntest3 <- data.frame(toPlot %>% arrange(Age, V28))\ntest4 <- as.tibble(toPlot %>% arrange(Age, V28))\n# test5=tibble(toPlot%>%arrange(Age,V28)) ## Error: Column `toPlot %>% arrange(Age, V28)` must be a 1d atomic vector or a list\n\n\ng1 <- ggplot(test, aes(V28, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\n\ng2 <- ggplot(test2, aes(V28, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\ng3 <- ggplot(test3, aes(V28, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\ng4 <- ggplot(test4, aes(V28, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\n\ng1\n\n\n\ng2\n\n\n\ng3\n\n\n\ng4\n\n\n\n\nMystery #2\nThis mystery comes from a scenario at work. I had one dataset that had different states than another dataset (state was treated as a factor). I wanted to find new levels and replace them with a catch all “other” level. I started with a data frame, but I used complete from tidyr at some point, and unknowingly had switched to a tibble. Therefore, my subsetting procedure was not doing what I expected. Here is a simplified example:\n\n\nstate <- tibble(state = as.factor(c(\"AL\", \"AK\", \"AR\", \"AS\")))\nstate2 <- tibble(state = as.factor(c(\"AL\", \"AK\", \"AR\", \"AS\", \"CA\", \"CO\")))\n\n\n\nWhat I did:\n\n\nstate2[, 1] %in% levels(state[, 1])\n\n\n[1] FALSE\n\nlevels(state[, 1])\n\n\nNULL\n\nWhat I should have did given that I’m working with tibbles:\n\n\ntest <- state2[[1]] %in% levels(state[[1]])\ntest\n\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\nKey: subsetting tibbles by column requires the double brackets\nWhat I did:\n\n\nstate2[, 1] <- as.character(state2[, 1])\nstate2\n\n\n# A tibble: 6 x 1\n  state              \n  <chr>              \n1 c(2, 1, 3, 4, 5, 6)\n2 c(2, 1, 3, 4, 5, 6)\n3 c(2, 1, 3, 4, 5, 6)\n4 c(2, 1, 3, 4, 5, 6)\n5 c(2, 1, 3, 4, 5, 6)\n6 c(2, 1, 3, 4, 5, 6)\n\nWhat I should have did given that I’m working with tibbles:\nKey: again this is a subsetting syntax issue\n\n\nstate2 <- tibble(state = as.factor(c(\"AL\", \"AK\", \"AR\", \"AS\", \"CA\", \"CO\")))\n\nstate2[[1]] <- as.character(state2[[1]])\nstate2[test, 1] <- \"other\"\n\n\n\nCompare to behavior on a dataframe (this is the output I expected):\n\n\nstate <- data.frame(state = as.factor(c(\"AL\", \"AK\", \"AR\", \"AS\")))\nstate2 <- data.frame(state = as.factor(c(\"AL\", \"AK\", \"AR\", \"AS\", \"CA\", \"CO\")))\n\nstate2[, 1] %in% levels(state[, 1])\n\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\nlevels(state[, 1])\n\n\n[1] \"AK\" \"AL\" \"AR\" \"AS\"\n\nstate2[, 1] <- as.character(state2[, 1])\nstate2\n\n\n  state\n1    AL\n2    AK\n3    AR\n4    AS\n5    CA\n6    CO\n\nstate2[test, 1] <- \"other\"\nstate2\n\n\n  state\n1 other\n2 other\n3 other\n4 other\n5    CA\n6    CO\n\nstate2\n\n\n  state\n1 other\n2 other\n3 other\n4 other\n5    CA\n6    CO\n\nMystery #3\nThis mystery comes from my readr post where I wanted to remove rows that had NAs in certain columns and a certain number of characters in another example. Here is a simplified example:\n\n\ntb <- tibble(x = c(NA, NA, 3), y = c(NA, 2, 1), z = c(\"A\", \"BC\", \"D\"))\ntb[, 1] ## tibble\n\n\n# A tibble: 3 x 1\n      x\n  <dbl>\n1    NA\n2    NA\n3     3\n\ntb[[1]] ## vector\n\n\n[1] NA NA  3\n\nThe difference in subsetting syntax is a reoccurring issue. I use the syntax expecting my data to be a dataframe, but I forgot that tidyverse functions switch to tibbles.\n\n\ntb[-which(is.na(tb[, 2]) & is.na(tb[, 1]) & nchar(tb[, 3]) <= 1), ]\n\n\n# A tibble: 0 x 3\n# … with 3 variables: x <dbl>, y <dbl>, z <chr>\n\nWeird! I expected the output to remove the first row. What’s going on?\n\n\nis.na(tb[, 2])\n\n\n         y\n[1,]  TRUE\n[2,] FALSE\n[3,] FALSE\n\nis.na(tb[, 1])\n\n\n         x\n[1,]  TRUE\n[2,]  TRUE\n[3,] FALSE\n\nThese are fine.\n\n\nnchar(tb[, 3]) <= 1\n\n\n    z \nFALSE \n\nnchar(tb[, 3])\n\n\n z \n17 \n\nAh, here is the culprit! Switching to tibble subsetting syntax…\n\n\nnchar(tb[[3]]) <= 1\n\n\n[1]  TRUE FALSE  TRUE\n\nMuch better.\nOriginal:\n\n\nis.na(tb[, 2]) & is.na(tb[, 1]) & nchar(tb[, 3]) <= 1\n\n\n         y\n[1,] FALSE\n[2,] FALSE\n[3,] FALSE\n\nFix:\n\n\nis.na(tb[, 2]) & is.na(tb[, 1]) & nchar(tb[[3]]) <= 1\n\n\n         y\n[1,]  TRUE\n[2,] FALSE\n[3,] FALSE\n\ntb[-which(is.na(tb[, 2]) & is.na(tb[, 1]) & nchar(tb[[3]]) <= 1), ]\n\n\n# A tibble: 2 x 3\n      x     y z    \n  <dbl> <dbl> <chr>\n1    NA     2 BC   \n2     3     1 D    \n\nCompare to behavior on a dataframe:\n\n\ntb <- tibble(x = c(NA, NA, 3), y = c(NA, 2, 1), z = c(\"A\", \"BC\", \"D\"))\ntb2 <- as.data.frame(tb)\n\ntb2[-which(is.na(tb2[, 2]) & is.na(tb2[, 1]) & nchar(tb2[, 3]) <= 1), ]\n\n\n   x y  z\n2 NA 2 BC\n3  3 1  D\n\nThis is the output I expected.\n\n\nis.na(tb2[, 2])\n\n\n[1]  TRUE FALSE FALSE\n\nis.na(tb2[, 1])\n\n\n[1]  TRUE  TRUE FALSE\n\nnchar(tb2[, 3]) <= 1\n\n\n[1]  TRUE FALSE  TRUE\n\nThe double bracket subsetting also works:\n\n\nis.na(tb2[[2]])\n\n\n[1]  TRUE FALSE FALSE\n\nis.na(tb2[[1]])\n\n\n[1]  TRUE  TRUE FALSE\n\nnchar(tb2[[3]]) <= 1\n\n\n[1]  TRUE FALSE  TRUE\n\n\n\nis.na(tb2[, 2]) & is.na(tb2[, 1]) & nchar(tb2[, 3]) <= 1\n\n\n[1]  TRUE FALSE FALSE\n\nis.na(tb2[[2]]) & is.na(tb2[[1]]) & nchar(tb2[[3]]) <= 1\n\n\n[1]  TRUE FALSE FALSE\n\nTake-Away: The unexpected behavior that led to most of these mysteries turned out to be because I was using the wrong subsetting syntax. \nSince the double bracket subsetting works for dataframes and tibbles, I should transition to using this syntax so that I am not surprised by output when a tibble gets thrown into the mix.\nResources (these helped me solve my mysteries):\nhttp://r4ds.had.co.nz/tibbles.html\nhttps://gist.github.com/jennybc/37481d9d784d2e8222b3\nhttps://www.rdocumentation.org/packages/ggplot2/versions/1.0.0/topics/aes_string\nhttp://adv-r.had.co.nz/Functions.html#function-arguments\nhttp://adv-r.had.co.nz/Computing-on-the-language.html\nFeedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": "posts/2021-06-05-tibble-troubles/tibble-troubles_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-06-05T14:39:09-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-usda-wrangling-with-readr/",
    "title": "Wrangling USDA Data with readr",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-26",
    "categories": [],
    "contents": "\nWhen I saw that this week’s blog post was supposed to be about readr I drew a blank on how to get my hands dirty using the functionality in the package. I didn’t want to use the same sample data in the documentation, but I also didn’t want to go scouring for a dataset that I wasn’t really motivated to munge. Then fate stepped in.\nAt work I wanted to get some data from the USDA that is not available through their API service. Many of the USDA reports come in .txt or .pdf files, and these files contain tables that have information that I needed. I literally spent days trying to get things organized (but I did it!). Even though it still took a long time (and I am not a patient person), I was grateful that I had perused the readr documentation ahead of time in preparation for writing this blog.\nI can’t go into too much detail about what I did at work, but instead, I’ll show a representative example of a USDA text file full of tables. This example is actually a bit gnarlier than the stuff I was dealing with, so this seems fair.\nGo ahead and look at this original. WHY?! Why must our government store data in a super inaccessible format?! As much as we complain about Excel, I would be grateful for a .xls file here.\n\nBut we must persevere.\nDisclaimer: I am not claiming that this is the best way to use readr to wrangle the information in these tables. I would love if someone had a less clunky approach they were willing to share.\nRead in using the default read_table\nI manually looked for the number of lines I could skip before getting to the good stuff.\n\n\nrequire(readr)\nrequire(dplyr)\nsetwd(\"~/Desktop\")\nraw <- read_table(\"usv1.txt\")\n\nhead(raw)\n\n\n# A tibble: 6 x 1\n  `United States`                                    \n  <chr>                                              \n1 \"Summary and State Data\"                           \n2 \"Volume 1 \\x95 Geographic Area Series \\x95 Part 51\"\n3 \"AC-12-A-51\"                                       \n4 \"Issued May 2014\"                                  \n5 \"United States Department of Agriculture\"          \n6 \"Tom Vilsack, Secretary\"                           \n\nrawSkip <- read_table(\"usv1.txt\", skip = 475)\n\n\n\ngrep for “Table” (or any other key words)\nNote: This is why I manually skipped over the table of contents, since it lists all the tables.\n\n\ntables <- which(unlist(lapply(rawSkip[, 1], function(x) {\n  grepl(\"Table\", x)\n})) == T)\n\n\n\nSimplify the problem further to start\nEven Table 1 has lots of components and weird structure, so let’s simplify to the smallest chunk in Table 1 that seems to stand alone.\n\n\ntoSave <- rawSkip[tables[1]:(tables[1] + 10), ] %>% as.data.frame()\ntoSave\n\n\n                                                                                                                           [For meaning of abbreviations and symbols, see introductory text.]\n1                                                                                                                 Table 1.  Historical Highlights:  2012 and Earlier Census Years (continued)\n2                                                                                                                          [For meaning of abbreviations and symbols, see introductory text.]\n3  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n4                                                                         :                :                :                :                :                     Not adjusted for coverage\n5                                                    :                :                :                :                :-------------------------------------------------------------------\n6                             All farms                    :      2012      :      2007      :      2002      :      1997      :      1997      :      1992      :      1987      :      1982\n7  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n8                                                                                                                                         Livestock and poultry: - Con.                     :\n9                                                                                                                                                                                           :\n10     Layers inventory (see text) ...............farms:        198,272          145,615          98,315             (NA)             (NA)             (NA)             (NA)             (NA)\n11                                               number:    350,715,978      349,772,508     334,435,155             (NA)             (NA)             (NA)             (NA)             (NA)\n\nTrim a bunch of extra characters using gsub\nI initially thought that some of these extra characters (like “:”) would help as delimiters, but because the first column is broken away from the rest in a different way, using the extra characters to help split things up didn’t end up working.\n\n\ntoSave[, 1] <- gsub(\":\", \" \", toSave[, 1])\ntoSave[, 1] <- gsub(\"\\\\.\", \"\", toSave[, 1])\n\n\n\nBecause white space acts as a delimiter everywhere except the first column, I wanted to replace the spaces between words in the first column with something else.\n\n\ncollapseNames <- function(x) {\n  # browser()\n  words <- unlist(strsplit(x, \" \")) ## get individual words\n  if (length(which(words == \"\")) > 0) {\n    toReturna <- paste(words[1:(which(words == \"\")[1] - 1)], collapse = \"_\") ## collapse the actual words in column 1\n    ## need to paste the rest\n    toReturnb <- paste(words[which(words == \"\")[1]:length(words)], collapse = \" \") ## paste on the info in the extra columns\n\n    toReturn <- paste(toReturna, toReturnb, sep = \" \")\n  } else {\n    toReturn <- x ## if can't be broken into words, just return the line\n  }\n  return(toReturn)\n}\n\ntoSave[, 1] <- unlist(lapply(toSave[1:nrow(toSave), 1], collapseNames))\ntoSave[5:10, 1]\n\n\n[1] \"                                                                      -------------------------------------------------------------------\"                                                 \n[2] \"All_farms                           2012             2007             2002             1997             1997             1992             1987             1982\"                           \n[3] \"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\"\n[4] \"Livestock_and_poultry  - Con                     \"                                                                                                                                         \n[5] \" \"                                                                                                                                                                                         \n[6] \"Layers_inventory_(see_text)_farms         198,272          145,615          98,315             (NA)             (NA)             (NA)             (NA)             (NA)\"                   \n\nSave a subset and read it back in (using read_table2) with a forced number of columns (using col_names)\nread_table2 “allows any number of whitespace characters between columns, and the lines can be of different lengths.”\nBy giving column names we can ensure that the desired number of columns is respected.\n\n\nsetwd(\"~/Desktop\")\nwrite.table(toSave, \"tmp.txt\", row.names = F, col.names = F)\nreadIn <- read_table2(\"tmp.txt\", skip = 5, col_names = paste(\"V\", 1:9, sep = \"\"))\nhead(readIn)\n\n\n# A tibble: 6 x 9\n  V1               V2      V3     V4     V5    V6    V7    V8    V9   \n  <chr>            <chr>   <chr>  <chr>  <chr> <chr> <chr> <chr> <chr>\n1 \"\\\"All_farms\"    \"2012\"  2007   \"2002\" 1997  1997  1992  1987  \"198…\n2 \"\\\"------------…  <NA>   <NA>    <NA>  <NA>  <NA>  <NA>  <NA>   <NA>\n3 \"\\\"Livestock_an… \"-\"     Con    \"\\\"\"   <NA>  <NA>  <NA>  <NA>   <NA>\n4 \"\\\"\"             \"\\\"\"    <NA>    <NA>  <NA>  <NA>  <NA>  <NA>   <NA>\n5 \"\\\"Layers_inven… \"198,2… 145,6… \"98,3… (NA)  (NA)  (NA)  (NA)  \"(NA…\n6 \"\\\"number\"       \"350,7… 349,7… \"334,… (NA)  (NA)  (NA)  (NA)  \"(NA…\n\nParse numbers (using parse_number) and remove empty rows\nWe convert strings that are clearly numbers into numbers (remove commas, etc.) using parse_number. This also has the added benefit of making filler strings into NA values in the columns where we expect numbers.\n\n\nreadIn[, 2:9] <- apply(readIn[, 2:9], 2, parse_number)\n\nreadIn <- readIn[-which(is.na(readIn[, 2]) & is.na(readIn[, 3])), ]\n\nreadIn\n\n\n# A tibble: 3 x 9\n  V1                   V2      V3     V4    V5    V6    V7    V8    V9\n  <chr>             <dbl>   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 \"\\\"All_farms\"    2.01e3  2.01e3 2.00e3  1997  1997  1992  1987  1982\n2 \"\\\"Layers_inve…  1.98e5  1.46e5 9.83e4    NA    NA    NA    NA    NA\n3 \"\\\"number\"       3.51e8  3.50e8 3.34e8    NA    NA    NA    NA    NA\n\nGet more ambitious…\nNow that we got things working for a subset of the table, let’s try to do the same thing for the rest of the document.\n\n\ntoSave <- rawSkip[tables[1]:nrow(rawSkip), ] %>% as.data.frame()\ntoSave[, 1] <- gsub(\":\", \" \", toSave[, 1])\ntoSave[, 1] <- gsub(\"\\\\.\", \"\", toSave[, 1])\ntoSave[, 1] <- unlist(lapply(toSave[1:nrow(toSave), 1], collapseNames))\n\n\n\n\n\nsetwd(\"~/Desktop\")\nwrite.table(toSave, \"tmp.txt\", row.names = F, col.names = F)\nreadIn <- read_table2(\"tmp.txt\", skip = 5, col_names = paste(\"V\", 1:9, sep = \"\"))\n\nreadIn[, 2:9] <- apply(readIn[, 2:9], 2, parse_number)\n\n\n\nDeal with non-ASCII characters and remove extra rows\nThe first column of this dataframe contained some non-ASCII characters which was giving the string related functions a hard time. We remove those and then get rid of some extra rows.\nThe resulting file still has some rather sparse rows, but it maintains the structure of the document rather well. We could now grep for the things we are interested in and easily get the numeric values associated with that chunk of the file.\n\n\n# https://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files\nreadIn[, 1] <- iconv(readIn[, 1], \"latin1\", \"ASCII\", sub = \"\")\nreadIn <- as.data.frame(readIn) ## tibble is giving me a hard time here\n\n\nreadIn <- readIn[-which(is.na(readIn[, 2]) & is.na(readIn[, 3]) & nchar(readIn[, 1]) <= 1), ]\n\n\n\nAn interesting dead end…\nFuture Pipe Dream: I wanted to make a tokenizer that would first try one delimiter, and if it didn’t split the line into the desired number of columns, then it would try the other one (using tokenizer_delim).\nI used read_delim_chunked to help understand how many columns a particular delimiter would produce. I still think designing better callbacks could help make a more flexible tokenizer, but further investigation will have to be saved for later (perhaps for a future post).\n\n\nf <- function(x, pos) {\n  length(which(!is.na(x)))\n} ## tell how many columns the data is actually put in\nf2 <- function(x, pos) {\n  x\n} ## show what the output will look like if we use this delimiter\n\nsetwd(\"~/Desktop\")\ntest <- read_delim_chunked(\"tmp.txt\", delim = \"   \", callback = DataFrameCallback$new(f), chunk_size = 1, col_names = paste(\"V\", 1:9, sep = \"\"))\n## want this to be 9\n\n# Note: the callback happens per chunk, so I made the chunk_size 1 to just get the answers per line\n\ntest2 <- read_delim_chunked(\"tmp.txt\", delim = \"   \", callback = DataFrameCallback$new(f2), chunk_size = 1, col_names = paste(\"V\", 1:9, sep = \"\"))\n\nhead(test)\n\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n[6,]    1\n\nhead(test2)\n\n\n# A tibble: 6 x 9\n  V1                   V2    V3    V4    V5    V6    V7    V8    V9   \n  <chr>                <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n1 \"Table_1  Historica… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n2 \"[For meaning of ab… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n3 \"------------------… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n4 \"                  … <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n5 \"                  … <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n6 \"All_farms         … <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n\nThis post took much more time than I anticipated, but it is reassuring that eventually we can make some sense of this poorly formatted data. Even though this approach seems specialized, there are files for other agricultural census years that I hope would at least keep a consistent, if gross, formatting approach, that we could repurpose this code for. I do wonder if we could do something more clever, perhaps just with readLines, so I’m open to other ideas.\nSidenote: If anyone has a similarly convoluted way to wrangle a particular type of government data, I would be curious to see the approach. It would be awesome if we could organize these approaches in one place. Even if they are hacky, it would help increase accessibility of data that hasn’t been API-ified yet.\nFeedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:44:01-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-grading-with-tidyr/",
    "title": "Grading with tidyr",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, I no longer could easily find my fake data file. I am using eval = F to preserve the post, but we will not be able to see output. I will try to find a copy of the data in my backup drive and fix at a later time.\nIt’s that time of year… The end of the semester means grading galore for professors and graduate student instructors. In this post, I will explore tidyr in the context of organizing and calculating grades.\nI have some familiarity with reshape2, but I always have to Google an example to remember how to go from wide to long format and vice versa. I’m hoping the tidyr functions will be more intuitive, so I won’t end up like:\n\n\n\nrequire(tidyr)\nrequire(dplyr)\nrequire(ggplot2)\n\n\n\n\n\n\nI’ve generated some fake grade data based on the format that grades come in when you are an instructor at Berkeley (we use bCourses, similar to Moodle, Canvas, etc.).\n\n\nsetwd(\"~/Desktop\")\ngrades=read.csv(\"gradesFake.csv\")\n\n\n\nEach row is a student. Each column is an assignment. The value in this table is the number of points earned.\n\n\nhead(grades,2) \n\n\n\ngather: use when your column names are actually levels of a particular variable\nIn the gather syntax, the first element (after piping in the dataframe) is the new key (the name of the new variable we want, refers to what the current column names are) and the second element is the value (associated with the key, refers to what values are currently in each column). The next arguments are the columns that are going to be turned into key-value pairs. By using the minus sign we can say we want to reshape every column except student.\n\n\ngathered.grades=grades %>% gather(assignment,grade,-Student)\n\nhead(gathered.grades,2)\n\n\n\nThis gathered format allows us to easily group by student or assignment and see how things are going. Is a particular student struggling? Is there an assignment that everyone is stuggling with?\nNote: This example is oversimplifying things since we haven’t yet said how many points each assignment is worth, but we’ll get there.\n\n\ngathered.grades %>% group_by(Student) %>% summarise(meanPoints=mean(grade),sdPoints=sd(grade))%>% head(2)\n\ngathered.grades %>% group_by(assignment) %>% summarise(meanPoints=mean(grade),sdPoints=sd(grade))%>% head(2)\n\n\n\nspread: use when you want the levels of a variable to be separate columns\nWe can undo gather by using spread. We may want to look at this shape if we want to see by assignment, how each student does.\nAnnoyingly, this doesn’t get us quite back to the original data because the assignments are organized alphabetically instead of numerically.\n\n\nbyStudent=gathered.grades %>% spread(assignment, grade) \nbyAssignment=gathered.grades %>% spread(Student, grade) \n\nhead(byStudent,2) ## alphabetical weirdness\nhead(byAssignment,2) ## alphabetical weirdness\n\n\n\nBut we can do a little hack and get things back to normal.\n\n\norderIwant=paste(\"A\",1:(ncol(byStudent)-1),sep=\"\")\n\nhead(byStudent[,c(\"Student\",orderIwant)],3)\n\nbyAssignmentAdjust=byAssignment[unlist(lapply(orderIwant,function(x){which(pull(byAssignment,assignment)==x)})),]\n\nrow.names(byAssignmentAdjust)=NULL\n\nhead(byAssignmentAdjust,3)\n\n\n\nCalculate grades\nFor simplicity, let’s assume that there exists a student for each assigment who got a perfect score, so we can use the maximum per assignment as the number of possible points.\n\n\nscores1=select(byStudent,starts_with(\"A\")) \nscores2=select(byAssignment,one_of(LETTERS[1:(ncol(byAssignment)-1)])) \n\nhead(scores1,2)\nhead(scores2,2)\n\n\n\nThe use of purrr is coming in a future blog post (with @kelliotto) but for now I’m sticking to apply.\n\n\npossiblePoints=sum(apply(scores1,2,max))\npointsPerStudent=apply(scores1,1,sum)\nstudentPercentages=pointsPerStudent/possiblePoints\nstudentPercentages\n\n\n\n\n\npossiblePoints=sum(apply(scores2,1,max))\npointsPerStudent=apply(scores2,2,sum)\nstudentPercentages=pointsPerStudent/possiblePoints\nstudentPercentages\n\n\n\nspread: use when you want to split levels (or values) of a variable into multiple components\nWe can imagine many different ways of organizing grades that may require other verbs in the tidyr package. What if our assignment names also included the due date? We may want to be able to calculate the average grade per month of the semester to assess pacing.\n\n\ngrades2=grades\n#https://stackoverflow.com/questions/21502332/generating-random-dates\ntd = as.Date('2018/05/01') - as.Date('2018/02/01')\ndates=as.Date('2018/02/01') + sample(0:td, ncol(grades)-1)\ndates=dates[order(dates)]\nnames(grades2)[2:ncol(grades2)]=paste(names(grades2)[2:ncol(grades2)],dates,sep=\"_\")\nhead(grades2,2)\n\n\ngathered.grades2=gather(grades2,assignment,grade,-Student)\nhead(gathered.grades2,2)\n\n\ngathered.grades.dueDates=gathered.grades2 %>% separate(assignment,c(\"assignment\",\"year\",\"month\",\"day\"))\nhead(gathered.grades.dueDates,2)\n\n\n\nThere is not a lot to go on here, but we could imagine breaking things down by week instead of month to get a better sense.\n\n\ngathered.grades.dueDates=gathered.grades %>% group_by(assignment)%>%summarise(maxPoss=max(grade)) %>%inner_join(gathered.grades.dueDates,by=c(\"assignment\"=\"assignment\"))\n\ngathered.grades.dueDates %>% group_by(month)%>% summarise(avgGrade=mean(grade/maxPoss))\n\n\n\nWe could do something similar with str_split but it would take many more lines of code.\nunite: use when you want to concatenate multiple variables into one\nTo undo separate, we use unite. We want to collapse all but Student and grade into a variable called assignment. I would usually use paste.\n\n\ngathered.grades.dueDates %>%unite(assignment,-Student, -grade,sep=\"-\") %>% head(2)\n\n\n\nreplace_NA: use to replace missing values with a particular value\nWe may want to replace assignments that are missing with zeros. replace_na requires a named list of what to replace an NA with in each column. Since we have many assignment columns, I had to ask Stack Overflow for some assistance.\n\n\n## this seems like a prime candidate for walk from purrr, to be continued...\ngrades[1,sample(1:(ncol(grades)-1),1)]=NA\ngrades[5,sample(1:(ncol(grades)-1),1)]=NA\ngrades[10,sample(1:(ncol(grades)-1),1)]=NA\ngrades[15,sample(1:(ncol(grades)-1),1)]=NA\n\n\n#https://stackoverflow.com/questions/45576805/how-to-replace-all-na-in-a-dataframe-using-tidyrreplace-na\nmyList <- setNames(lapply(vector(\"list\", ncol(grades)-1), function(x) x <- 0), names(grades)[-1])\nhead(myList,2)\ngrades=grades %>% replace_na(myList)\nhead(grades,2)\n\n\n\ndrop_na: use when you want to remove rows with missing values in certain columns\n\n\n\ndrop_na may be useful when we want to drop each student’s lowest grade in a certain category of grades. Assuming all other NA values are converted to zeros…\n\n\ngathered.grades.cat=gathered.grades %>% mutate(category=sample(1:3,nrow(gathered.grades),replace=T))\n\nbyStudent=split(gathered.grades.cat,pull(gathered.grades.cat,Student))\n\n\ndropLowestScore=function(studentGrades,categoryToDrop){\n\n  catToDrop=studentGrades %>% filter(category==categoryToDrop)\n  \n  toDrop=which.min(pull(catToDrop,grade))\n  \n  studentGrades$grade[toDrop]=NA\n  ## I don't think I can use pull here\n  \n  studentGradesAdj=studentGrades %>% drop_na()\n  \n  return(studentGradesAdj)\n}\n\nadjustedGrades=do.call(\"rbind\",lapply(byStudent,dropLowestScore,1))\n\nlength(unique(pull(adjustedGrades,assignment)))\nadjustedGrades%>%group_by(Student)%>% summarise(count=n())%>%head(2)\n\n\n\ncomplete: use when you want to make implicit missing data explicit\nI often use expand.grid and then merge to do this. This is much more concise.\nNote: If we only want to include levels of a variable that are present in the data we use nesting to denote this within the complete call.\n\n\ndim(adjustedGrades)\ngathered.grades.cat%>%complete(Student,assignment)%>% dim\n\n\n\nseparate_rows: use when a value contains more than one piece of information and you want separate rows for each piece of information\nNow suppose our spreadsheet of grades contains the maximum points possible for each assignment. We can separate this information to make it easier to do calculations.\n\n\nmaxPts=apply(grades[,2:ncol(grades)],2,max)\n\npasteGrades=function(idx){\n  x=paste(grades[,idx+1],rep(maxPts[idx],nrow(grades)),sep=\"/\")\n  return(x)\n}\n\npastedGrades=lapply(1:(ncol(grades)-1),pasteGrades)\n\nadjGrades=do.call(\"cbind\",pastedGrades)\n\ngrades3=cbind.data.frame(pull(grades,Student),adjGrades)\nnames(grades3)=names(grades)\n\nhead(grades3,2)\n\nseparated.grades=grades3 %>% separate_rows(-Student,sep=\"/\")\nhead(separated.grades,4)\n\npointsEarned=separated.grades[seq(1,nrow(separated.grades),by=2),]\npointsPossible=separated.grades[2,-1]\nrow.names(pointsPossible)=NULL\n\nhead(pointsEarned,2)\npointsPossible\n\n\n\nnest: use when you want to see which levels of a variable occur with the same combination of levels of other variables\nWe can use nest to see who got the top grade for each assignment (and use unnest to undo the operation). This call makes every row a combination of assignment and grade level. The last column is a list of all the students who received that particular grade on the assignment.\n\n\nbyAssignmentGrade=gathered.grades%>%nest(Student)\nhead(byAssignmentGrade)\n\npull(byAssignmentGrade,data)[1]\n\nunnest(byAssignmentGrade,data) %>%head\n\n\n\nThat covers most of the tidyr verbs. I suspect that I will still need to look up examples to remember how to use gather and spread, but I’m hoping this cheat sheet will help me remember when to use which without having to try both on a sample data set.\nThe other verbs that I can see myself using frequently are complete and spread. I’m doing a project now where I use the expand.grid and merge trick way too often. I suspect it is a lot slower than complete, so I’m going to make this switch and see if it speeds things up.\nP.S. Just for the record: before posting this I searched for the dollar sign and rewrote the lines that involved it. Working on that guilty pleasure…\nHappy Grading!\nFeedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:59:09-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-guilty-pleasures-dplyr/",
    "title": "Guilty Pleasures via dplyr",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-09",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, I no longer could easily find this data (the 538 GitHub data page has changed?). I am using eval = F to preserve the post, but we will not be able to see output. I will try to find a copy of the data in my backup drive and fix at a later time.\nNext up on my to-learn list is dplyr. I use group_by and summarize fairly regularly, but there is other functionality that I want to learn to take advantage of.\nmutate\nselect\nfilter\narrange\npull (instead of my favorite, $)\nvarious joins (instead of merge)\nIn anticipation of The Bachelorette starting at the end of May, I’m going to wrangle some data on previous contestants. Yes, watching The Bachelorette/The Bachelor is my guilty pleasure. But this feels fitting because the dollar sign is my R guilty pleasure. I think this will be the hardest habit to break, but here I try to gain some muscle memory for mutate and pull.\n\nI feel the disapproval already, but read this super-scientific article before judging me.\n\n\nrequire(stringr)\nrequire(dplyr)\nrequire(ggmap)\nrequire(fuzzyjoin)\nrequire(maps)\nrequire(tigris)\nrequire(sp)\nrequire(acs)\n\n\n\nLuckily, both 538 and Kaggle have some data on this, so I don’t have to do any web scraping.\n\n\n#https://github.com/fivethirtyeight/data/tree/master/bachelorette\n#https://www.kaggle.com/brianbgonz/the-bachelor-contestants/data\n\nsetwd(\"~/Desktop/data-538/bachelorette\")\n\ncontestants<-read.csv(\"contestants.csv\",stringsAsFactors=F)  ## just female contestants\nbachelors<-read.csv(\"bachelors.csv\",stringsAsFactors=F) ## bachelors\nweekByWeek<-read.csv(\"bachelorette.csv\",stringsAsFactors=F) ## both\n\n\n\nThese datasets contain different information, so the goal of this exercise is to wrangle them together and do something simple with the result. Plenty of others have done fancier stuff with this kind of data. I’m just trying to learn some new tidy verbs.\n\n\nhead(contestants)\nhead(bachelors)\nhead(weekByWeek,2)\n\n\n\nGet rid of headers.\n\n\nweekByWeek=weekByWeek[-which(weekByWeek$SEASON==\"SEASON\"),]\n\n\n\nCounting Dates\nUse select and mutate to add the number of dates.\nUsually I would usually just use the dollar sign to add new columns (and I would just manually specify the dates columns).\n\n\ndates=select(weekByWeek,starts_with(\"DATES\")) \n\nweekByWeek=weekByWeek %>% mutate(numOneOnOneDates=apply(dates,1,function(x){length(which(x==\"D1\"))}))\n\nweekByWeek=weekByWeek %>% mutate(numDates=apply(dates,1,function(x){length(which(x!=\"\"))}))\n\nweekByWeek=weekByWeek %>% mutate(numGroupDates=numDates-numOneOnOneDates) ## \n\n\n\nString Processing\nGet the names ready to join using mutate and pull. To standardize each data set, I want first names and last initials in all capitals.\nNote: The string processing here is rudimentary. stringr will have its own blog post.\n\n\nweekByWeek= weekByWeek %>% mutate(firstName=unlist(lapply(pull(weekByWeek, CONTESTANT),\n                                        function(x){unlist(str_split(x, \"_\"))[2]})))\n\nweekByWeek= weekByWeek %>% mutate(lastInitial=unlist(lapply(pull(weekByWeek, CONTESTANT),\n                                                          function(x){unlist(str_split(x, \"_\"))[3]})))\n\nweekByWeek=weekByWeek %>% mutate(lastInitial=unlist(lapply(pull(weekByWeek,lastInitial),function(x){ifelse(is.na(x),\"\",x)})))\n\nweekByWeek=weekByWeek %>% mutate(nameNice=paste(firstName,lastInitial,sep=\" \")) \n\n\n\ncontestants=contestants %>% mutate(firstName=unlist(lapply(pull(contestants,Name),function(x){unlist(str_split(x,\" \"))[1]})))\n\ncontestants=contestants %>% mutate(lastName=unlist(lapply(pull(contestants,Name),function(x){unlist(str_split(x,\" \"))[2]}))) \n\ncontestants=contestants %>% mutate(lastInitial=unlist(lapply(pull(contestants,lastName),function(x){unlist(str_split(x,\"\"))[1]}))) \n\ncontestants=contestants %>% mutate(lastInitial=unlist(lapply(pull(contestants,lastInitial),function(x){ifelse(is.na(x),\"\",x)})))\n\ncontestants=contestants %>% mutate(nameNice=toupper(paste(firstName,lastInitial,sep=\" \"))) \n\n\n\nDeal with some weird entries (nicknames have parentheses that mess things up).\n\n\ncontestants[which(grepl(\"\\\\(\",contestants$Name)),]\ncontestants$nameNice[which(grepl(\"Bowe\",contestants$Name))]=\"Britt N\"\ncontestants$nameNice[which(grepl(\"McAllister\",contestants$Name))]=\"Alexa A\"\n\n\n\nMerging/Joining\nBefore we start merging, we should have a sense of what the best we can do is. Kaggle warns us that the data is missing some seasons.\n\n\ndim(contestants)\ndim(weekByWeek)\n\n\n\nI usually use merge, but dplyr focuses on inner_join, left_join, right_join, etc. so let’s get used to that syntax.\n\n\nweekByWeek=weekByWeek %>% mutate(SEASON=as.numeric(SEASON))\ntryMerge=inner_join(contestants,weekByWeek,by=c(\"nameNice\"=\"nameNice\",\"Season\"=\"SEASON\"))\ndim(tryMerge)\n\n\n\nBecause some contestants don’t have last names listed in the Kaggle data, we are losing a lot of rows. Within a season we should be able to do a rough join instead to recover some of these. However, there are seasons where multiple contestants have the same first name. We can see that duplicates do occur with this fuzzy join.\n\n\nmergedData=weekByWeek %>% regex_inner_join(contestants, by = c(nameNice = \"nameNice\",SEASON=\"Season\"))\ndim(mergedData)\nlength(which(duplicated(mergedData$CONTESTANT))) \n\n\n\nLocation, Location, Location\nSince one of the datasets only has the female contestants, our merged file will only have the women in it. Where are the female contestants from?\n\n\ncoordinates=geocode(pull(mergedData,Hometown),output=\"latlon\")\nwrite.csv(coordinates,\"bachelorCoords.csv\",row.names=F)\n\n\n\n\n\n\nUsing my ggplot skills from previous posts…\n\n\nall_states <- map_data(\"state\")\n\np <- ggplot()+ geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour=\"black\", fill=\"white\" )\np <- p+ geom_point(data=coordinates,aes(x=lon,y=lat))+xlim(-125,-60)+ylim(25,50)+theme_void()\np\n\n\n\nLet’s find out how many per state (and take advantage of arrange).\nMore building off of previous ggplot skills…\n\n\npts = SpatialPoints(coordinates[complete.cases(coordinates),])\n\n#https://journal.r-project.org/archive/2016/RJ-2016-043/RJ-2016-043.pdf\n## There is probably an easier way to do this.\nus_states <- unique(fips_codes$state)[1:51]\ncontinental_states <- us_states[!us_states %in% c(\"AK\", \"HI\")]\nus_pumas <- rbind_tigris(\n lapply(\n    continental_states, function(x) {\n      pumas(state = x, cb = TRUE)\n    }\n  )\n)\n\nproj4string(pts)=proj4string(us_pumas) ## this is needed for over\n\nwithinContinental=over(pts,us_pumas)\n\nbyState=group_by(withinContinental, STATEFP10) %>% summarise(count=n()) \nbyState=byState %>% mutate(STATEFP10=as.numeric(STATEFP10))\nbyState=inner_join(byState,fips.state,by=c(\"STATEFP10\"=\"STATE\"))%>% arrange(desc(count))\n\n\n\nI’m curious if the males participating on The Bachelorette are from the same types of places. It seems to me like small town southern girls are more represented than small town southern boys, but I want to test this theory. Let’s filter for the South to at least start getting a sense.\n\n\nbyState\n\nregion=cbind.data.frame(state.abb, as.character(state.region))\nnames(region)=c(\"abb\",\"reg\")\n\nsouth=inner_join(byState,region,by=c(\"STUSAB\"=\"abb\"))%>%filter(reg==\"South\")\nsouth\n\nsum(pull(south,count))/sum(pull(byState,count))\n\n\n\nThat covers the dplyr verbs, and I’m starting to get the hang of it. I think replacing the dollar sign with mutate will come more easily, but I fear that pull will always be a stretch for me. Fingers crossed I can kick the dollar sign habit.\nFeedback, questions, comments, etc. are welcome (@sastoudt). What’s your R guilty pleasure?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T15:12:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-relive-thesis-ggplot-pt2/",
    "title": "Reliving my Undergrad Thesis via ggplot2: Part 2",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-01",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, this code was revealed to be deprecated. I am using eval = F to preserve the post, but code will not run as is. I will try to update at some point (or if you are reading this and now what to do to fix it, let me know).\nIn a previous post I tackled reproducing one type of plot from my undergrad thesis (maps with color coded dots). The goal for this post is to recreate an interpolated heat map over an actual US map in ggplot. Full disclosure: this was a struggle, and it still isn’t perfect.\nThis was definitely me at many points throughout the process.But I recognize that practice builds intuition, so if you know how I can do something better or how to answer one of my lingering questions, please reach out!\nThe original plots were made by:\nProjecting training data.\nFiting model on projected coordinates.\nMaking predictions on a grid bounded by the bounding box of the continental United States.\nPlotting using a hacky version of fitted.contour() where the colors map to quantiles.\nColoring over everything that is outside of the continental United States in white.\nSee the code here.\nTo switch this over to ggplot I first made two simplifications:\nUse lat/long instead of projected points.\nSkip the heat map at first and just do color coded points.\nFirst I get set up, build the model, and get predictions on a grid. Nothing has changed here from my original approach except doing the model on the lat/long coordinates instead of the projected points. The details of the model are not important here. Feel free to skip past this chunk and just know that coordPred holds the predictions we want to plot on a heatmap.\n\n\n### set up ###\nrequire(ggplot2)\nrequire(mgcv)\nrequire(maps)\nrequire(sp)\nrequire(tigris)\nrequire(RColorBrewer)\n\n\n\n\n\nsetwd(\"~/Smith/Senior Year/Stoudt_Thesis/Data\")\nus <- read.csv(\"usToUseNoDuplicates.csv\")\n\nsetwd(\"~/Smith/Senior Year/Stoudt_Thesis/Data/FINAL\")\ntraining <- read.csv(\"trainingFinal.csv\")\n\n## do on original coordinates for now\n## get bounding box for prediction\n# x = seq(from=min(us$long), to=max(us$long), by=.1)  ## clips off part of west coast\nx <- seq(from = min(map_data(\"state\")$long), to = max(map_data(\"state\")$long), by = .1)\n# y = seq(from=min(us$lat), to=max(us$lat), by=.1)\ny <- seq(from = min(map_data(\"state\")$lat), to = max(map_data(\"state\")$lat), by = .1)\nnx <- length(x)\nny <- length(y)\nxy <- expand.grid(long = x, lat = y)\n\nb1 <- c(\"cr\", \"ps\", \"tp\")\nisPenalized1 <- c(T, F)\nte1Param <- as.data.frame(expand.grid(b1, isPenalized1))\nnames(te1Param) <- c(\"basis\", \"isPenalized\")\nte1Param$basis <- as.character(te1Param$basis)\n\ntraining$uraniumTransform <- 1 / (1 + exp(-training$uranium)) # transform to [0,1] for beta\n\n## optimal parameters in this case\ndata <- te1Param[3, ]\nbasis <- data[[1]]\nisPenalized <- data[[2]]\n\nuranium.gam <- gam(uraniumTransform ~ te(long, lat, bs = basis, fx = isPenalized),\n  family = betar(link = \"logit\"), data = training\n)\n\nzGRID.gam <- predict(uranium.gam, newdata = xy)\n\ncoordPred <- cbind.data.frame(x, y, zGRID.gam)\n\n\n\nNow I need to eliminate all the predictions that lie outside of the continental United States. To do this, I get the borders of the continental United States and use the over function to figure out which of the predicted points lie within the borders.\nFun Fact: The pumas function outputs progress to the console that we want to suppress in this RMarkdown file.\n\n\n#### get within continental US ####\n\npts <- SpatialPoints(coordPred[, 1:2])\n\n# https://journal.r-project.org/archive/2016/RJ-2016-043/RJ-2016-043.pdf\n## There is probably an easier way to do this.\nus_states <- unique(fips_codes$state)[1:51]\ncontinental_states <- us_states[!us_states %in% c(\"AK\", \"HI\")]\nus_pumas <- rbind_tigris(\n  lapply(\n    continental_states, function(x) {\n      pumas(state = x, cb = TRUE)\n    }\n  )\n)\n\nproj4string(pts) <- proj4string(us_pumas) ## this is needed for over\n\nwithinContinental <- over(pts, us_pumas)\n\ntoPlot <- coordPred[which(!is.na(withinContinental[, 1])), ] ## all the same NA structure\nnames(toPlot)[3] <- \"z\"\n\n\n\nLingering Question: Is there an easier way to get the boundary of the continental US?\nNow that we have dropped all of the values outside our area of interest, we can use the code from my last post to plot points color coded by the predicted values.\n\n\nall_states <- map_data(\"state\")\np <- ggplot() +\n  geom_polygon(data = all_states, aes(x = long, y = lat, group = group), colour = \"black\", fill = \"white\")\np <- p + geom_point(data = toPlot, aes(x = x, y = y, col = cut(z, quantile(z, seq(0, 1, by = .1), include.lowest = T)))) + scale_colour_manual(name = \"Predicted Quantile of \\n Uranium (ppm)\", values = c(brewer.pal(9, \"YlOrRd\"), \"black\"))\n\n\n\nSticking Point: For awhile I couldn’t figure out how to put the state lines over these points until I found geom_path (I was trying to do geom_polygon with fill=NA).\n\n\n# https://cran.r-project.org/web/packages/autoimage/vignettes/ggplot2-comparison.html\np + geom_path(aes(x = long, y = lat, group = group), data = all_states) + coord_map(\"lambert\", parameters = c(c(33, 45)))\n\n\n\nNext, I’m going to try to make an actual heatmap instead of just color coding the predicted grid points (although on this size of a plot, we can’t tell that these are individual plots).\nFun Fact: I’m using the ColorBrewer palette for consistency but @hadleywickham recommended viridis palettes. ColorBrewer palettes are “accurate in lightness and hue, but not in saturation” while viridis palettes are “perceptually uniform (i.e. changes in the data should be accurately decoded by our brains) even when desaturated”.\nSticking Point: I first tried this approach. My grid was too dense for the interpolation to run quickly, but even when I made the grid more coarse, the plot didn’t have continuous color (it looked weird).\nSo then I found stat_summary_2d which works pretty well except we now lose control over the break points in the colors.\n\n\n# https://stackoverflow.com/questions/37529116/how-to-plot-a-heat-map-with-irregular-data-in-ordinates-in-ggplot?rq=1\np <- ggplot(toPlot, aes(x = x, y = y, z = z)) +\n  stat_summary_2d(binwidth = 0.3) +\n  scale_fill_gradientn(colours = c(brewer.pal(9, \"YlOrRd\"), \"black\"), \"Predicted Uranium (ppm)\")\np\n\n\n\nSticking Point: I wanted to change the title on this legend, but when I tried through the use of guides, the whole legend changes.\nBut seriously, why?!But thanks to this I learned that I can just add text to scale_fill_gradientn.\n\n\np + guides(fill = guide_legend(title = \"Predicted Uranium \\n (ppm)\"))\n\n\n\nLingering Question: How do I color code by quantile like I did before (using cut on z)? The motivation for doing this is that the distribution of predictions is highly skewed right. If we bin the colors such that an equal number of points are in each point, the last bin will have a huge range (5ish to 30ish ppm). We see more contrast in the overall predictions if we color code by quantile instead.\nI can project this map after the fact and use the theme_void tip from @sharoz.\n\n\np <- p + geom_path(aes(x = long, y = lat, z = NA, group = group), data = all_states)\np + coord_map(\"lambert\", parameters = c(c(33, 45))) + theme_void()\n\n\n\nHowever, I would like to build the model on the projected scale to avoid distortion. For example, this is what the plot looks like if we model on the projected points (from my original approach). We can see more curvature.\n\nI really don’t want to back project the predictions to the lat/long scale to plot just to reproject them via coord_map. There must be a better way.\nThe use of coord_map doesn’t transform the coordinates of the points themselves.\n\n\nstates <- map_data(\"state\")\np <- ggplot(states, aes(long, lat)) +\n  geom_polygon(aes(group = group)) +\n  coord_map(\"lambert\", parameters = c(c(33, 45)))\n\nhead(states$long)\nhead(p$data$long) ## doesn't actually transform coordinates\n\n\n\nLingering Question: How do I tell ggplot that I have points that are in a different projection when I want to add a map outline to them?\nIn my previous post, I felt satisfied with my ggplot version of my thesis plot, but for this post there are still some unanswered questions and room for more efficient solutions. There is also some lingering frustration that I lack control over certain aspects of plotting using ggplot, although I feel this most prominently when making maps. If anyone has resources that helped them control particular aspects of maps in ggplot, please pass them along.\nFeedback, questions, comments, etc. are welcome (@sastoudt).\nThank you to @dpseidel for pep talks throughout.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T15:00:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-relive-thesis-ggplot-pt1/",
    "title": "Reliving my Undergrad Thesis via ggplot2: Part 1",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-04-23",
    "categories": [],
    "contents": "\nAs the first step in tidying my life, I revamp the maps in my undergraduate thesis using ggplot.\nI admit I am a reluctant ggplot2 user. I feel like I don’t have control over small details, and I’m constantly Googling to change something small. However, I recognize the benefits of ggplot deep down and know that if I just get used to the syntax, I’ll slowly break away from reliance on Google. But, I’m currently still in Googling mode, so throughout this post, when I have to make adjustments to the basic ggplot, I include the phrases I Google as well as the link to the help I end up using.\nIn my undergraudate thesis I predicted amounts of trace uranium in the sediment across the continental United States using various geostatistical methods. Look here for a brief overview. My thesis contained many, many maps, but I made them in a really gross way using base plot.\nThere are two main types of plots in my thesis:\nMaps with color coded dots (usually plotting residuals).\nInterpolated heat maps over an actual map (usually plotting predicted surfaces).\nIn this post I’ll focus on #1. The one time I actively choose to use ggplot2 is when I need to color-code by a particular variable. This requires a few extra steps in base plot, so ggplot is actually quicker for me. When I looked back at my thesis code (B.G. – before ggplot) to refresh my memory for this post, I couldn’t believe how many lines of code were needed to make this type of plot. I posted an example of my original code, so that we can all appreciate how much ggplot streamlines things (this is probably not the most efficient base code either).\n\nI will follow up with #2 in another post (I expect this one to be more challenging, so I want to build up to it).\nThe data I used in my thesis is curated from various USGS datasets. Unfortunately, my undergraduate thesis came before my knowledge of GitHub, so I cannot easily point to the preprocessing and analysis code online. However, if you are reading this and want to know more, I will endeavor to get all of the relevant code up on GitHub for you. This is on my long-term to-list, but I’ll make it a priority if someone would find it useful.\nLet’s just see what a simple ggplot looks like, color coded by the amount of uranium.\n\n\nrequire(ggplot2)\nsetwd(\"~/Smith/Senior Year/Stoudt_Thesis/FINAL_CODE/minDataForThesis\")\nus <- read.csv(\"usToUseNoDuplicates.csv\")\nggplot(us, aes(x = long, y = lat, col = uranium)) +\n  geom_point()\n\n\n\n\nMy first major problem with this plot is the default blue scale. I really can’t tell the difference between various blues even with data less skewed than this. Let’s try the Color Brewer palette that I used in my thesis.\nGoogle “color by value ggplot2”\n\n\nrequire(RColorBrewer)\n# http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually\nggplot(us, aes(x = long, y = lat, col = uranium)) +\n  geom_point() +\n  scale_color_gradientn(colours = c(brewer.pal(9, \"YlOrRd\"), \"black\"))\n\n\n\n\nWe still have issues with extreme skew in the uranium values. Most are trace values, but there are some locations that are near old uranium mines that have very large values of uranium. In my thesis I color coded by percentiles. Let’s try that.\nGoogle “color by percentile ggplot 2”\n\n\n# https://stackoverflow.com/questions/18473382/color-code-points-based-on-percentile-in-ggplot\nggplot(us, aes(x = long, y = lat, col = cut(uranium, quantile(uranium, seq(0, 1, by = .1)), include.lowest = T))) +\n  geom_point() +\n  scale_colour_manual(values = c(brewer.pal(9, \"YlOrRd\"), \"black\"))\n\n\n\n\nNow I want the actual map of the United States to be underneath these points.\nGoogle “add outline of us map ggplot”\nWhile we’re at it, let’s make the name of the legend less gross, so the map isn’t squished.\nGoogle “name of legend ggplot2”\n\n\nrequire(maps)\nall_states <- map_data(\"state\")\np <- ggplot() +\n  geom_polygon(data = all_states, aes(x = long, y = lat, group = group), colour = \"black\", fill = \"white\")\n\n\n\n\n\n# https://uchicagoconsulting.wordpress.com/tag/r-ggplot2-maps-visualization/\n# http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/\np <- p + geom_point(data = us, aes(x = long, y = lat, col = cut(uranium, quantile(uranium, seq(0, 1, by = .1))))) + geom_point() + scale_colour_manual(name = \"Quantile of \\n Uranium (ppm)\", values = c(brewer.pal(9, \"YlOrRd\"), \"black\"))\np\n\n\n\n\nThis data is supposed to be clean. What is that one annoying missing value?\n\n\nwhich(is.na(cut(us$uranium, quantile(us$uranium, seq(0, 1, by = .1)))))\n\n\n[1] 26104\n\nus$uranium[26104]\n\n\n[1] -1.2\n\nrange(us$uranium)\n\n\n[1]  -1.2 419.6\n\nOh, cut doesn’t include the minimum. That is kind of annoying.\nGoogle “cut to include minimum R”\nYay, I can just specify that I want the minimum included. That was a quick fix.\n\n\n# https://stackoverflow.com/questions/12245149/cut-include-lowest-values\n\np <- ggplot() +\n  geom_polygon(data = all_states, aes(x = long, y = lat, group = group), colour = \"black\", fill = \"white\")\np <- p + geom_point(data = us, aes(x = long, y = lat, col = cut(uranium, quantile(uranium, seq(0, 1, by = .1)), include.lowest = T))) + geom_point() + scale_colour_manual(name = \"Quantile of \\n Uranium (ppm)\", values = c(brewer.pal(9, \"YlOrRd\"), \"black\"))\np\n\n\n\n\nSo far I have just been using latitude and longitude, but in my thesis I projected my data to avoid distortion.\nGoogle “projected maps ggplot2”\n\n\n# https://ggplot2.tidyverse.org/reference/coord_map.html\np <- ggplot() +\n  geom_polygon(data = all_states, aes(x = long, y = lat, group = group), colour = \"black\", fill = \"white\") +\n  coord_map(\"lambert\", parameters = c(c(33, 45)))\np\n\n\n\n\n\n\np <- p + geom_point(data = us, aes(x = long, y = lat, col = cut(uranium, quantile(uranium, seq(0, 1, by = .1)), include.lowest = T))) + geom_point() + scale_colour_manual(name = \"Quantile of \\n Uranium (ppm)\", values = c(brewer.pal(9, \"YlOrRd\"), \"black\"))\np\n\n\n\n\nThat was actually easier than I anticipated. Now all that is left is to clean up the labels and the background.\nGoogle “plain background ggplot”\n\n\n# http://felixfan.github.io/ggplot2-remove-grid-background-margin/\np <- p + theme(\n  panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n  panel.background = element_blank(), axis.line = element_line(colour = \"black\")\n)\np\n\n\n\n\nI actually have the syntax for axis labels memorized (yay!).\n\n\np + xlab(\"longitude\") + ylab(\"latitude\") + ggtitle(\"Uranium in the Continental United States\")\n\n\n\n\nSince I made many versions of this type of plot in my thesis, if I could go back in time, I would make a function to create these types of plots (by passing in the column name to color by), or at least a theme, to minimize the amount of typing/copy-paste.\nCreating themes may actually help me use ggplot more readily. If I have standard themes for types of plots I make regularly, I won’t have to get out of the zone to re-Google how to change pieces of the plot. I also might try to use qplot when I’m in the exploratory stage and want to make quick plots. Then I can move to ggplot for more formal plots.\nStay tuned for my attempt at an interpolated heat map over an actual map in ggplot…\nFeedback, questions, comments, etc. are welcome (@sastoudt).\nSpecial thanks to @BaumerBen and @askdrstats for helping me with my undergraduate thesis and for bearing with me through the gross code and many, many maps.\nThank you to @dpseidel for reading through this to make sure I was making sense.\nSuggestions from Twitter\n\n\nI’d recommend looking j to viridis scale and using ggplot2::cut_number()\n\n— Hadley Wickham (@hadleywickham) April 24, 2018\n\n\n\ntry using theme_void() to get rid of the unneeded axes\n\n— Steve Haroz (@sharoz) April 24, 2018\n\nLet’s try these tips out!\n\n\nrequire(viridis)\np <- ggplot() +\n  geom_polygon(data = all_states, aes(x = long, y = lat, group = group), colour = \"black\", fill = \"white\") +\n  coord_map(\"lambert\", parameters = c(c(33, 45)))\n\np <- p + geom_point(data = us, aes(x = long, y = lat, col = cut_number(uranium, 10))) + geom_point() +\n  scale_color_viridis(discrete = TRUE, option = \"inferno\", name = \"Quantile of \\n Uranium (ppm)\")\n# scale_colour_manual(name=\"Quantile of \\n Uranium (ppm)\",values=c(brewer.pal(9, \"YlOrRd\"),\"black\"))\np + theme_void()\n\n\n\n\nUsing cut_number instead of quantile means that each color bin has roughly the same number of points in it. We can see that the bins containing the smallest and largest values have a wide range to ensure that they have the same number of points as more dense, yet narrower ranges. This makes the majority of points have better color contrast, but it hides outliers.\nI thought Color Brewer was a reasonable color palette, but wanted to know why viridis might be preferred. Through some Googling: This says that Color Brewer palettes are “accurate in lightness and hue, but not in saturation” while this says that viridis palettes are “perceptually uniform (i.e. changes in the data should be accurately decoded by our brains) even when desaturated”.\n\n\n\n",
    "preview": "posts/2021-06-05-relive-thesis-ggplot-pt1/relive-thesis-ggplot-pt1_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-05T14:49:22-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-tidyverse-straggler/",
    "title": "Confessions of a Tidyverse Straggler",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-04-18",
    "categories": [],
    "contents": "\nThese are my confessions…\nObligatory Usher ReferenceI learned R before the tidyverse (shout out to @BaumerBen’s Multiple Regression class Fall 2016) and have failed to update my skills. There, I said it. I’m a statistics PhD student whose code is riddled with dollar signs (pull Sara!) and the occassional loop that I’ve never wrangled into a proper apply statement (which should now be switched to purrr?). And although I have picked up some ggplot2 and dplyr (I’m not THAT much of an R hermit), I really don’t know what new functionality is available. This prevents my code from keeping up with the times. It has always been on my to-do list to work through the tidyverse documentation so that I can start using the packages in my own work, but I’ve always prioritized “done” over “pretty”.\nBut no longer! My plan of attack is to start blogging about my tidyverse immersion, one package at a time. Each week(ish) I will read up on a tidyverse package and ideally, update some gross code from one of my current or previous projects, showing the before and after. Maybe I’ll aim to post on Monday’s… #MakeoverMonday anyone?\nWhat do I need to master?\nCore Tidyverse\nggplot2: #TidyTuesday is going to be super helpful for this.\ndplyr\ntidyr\nreadr\npurrr\ntibble\nstringr\nforcats\nExtras\nreadxl, haven, jsonlite, xml2\nhttr, rvest\nlubridate, hms\nblob\nrlang\nmagrittr\nglue\nmodelr\nbroom\nlobstr\nsf\nbookdown, blogdown\nreprex\nThat is a lot of weeks! So why am I doing this now?\nHadley Wickham gave a talk in our department recently, and I was one of the lucky five to have lunch with him beforehand. He mentioned that failing to update one’s R skills is like never updating R and RStudio versions (I’m occasionally guilty on this front as well, but let’s deal with one character flaw at a time).\nWhen I was trying to help students at UC Berkeley’s third annual DataFest, I realized the students didn’t know what I was talking about when I was describing how I would approach a coding problem. I needed a tidyverse translator!\nI’m trying to be more open about my work (GitHub, Twitter), and there is only so much embarassment I can take.\nIf not now, when?\nSo stay tuned, and hold me accountable.\nAdvice? Solidarity? Let me know! (@sastoudt)\nSpecial thanks to @dpseidel for giving me feedback while I was drafting this post (and telling me to pull not select instead of $, see I’m learning already!).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:50:21-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-tools-for-gerrymandering-analysis/",
    "title": "Generating and Visualizing Valid Redistricting Scenarios",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-03-29",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, this code was revealed to be deprecated. I am using eval = F to preserve the post, but code will not run as is. I will try to update at some point (or if you are reading this and now what to do to fix it, let me know).\nI recently spent some time at the Geometry of Redistricting Hackathon where I learned about quantitative approaches to assessing gerrymandering. Check out the Metric Geometry and Gerrymandering Group on GitHub to see how you can get involved. I focused on improving documentation during my brief time at the hackathon, but I did not get a chance to contribute as much as I would have liked during the hackathon itself due to my own time constraints.\nHere I hope to continue to improve the overall documentation by giving some intuition about how one can generate valid redistricting plans and evaluate them based on compactness measures using the mandeR and redist packages as well as some code from the MCMC visualization project. The mandeR package takes shapefiles and calculates a variety of compactness measures for each polygon. The redist package implements a Markov Chain Monte Carlo (MCMC) approach to generating valid redistricting plans. We combine the functionality of both to get some intuition about the issues facing the quantitative study of gerrymandering.\n\n\nrequire(redist)\nrequire(mandeR)\nrequire(ggplot2) ## need latest version for geom_sf()\nrequire(sf) ## for working with shape files\nrequire(dplyr)\nrequire(parallel) ## speed up some of the calculations\nrequire(gridExtra)\n\n\n\n*See the README for mandeR if you have trouble installing it.\nThe redist package implements a new approach for simulating possible redistricting scenarios using MCMC. Fifield et al. define the problem as “a state consisting of \\(m\\) geographical units (e.g. census blocks or voting precincts) must be divided into \\(n\\) contiguous districts.”\nredist.enumerate()\nFor small \\(m\\) or \\(n\\) it may be possible to enumerate all valid redistricting plans with a specified number of districts given a set of geographic units. redist.enumerate() does this, but quickly becomes too slow as \\(m\\) and \\(n\\) increase. The example provided by the documentation enumerates all possible redistricting plans for 25 contiguous precincts in Florida. The method only expects an adjacency list (which geographic units share a boundary), and does not utilize any other spatial information.\n\n\ndata(algdat.pfull)\nptm <- proc.time()\ntest=redist.enumerate(adjobj=algdat.pfull$adjlist,ndists=2)#,popvec=algdat.pfull$precinct.data,popcons=0.05)\ntime=proc.time() - ptm\n\ntime\n\nlength(test)\n\ntest[[1]]\n\n\n\nWe can see the number of valid redistricting scenarios. A valid redistricting scenario gives a district label to each geographic unit (here a 1 or 2).\nIf we want to impose equal population constraints for each district, the popvec argument allows us to give population values for each geographical unit, and popcons gives the threshold for how far from equal population we will allow (here we specify within 5%).\n\n\nptm <- proc.time()\ntest=redist.enumerate(adjobj=algdat.pfull$adjlist,ndists=2,popvec=algdat.pfull$precinct.data$pop,popcons=0.05)\ntime=proc.time() - ptm\n\ntime\n\nlength(test)\n\ntest[[1]]\n\n\n\nWe can see there are many fewer valid redistricting scenarios when we impose this constraint.\nIf we want three districts instead of two, we already reach intractability (at least for my patience level on my laptop).\n\n\ntest=redist.enumerate(adjobj=algdat.pfull$adjlist,ndists=3) ## don't run\n\n\n\nredist.mcmc()\nIf it is not feasible to try all possible combinations, how can we generate possible redistricting scenarios efficiently? The challenge is that a random “redistricting” may not be a valid one. We also need to impose a certain structure (contiguous geographic units) and set of constraints (equal population, geographical compactness). Previous approaches are inefficient and ad-hoc. redist.mcmc() is an algorithm that uses MCMC to uniformly sample redistricting plans with a specified number of contiguous districts subject to constraints.\nLet’s walk through the example in the documentation:\n\n\n## Get an initial partition\ninitcds <- algdat.pfull$cdmat[,sample(1:ncol(algdat.pfull$cdmat), 1)]\n## Run the algorithm\nalg_253 <- redist.mcmc(adjobj = algdat.pfull$adjlist, popvec = algdat.pfull$precinct.data$pop, initcds = initcds, nsims = 10000)\n\n\n\n\n\nnames(alg_253)\ndim(alg_253$partitions)\nalg_253$partitions[1,1:10]\n\n\n\nVisualization of Districts\nFor each geographic unit, we see which district it is placed in per iteration. We can also track various constraint measures. However, this all operates via adjacency list, which isn’t easy to parse. What if we want to visualize these redistricting scenarios to better see what is going on? We will use some helper functions from the mcmcviz project.\n\n\n## wrapper for redist.mcmc\n## pre-procceing: change shapefile to adjacency list\n## post-processing: thinning\n#redistrict = function(geom, nsims, nthin, nburn, ndists, popcons, eprob, lambda) {\n  redistrict = function(geom, nsims, nthin, nburn, ndists, popcons) { ## changed for our example\n  adj_obj = st_relate(geom, pattern = \"****1****\")\n  mcmc = redist.mcmc(adj_obj, geom$population, \n                     nsims=nsims+nburn, ndists=ndists, popcons=popcons) ## removed eprob, lambda for our example\n  \n  mcmc$partitions %>% as.data.frame() %>% as.list() ##thin(nsims, nburn, nthin=nthin) %>% ## took out thin, couldn't find the appropriate function (not coda)\n}\n\n## groups geographic units into districts, makes polygons by pasting all together\ncreate_district_map = function(geom, districts)\n{\n  mutate(geom, district = as.character(districts)) %>% \n    group_by(district) %>% \n    summarize(\n      population = sum(population), \n      geometry = st_union(geometry)\n    ) \n}\n\n## gets a district map per iteration\ngather_maps=function(geom, iters) {\n  mclapply(iters,  create_district_map, geom = geom, mc.cores = detectCores()) ## parallel\n}\n\n\n\nThe mcmcviz project also has some real shapefiles of Anne Arundel, MD that we will use here. I have this data downloaded locally, but I would love if someone could tell me how to load a shapefile from GitHub via code (I suspect issues because the other .dbf, .prj, etc. files are also needed at the same time).\n\n\nsetwd(\"~/Desktop/mcmcviz/data\")\ngeom = st_read(\"AnneArundelN.shp\", quiet = TRUE)\nnames(geom) = tolower(names(geom)) ## needed in order for redist.rsg to be able to create an initial districting\niters = redistrict(geom, nsims=1000, nthin=10, nburn=100, ndists=3, popcons=0.05)\n\nmaps = gather_maps(geom, iters) ## time intensive even in parallel\n\n\n\n\n\n## look at a few iterations\nmapdata1 = maps[[1]]\nmapdata2 = maps[[10]]\nmapdata3 = maps[[20]]\nmapdata4 = maps[[30]]\nmapdata5 = maps[[40]]\nmapdata6 = maps[[50]]\n\nmapDistrict<-function(idx){\n  mapdata=maps[[idx]]\n  g1=ggplot(mapdata)+geom_sf(aes(fill=district))+theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n  return(g1)\n}\n\ng1=mapDistrict(1)\ng2=mapDistrict(10)\ng3=mapDistrict(20)\ng4=mapDistrict(30)\ng5=mapDistrict(40)\ng6=mapDistrict(50)\n\ngrid.arrange(g1,g2,g3,g4,g5,g6,ncol=2)\n\n\n\n\n\n\nCompactness Measures\nUsing the README from mandeR we can now check the compactness measures of the proposed redistricting scenarios.\nExpanding on the description of compactness measures from compactnesslib:\nconvex hull score: ratio of the area of the district to the area of the minimum convex polygon that can enclose the district’s geometry\nReock score: measure of the ratio of the area of the district to the area of the minimum bounding circle that encloses the district’s geometry.\nSchwartzberg score: ratio of the perimeter of the district to the circumference of a circle whose area is equal to the area of the district\nPolsby-Popper measure: ratio of the area of the district to the area of a circle whose circumference is equal to the perimeter of the district\nRead more about compactness measures here.\n\n\nmapdata = maps[[1]]\n\n#Convert the shapefile to WKT (class needed by compactlib)\nwkt_str <- lapply(st_geometry(mapdata),st_as_text)\n\n#Retrieve compactness scores from mandeR\nscores <- lapply(wkt_str,getScoresForWKT)\nscores=do.call(rbind,scores)\nscores$id=1:nrow(scores)\n\n#Merge scores back into districts\ndists<-merge(mapdata,scores,by.x=\"district\",by.y=\"id\")\n\nnames(dists)\n\nmapScore<-function(dists,name){\n  g1=ggplot(dists)+geom_sf(aes_string(fill =name ))+theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n}\n\n\n\ng1=mapScore(dists,\"population\")\ng2=mapScore(dists,\"CvxHullPS\")\ng3=mapScore(dists,\"ReockPS\")\ng4=mapScore(dists,\"Schwartzbe\")\ng5=mapScore(dists,\"PolsbyPopp\")\ngrid.arrange(g1,g2,g3,g4,g5,ncol=2)\n\nmapdata = maps[[50]]\n\n#Convert the shapefile to WKT \nwkt_str <- lapply(st_geometry(mapdata),st_as_text)\n\n#Retrieve compactness scores from mandeR\nscores <- lapply(wkt_str,getScoresForWKT)\nscores=do.call(rbind,scores)\nscores$id=1:nrow(scores)\n\n#Merge scores back into districts\ndists<-merge(mapdata,scores,by.x=\"district\",by.y=\"id\")\n\ng1=mapScore(dists,\"population\")\ng2=mapScore(dists,\"CvxHullPS\")\ng3=mapScore(dists,\"ReockPS\")\ng4=mapScore(dists,\"Schwartzbe\")\ng5=mapScore(dists,\"PolsbyPopp\")\ngrid.arrange(g1,g2,g3,g4,g5,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Across Possible Redistricting Scenarios\nIf we want to more systematically compare all of the redistricting options, we can get compactness scores for all of the iterations.\n\n\ngetScoresFn=function(mapdata){\n  wkt_str <- lapply(st_geometry(mapdata),st_as_text)\n\n#Retrieve compactness scores from mandeR\nscores <- lapply(wkt_str,getScoresForWKT)\nscores=do.call(rbind,scores)\nscores$id=1:nrow(scores)\n\n#Merge scores back into districts\ndists<-merge(mapdata,scores,by.x=\"district\",by.y=\"id\")\nreturn(dists)\n}\n\nscoresPerIter=mclapply(maps,getScoresFn,mc.cores = detectCores())\n\n\n\nThen we can plot the density of the scores. This can help us see which particular redistricting plans are extreme (used as evidence for intentional gerrymandering).\n\n\npar(mfrow=c(1,2))\nplot(density(unlist(lapply(scoresPerIter,function(x){mean(x$PolsbyPopp)}))),main=\"Avg PolsbyPopp\")\nplot(density(unlist(lapply(scoresPerIter,function(x){sd(x$PolsbyPopp)}))),main=\"SD PolsbyPopp\")\n\n\n\nSummary\nNow we can:\nTake any shapefile that contains geographic units.\nUse the redist.mcmc() function in redist to get possible redistricting scenarios.\nUse the getScoresForWKT() function in mandeR to get compactness scores for the districts proposed in each iteration of the MCMC.\nPlot the different district scenarios along with their scores to visually assess their suitability\nLook at distributions of particular characteristics of possible redistricting scenarios to help us identify particular scenarios that may be intentionally chosen unfairly.\nFeedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:57:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-05-song-repetition/",
    "title": "Analyzing Song Repetition in R Using geniusr",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-03-27",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, this code was revealed to be deprecated. I am using eval = F to preserve the post, but code will not run as is. I will try to update at some point (or if you are reading this and now what to do to fix it, let me know).\nRick Wicklin (@RickWicklin) posted a blog post recently about how to visualize repetition in song lyrics using SAS. I wanted to do the same thing using R and also utilizing geniusr to more easily access a variety of song lyrics to compare.\nNote: There is also geniusR that I did not dig into further.\nFirst let’s create some functions to make it easy to compare a bunch of different songs.\nStep 1: Get Lyrics\n\n\nrequire(geniusr)\nrequire(dplyr)\nrequire(stringr)\n\n\n\nTo use the Genius API, you will need a client access token.\nCreate a new API client here.\nFor the app website URL you can use http://example.com .\nFor the redirect URL you can use example.com .\nThis will prompt you to enter your client access token.\n\n\ngenius_token()\n\n\n\nLet’s test it out with the catchy ``Shape of You\" by Ed Sheeran.\n\n\nfindSong <- search_song(search_term = \"shape of you\") %>%\n  filter(artist_name == \"Ed Sheeran\")\n\nlyrics <- scrape_lyrics_id(song_id = findSong$song_id[1]) \n## more than one of this song because of remixes, we just want the first one\n\nhead(lyrics)\n\n\n\nStep 2: Preprocess Lyrics\nThis gives us a line per row, but we want individual words. We also want to pre-process the text, removing punctuation and transferring to lower case.\n\n\nallone=str_c(lyrics$line,collapse=\" \") ##concatenate\n\n## remove punctuation\nallone=gsub(\",\",\"\",allone)\nallone=gsub(\"\\\\.\",\"\",allone)\nallone=gsub(\"?\",\"\",allone)\nallone=gsub(\"!\",\"\",allone)\nallone=gsub(\"\\\\(\",\"\",allone)\nallone=gsub(\"\\\\)\",\"\",allone)\n\nalloneLower=tolower(allone)\n\nwordByWord=unlist(strsplit(alloneLower,\" \"))\n\nhead(wordByWord)\n\n\n\nWe can generalize this code into a function for future use.\n\n\nprocessSong<-function(songName,artist){\n  findSong <- search_song(search_term = songName) %>%\n    filter(artist_name == artist)\n  \n  lyrics <- scrape_lyrics_id(song_id = findSong$song_id[1])\n  \n\n  allone=str_c(lyrics$line,collapse=\" \")\n  \n  ## remove punctuation\n  allone=gsub(\",\",\"\",allone)\n  allone=gsub(\"\\\\.\",\"\",allone)\n  allone=gsub(\"?\",\"\",allone)\n  allone=gsub(\"!\",\"\",allone)\n  allone=gsub(\"\\\\(\",\"\",allone)\n  allone=gsub(\"\\\\)\",\"\",allone)\n  \n  alloneLower=tolower(allone)\n  \n  wordByWord=unlist(strsplit(alloneLower,\" \"))\n  \n}\n\n\n\nStep 3: Make Plot\nWe create a matrix of zeros that has the same number of rows and columns as number of words in the song.\nIf a word occurs as the second and tenth word in the song, we have a match, and (2,10) will be represented by a one.\n\n\ntoPlot=matrix(0,nrow=length(wordByWord),ncol=length(wordByWord))\n\nfindMatch<-function(x,corpus){\n  matches=which(corpus==corpus[x])\n cbind(rep(x,length(matches)),matches)\n} ## get (i,j) pairs to fill in\n  \nmatches=lapply(1:length(wordByWord),findMatch,wordByWord)\ntoFill=do.call(\"rbind\",matches)\ntoPlot[toFill]=1\n\n\n\nNote: to match the plots made in Wicklin’s post, we make (1,1) in the top left corner. White denotes repetition. A groups of repeated words often is a chorus.\n\n\nmtx.tmp.h <- apply(toPlot, 1, rev) ## need to flip horizontally\n# thank you to: https://everydropr.wordpress.com/2012/10/16/a-simple-way-to-flip-a-matrix-or-an-array/\n\nimage(1:length(wordByWord),1:length(wordByWord),mtx.tmp.h,col=gray.colors(2),xaxt=\"n\",yaxt=\"n\",xlab=\"\",ylab=\"\") ## 1,1 bottom left instead of top left\n\n\n\nWe calculate the repetition score defined as the proportion of ones in the upper triangular portion of the matrix.\n\n\nlength(which(mtx.tmp.h[upper.tri(mtx.tmp.h)]==1))/length(which(upper.tri(mtx.tmp.h)))\n\n\n\nWe can also generalize this code into a function for future use.\n\n\nmakeRepetitionPlot<-function(lyrics,songName,artist){\n  matches=lapply(1:length(lyrics),findMatch,lyrics)\n  toFill=do.call(\"rbind\",matches)\n  toPlot=matrix(0,nrow=length(lyrics),ncol=length(lyrics))\n  toPlot[toFill]=1\n  \n  \n  mtx.tmp.h <- apply(toPlot, 1, rev)\n  #https://everydropr.wordpress.com/2012/10/16/a-simple-way-to-flip-a-matrix-or-an-array/\n  \n  repScore=length(which(mtx.tmp.h[upper.tri(mtx.tmp.h)]==1))/length(which(upper.tri(mtx.tmp.h)))\n    ##proportion of 1s in the upper triangular portion of the matrix\n\n  image(1:length(lyrics),1:length(lyrics),mtx.tmp.h,col=gray.colors(2),xaxt=\"n\",yaxt=\"n\",xlab=\"\",ylab=\"\",\n        main=paste(songName,\"by\",artist,sep=\" \"),sub=paste(\"Repetition Score =\",round(repScore,2),sep=\" \")) \n  ## 1,1 bottom left instead of top left\n  \n  \n  \n}\n\n\n\nNow that we have the code to make a repetition matrix for any song, we can compare different songs and artists. Below I give some examples of directions that further analysis could take.\nHits v. Underated Songs\nI would expect a hit to be more repetitive than another song (one without a music video/not played on the radio) off the same album.\n\n\nlyr=processSong(\"Shape of You\",\"Ed Sheeran\")\nlyr2=processSong(\"New Man\",\"Ed Sheeran\")\n\npar(mfrow=c(1,2))\nmakeRepetitionPlot(lyr,\"Shape of You\",\"Ed Sheeran\")\nmakeRepetitionPlot(lyr2,\"New Man\",\"Ed Sheeran\")\n\n\n\nThen v. Now\nIt would be interesting to compare repetition as artists evolve.\n\n\nlyr=processSong(\"Senorita\",\"Justin Timberlake\")\nlyr2=processSong(\"Say Something\",\"Justin Timberlake\")\n\npar(mfrow=c(1,2))\nmakeRepetitionPlot(lyr,\"Senorita\",\"Justin Timberlake\")\nmakeRepetitionPlot(lyr2,\"Say Something\",\"Justin Timberlake\")\n\n\n\nRap: Repetitive v. Story\nWhen I was trying to pick examples for this post, my brother Scott recommended looking at the song ``Gucci Gang\" because of how often those two words occur in the song. To juxtapose this repetition, I chose Eminem who is known for rap that tells more of a story with songs that sometimes do not even have a reoccurring chorus.\n\n\nlyr=processSong(\"Gucci Gang\",\"Lil Pump\")\nlyr2=processSong(\"Lose Yourself\",\"Eminem\")\n\npar(mfrow=c(1,2))\nmakeRepetitionPlot(lyr,\"Gucci Gang\",\"Lil Pump\")\nmakeRepetitionPlot(lyr2,\"Lose Yourself\",\"Eminem\")\n\n\n\nThe Different Genres of Taylor Swift\nIt would also be interesting to compare repetition between different genres. Luckily, Taylor Swift’s work has spanned both country and pop (and even some rap).\n\n\nlyr=processSong(\"Teardrops on My Guitar\",\"Taylor Swift\")\nlyr2=processSong(\"Shake it Off\",\"Taylor Swift\")\nlyr3=processSong(\"End Game\",\"Taylor Swift\")\n\npar(mfrow=c(1,3))\nmakeRepetitionPlot(lyr,\"Teardrops on My Guitar\",\"Taylor Swift\")\nmakeRepetitionPlot(lyr2,\"Shake it Off\",\"Taylor Swift\")\nmakeRepetitionPlot(lyr3,\"End Game\",\"Taylor Swift\")\n\n\n\nI am curious to see what others can find. If you use this code to find some interesting patterns in other songs (or improve the code in any way), please let me know (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T14:56:50-04:00",
    "input_file": {}
  }
]
