[
  {
    "path": "TidyTuesday/2021-06-05-hello-world/",
    "title": "Hello world!",
    "description": "Trying out a new collection for Tidy Tuesday contributions",
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2021-06-05",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T13:25:22-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-russian-troll-tweets/",
    "title": "Russian Troll Tweets",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2021-06-05",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, I had to re-download the data from the 538 GitHub data page and the formatting has changed enough to break my rbind. I am using eval = F to preserve the post, but we will not be able to see output. I will try to find a copy of the old data in my backup drive and fix at a later time.\n\n\nrequire(purrr)\nrequire(dplyr)\nrequire(ggplot2)\nrequire(gridExtra)\nrequire(stringr)\nrequire(readr)\nrequire(lubridate)\nrequire(data.table)\nrequire(tidyr)\n\n\n\nThis week’s Tidy Tuesday uses data from 538 that shows tweets from Russian trolls. Read more about the data here.\n\n\nsetwd(\"~/Desktop/russian-troll-tweets\")\n\nfiles <- list.files()\nfiles <- files[grepl(\".csv\", files)]\n\ngetData <- lapply(files,fread)\n\ntweet <- do.call(\"rbind\",getData)\n\n\n\nThis analysis was inspired by Jennifer Golbeck’s “Benford’s Law Applies to Online Social Networks”. Benford’s Law provides the expected frequency (non-uniform) of numbers’ first digits. In this paper she finds that both the number of followers and the number of following per user on Twitter follow Benford’s Law. She mentions that many accounts that deviate strongly from this pattern were engaged in unusual behavior. Does this subset of Russian troll accounts deviate from the expected pattern of Benford’s law? Could this help us identify trolls in the future?\n\n\nbenford = function(d) {\n  \n  log(1+1/d,base = 10)\n  \n}\n\nexpectedFreq <- benford(1:9)\n\n\ncbind.data.frame(1:9, expectedFreq)\n\n\n\nOverall: Snapshot of Accounts at Any Time\nFirst I just aggregate all tweets across the whole time period in the dataset and check the distribution of the first digit for both the followers and following.\nThis is an oversimplification because accounts that tweet more frequently will contribute more to the overall distribution, and the following and follower numbers per account change over time.\n\n\nd1Following <- parse_number(str_sub(tweet$following, 1, 1)) ## get first digit\nd1Followers <- parse_number(str_sub(tweet$followers, 1, 1)) ## get first digit\n\n\nfollowing1D <- as.vector(unname(table(d1Following[which(d1Following!=0)])/length(which(d1Following!=0))))\nfollowers1D <- as.vector(unname(table(d1Followers[which(d1Followers!=0)])/length(which(d1Followers!=0))))\n\ntoP <- cbind.data.frame(expectedFreq, following1D, followers1D, firstDigit = as.factor(1:9))\n\n\ng1 <- ggplot(toP, aes(expectedFreq, following1D, col = firstDigit)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1) + xlab(\"Expected Frequency \\n Benford's Law\") + ylab(\"Empirical Frequency \\n of First Digit\") + ggtitle(\"Number of Following\")\n\ng2 <- ggplot(toP, aes(expectedFreq, followers1D, col = firstDigit)) + geom_point(size = 3) + geom_abline(intercept = 0, slope = 1) + xlab(\"Expected Frequency \\n Benford's Law\")+ylab(\"Empirical Frequency \\n of First Digit\") + ggtitle(\"Number of Followers\")\n\n\ngrid.arrange(g1, g2, ncol = 2)\n\n\n\nThe distribution of following (under the account owner’s control) follows the expected distribuiton of first digits fairly well. However, the distribution of followers (which is less easily manipulated by the acocunt owner) shows that smaller numbers of followers (e.g. first digit equal to one) are overrepresented while larger numbers of followers are underrepresented.\nChange Over Time Per Account\nNow I break up the tweets by month-year chunks and get an average number of followers and following per account in each. I’m curious if accounts get closer to what we expect over time.\n\n\ntweet <- tweet %>% separate(publish_date,c(\"date\",\"time\"),sep=\" \")\ntweet$date <- parse_date(tweet$date,format=\"%m/%d/%Y\")\ntweet$month <- month(tweet$date)\ntweet$year <- year(tweet$date)\n\ntweet2 <- subset(tweet,year>=2015 & year<2018) ## beyond this time period, the bins are too sparse\n\n## still variation within this time period per author\ntoP = tweet2 %>% group_by(year, month, author) %>% summarise(mFollowing = mean(following), mFollower = mean(followers), sdFollowing = sd(following), sdFollower = sd(followers)) #%>% \n\ntoP$ym <- paste(toP$year,toP$month,sep=\"_\")  \n  \nbyChunk <- split(toP,toP$ym)  \n\nhelper <- function(x) {\n  test <- parse_number(str_sub(x, 1, 1))\n  as.vector(unname(table(test[which(test!=0)])/length(which(test!=0))))\n} ## get distribution of first digits for a vector x\n\n\n## per chunk\nfollowingD <- map(map(byChunk, ~.x$mFollowing), helper)\nfollowerD <- map(map(byChunk, ~.x$mFollower), helper)\n\n\n\nDifference Between Expected and Observed Distribution of First Digits\n\n\n## last two months of 2017 are missing a bin\ndiffFollowing <- map(followingD[1:26], ~.x-expectedFreq)\n\n\ndiffFollowingD <- do.call(\"rbind\", diffFollowing)\ndiffFollowingD <- as.data.frame(diffFollowingD)\ndiffFollowingD$ym <- names(diffFollowing)\n\n\ntoP <- diffFollowingD %>% gather(digit,diff,-ym) %>% separate(ym,c(\"year\",\"month\"),sep=\"_\") %>% mutate(digit = parse_number(digit)) %>% mutate(date = as.Date(paste(year, month, \"01\", sep = \"-\")))\n\n\n\n\n\n## last two months of 2017 are missing a bin\ndiffFollower <- map(followerD[1:26], ~.x-expectedFreq)\n\ndiffFollowerD <- do.call(\"rbind\", diffFollower)\ndiffFollowerD <- as.data.frame(diffFollowerD)\ndiffFollowerD$ym <- names(diffFollower)\n\n\ntoP2 <- diffFollowerD %>% gather(digit,diff,-ym) %>% separate(ym,c(\"year\",\"month\"),sep=\"_\") %>% mutate(digit = parse_number(digit)) %>% mutate(date = as.Date(paste(year, month, \"01\", sep = \"-\")))\n\n\n\nAssessing How Weird is Actually Weird\nBefore I plot the differences, I want to know how big a difference would actually be surprising since I expect variation even if Benford’s Law does apply. I draw samples from the distribution expected by Benford’s law (with sample size equal to the number of unique accounts in the data set). The dashed lines in the plots show the 97% empirical intervals from this simulated data. This is where we would expect differences to lie if the data actually follow Benford’s Law.\n\n\nsampleSize=length(unique(tweet2$author))\n\nsimD=rerun(1000,table(sample(1:9,sampleSize,prob = expectedFreq,replace=T))/sampleSize)\n\ndiffSim=map(simD,~.x-expectedFreq)\n\ntoAdd=cbind.data.frame(m=apply(do.call(\"rbind\",diffSim),2,mean),q25=apply(do.call(\"rbind\",diffSim),2,quantile,.025),q75=apply(do.call(\"rbind\",diffSim),2,quantile,.975),digit=1:9)\n\n\ntoP1b=merge(toP,toAdd,by.x=\"digit\",by.y=\"digit\")\ntoP2b=merge(toP2,toAdd,by.x=\"digit\",by.y=\"digit\")\n\n\n\n\n\nggplot(toP1b,aes(date,diff))+geom_point()+geom_line()+geom_hline(data=toP1b,aes(yintercept=q25),lty=2)+geom_hline(data=toP1b,aes(yintercept=q75),lty=2)+facet_wrap(~digit)+xlab(\"\")+ylab(\"Observed Proportion - Expected Proportion\")+ggtitle(\"Following: Differences by Digit\")\n\nggplot(toP2b,aes(date,diff))+geom_point()+geom_line()+geom_hline(data=toP2b,aes(yintercept=q25),lty=2)+geom_hline(data=toP2b,aes(yintercept=q75),lty=2)+facet_wrap(~digit)+xlab(\"\")+ylab(\"Observed Proportion - Expected Proportion\")+ggtitle(\"Follower: Differences by Digit\")\n\n\n\nFor both the following and follower distributions we see much larger deviations than we would expect due to chance, especially for 1, 2, and 3. There does seem to be some fluctuation over time.\nTake-Aways\nThe Russian troll accounts do not follow Benford’s Law in either their follower or following numbers. This could be a way to help identify trolls in the future. The following distribution could be manipulated to better match what we expect, but it would be harder to tamper with the follower distribution.\nIdeas for Next Steps\nLook into the similar laws for beyond the first digit.\nCompare to a sample of “normal” tweets from the same time period (make sure the differences we see here actually are bigger).\nDig into specific time periods when important events happened in the election season.\nFeedback, questions, comments, etc. are welcome (@sastoudt).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T15:50:14-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-christmas-songs/",
    "title": "Christmas Songs",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2019-12-24",
    "categories": [],
    "contents": "\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2019/2019-12-24\")\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sentimentr)\nlibrary(kableExtra)\nlibrary(spotifyr)\nlibrary(genius)\nlibrary(purrr)\n\nsongs <- read.csv(\"christmas_songs.csv\", stringsAsFactors = F)\nlyrics <- read_tsv(\"christmas_lyrics.tsv\")\n\n\n\nLet’s see how basic sentiment analysis classifies these Christmas hits. Luckily, I already have this code ready to go from my R Ladies Lightning Talk.\n\n\nallSentiment <- sentiment(lyrics$lyric)\n\nlyrics$id <- 1:nrow(lyrics)\n\nlyrics2 <- merge(allSentiment, lyrics, by.x = \"element_id\", by.y = \"id\")\n\n\n\nTop 5 Most Positive Songs (on average across lyric lines)\n\n\ntt <- lyrics2 %>%\n  group_by(track_title) %>%\n  summarise(meanSentiment = mean(sentiment)) %>%\n  arrange(desc(meanSentiment)) %>%\n  head(5)\n\nkable(tt) %>% kable_styling()\n\n\n\ntrack_title\n\n\nmeanSentiment\n\n\nSilent Night\n\n\n0.6897360\n\n\nSing Noel\n\n\n0.5814179\n\n\nGod Rest Ye Merry Gentlemen\n\n\n0.3192399\n\n\nHark! The Herald Angels Sing\n\n\n0.2859315\n\n\nO Holy Night\n\n\n0.2756963\n\n\n\nTop 5 Most Negative Songs (on average across lyric lines)\n\n\ntt <- lyrics2 %>%\n  group_by(track_title) %>%\n  summarise(meanSentiment = mean(sentiment)) %>%\n  arrange(meanSentiment) %>%\n  head(5)\n\nkable(tt) %>% kable_styling()\n\n\n\ntrack_title\n\n\nmeanSentiment\n\n\nYou’re A Mean One, Mr. Grinch\n\n\n-0.1200570\n\n\nThis Cold War With You\n\n\n-0.0752970\n\n\nTo Each His Own\n\n\n-0.0490823\n\n\nCome On Into My Arms\n\n\n-0.0465882\n\n\nI’ll Break Out Again Tonight\n\n\n-0.0404023\n\n\n\nDistribution of Sentiment\nThere is a big peak at zero and then plenty of positive sentiment songs, but I would have expected more. However, this is just some basic analysis, so a more sophisticated approach might yield more like we expect.\n\n\nlyrics3 <- lyrics2 %>%\n  group_by(track_title) %>%\n  summarise(meanSentiment = mean(sentiment))\n\nggplot(lyrics3, aes(meanSentiment)) +\n  geom_histogram() +\n  theme_minimal()\n\n\n\n\nWhat makes Mr. Grinch so negative?\n\n\ntt <- lyrics2 %>%\n  filter(track_title == \"You're A Mean One, Mr. Grinch\") %>%\n  select(sentiment, lyric) %>%\n  arrange(sentiment) %>%\n  head(5)\n\nkable(tt) %>% kable_styling()\n\n\n\nsentiment\n\n\nlyric\n\n\n-0.5833333\n\n\nYou’re a bad banana with a greasy black peel\n\n\n-0.5833333\n\n\nYou’re a bad banana with a greasy black peel\n\n\n-0.4472136\n\n\nYou’re a nasty, wasty skunk\n\n\n-0.4472136\n\n\nYou nauseate me, Mr. Grinch\n\n\n-0.4472136\n\n\nYou’re a crooked, jerky jocky\n\n\n\nWhat makes “Silent Night” so positive? Repetition! Free idea: analyze the repetition of the holiday hits.\n\n\ntt <- lyrics2 %>%\n  filter(track_title == \"Silent Night\") %>%\n  select(sentiment, lyric) %>%\n  arrange(desc(sentiment)) %>%\n  head(5)\n\nkable(tt) %>% kable_styling()\n\n\n\nsentiment\n\n\nlyric\n\n\n1.06066\n\n\nSleep in heavenly peace, sleep in heavenly peace\n\n\n1.06066\n\n\nSleep in heavenly peace, sleep in heavenly peace\n\n\n1.06066\n\n\nSleep in heavenly peace, sleep in heavenly peace\n\n\n1.06066\n\n\nSleep in heavenly peace, sleep in heavenly peace\n\n\n1.06066\n\n\nSleep in heavenly peace, sleep in heavenly peace\n\n\nNow let’s compare these songs to the songs on the Stoudt Christmas CD. This CD was lovingly curated by my dad, and I have listened to it every Christmas that I can remember, from in the car driving across Pennsylvania to see family to while decorating the tree. This year I don’t get to hear it played from the real CD at home, so I had to make a Spotify version. Check it out here. Usually I’m all for a good shuffled playlist, but this one has to be listened to in order, because TRADITION.\nAs soon as I hear those opening lines of Paul McCartney’s “Wonderful Christmastime” I know it’s holiday time!\n\n\nblogdown::shortcode(\"youtube\", \"94Ye-3C1FC8\")\n\n\n{{% youtube \"94Ye-3C1FC8\" %}}\n\n\n\nspotify_client_id <- \"\" ## put yours here\nspotify_client_secret <- \"\" ## put yours here\naccess_token <- get_spotify_access_token(client_id = spotify_client_id, client_secret = spotify_client_secret)\n\n\n\n\n\nstoudtChristmas <- get_playlist_tracks(\"0bso6lvKInn3myZ65vCSyj\", authorization = access_token)\n\n\n\n\n\nsafe_lyrics <- safely(genius_lyrics)\n\nartists <- unlist(lapply(stoudtChristmas$track.artists, function(x) {\n  x[1, \"name\"]\n}))\ntrack_name <- stoudtChristmas$track.name\n\nlyricsStoudt <- mapply(safe_lyrics, artists, track_name, SIMPLIFY = F)\n\ndidItWork <- lapply(lyricsStoudt, function(x) {\n  x$error\n})\n\nsum(unlist(lapply(didItWork, is.null))) ## 14 of 21 have lyrics\nsetwd(\"~/Desktop/tidytuesday/week52_2019\")\nsave(lyricsStoudt, file = \"lyricsStoudtChristmas.RData\")\n\n\n\nUnfortunately, this Bruce classic got dropped.\n\n\nblogdown::shortcode(\"youtube\", \"76WFkKp8Tjs\")\n\n\n{{% youtube \"76WFkKp8Tjs\" %}}\n\n\n\nsetwd(\"~/Desktop/tidytuesday/week52_2019\")\nload(file = \"lyricsStoudtChristmas.RData\")\n\ntest <- lapply(lyricsStoudt, function(x) {\n  x$result\n})\n\nallL <- do.call(\"rbind\", test)\n\nallSentiment <- sentiment(allL$lyric)\n\nallL$id <- 1:nrow(allL)\n\nallL2 <- merge(allSentiment, allL, by.x = \"element_id\", by.y = \"id\")\n\n\n\nTop 5 Most Positive Songs (on average across lyric lines)\n\n\ntt <- allL2 %>%\n  group_by(track_title) %>%\n  summarise(meanSentiment = mean(sentiment)) %>%\n  arrange(desc(meanSentiment)) %>%\n  head(5)\n\nkable(tt) %>% kable_styling()\n\n\n\ntrack_title\n\n\nmeanSentiment\n\n\nPeace On Earth / Little Drummer Boy\n\n\n0.2620499\n\n\nChristmas Wrapping\n\n\n0.2461397\n\n\nFeliz Navidad\n\n\n0.2355009\n\n\nRockin’ Around the Christmas Tree\n\n\n0.1862966\n\n\nWhite Christmas\n\n\n0.1810358\n\n\nTop 5 Most Negative Songs (on average across lyric lines)\n\n\ntt <- allL2 %>%\n  group_by(track_title) %>%\n  summarise(meanSentiment = mean(sentiment)) %>%\n  arrange(meanSentiment) %>%\n  head(5)\n\nkable(tt) %>% kable_styling()\n\n\n\ntrack_title\n\n\nmeanSentiment\n\n\nBlue Christmas\n\n\n0.0289615\n\n\nJingle Bell Rock\n\n\n0.0459830\n\n\nSnoopy’s Christmas\n\n\n0.0695365\n\n\nI Saw Three Ships\n\n\n0.0697308\n\n\nI Saw Mommy Kissing Santa Claus\n\n\n0.0698132\n\n\nHow does the Stoudt family Christmas album compare? We’re pretty positive (in red) in comparison to all of the hits.\n\n\nallL3 <- allL2 %>%\n  group_by(track_title) %>%\n  summarise(meanSentiment = mean(sentiment))\n\nggplot(lyrics3, aes(meanSentiment)) +\n  geom_histogram() +\n  theme_minimal() +\n  geom_histogram(data = allL3, aes(meanSentiment), fill = \"red\")\n\n\n\n\nHappy Holidays!\n\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-christmas-songs/christmas-songs_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-05T15:05:52-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-squirrel-census/",
    "title": "Squirrel Census",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2019-10-30",
    "categories": [],
    "contents": "\nIt has been a LONG time since I last participated in Tidy Tuesday. Apologies #rstats world! It turns out getting a PhD is… alot. But I obviously had to return for the Squirrel Census.\n\n\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(magrittr)\n\nsetwd(\"~/Desktop/tidytuesday/data/2019/2019-10-29\")\n\nsq <- read.csv(\"nyc_squirrels.csv\", stringsAsFactors = F)\nnames(sq)\n\n\n [1] \"long\"                                      \n [2] \"lat\"                                       \n [3] \"unique_squirrel_id\"                        \n [4] \"hectare\"                                   \n [5] \"shift\"                                     \n [6] \"date\"                                      \n [7] \"hectare_squirrel_number\"                   \n [8] \"age\"                                       \n [9] \"primary_fur_color\"                         \n[10] \"highlight_fur_color\"                       \n[11] \"combination_of_primary_and_highlight_color\"\n[12] \"color_notes\"                               \n[13] \"location\"                                  \n[14] \"above_ground_sighter_measurement\"          \n[15] \"specific_location\"                         \n[16] \"running\"                                   \n[17] \"chasing\"                                   \n[18] \"climbing\"                                  \n[19] \"eating\"                                    \n[20] \"foraging\"                                  \n[21] \"other_activities\"                          \n[22] \"kuks\"                                      \n[23] \"quaas\"                                     \n[24] \"moans\"                                     \n[25] \"tail_flags\"                                \n[26] \"tail_twitches\"                             \n[27] \"approaches\"                                \n[28] \"indifferent\"                               \n[29] \"runs_from\"                                 \n[30] \"other_interactions\"                        \n[31] \"lat_long\"                                  \n[32] \"zip_codes\"                                 \n[33] \"community_districts\"                       \n[34] \"borough_boundaries\"                        \n[35] \"city_council_districts\"                    \n[36] \"police_precincts\"                          \n\nWhat weird stuff can we find? The highlight_fur_color field catches my eye. Who knew squirrels were into highlights?\n\n\ntt <- sq %>%\n  group_by(unique_squirrel_id) %>%\n  summarise(primary_fur_color = primary_fur_color[1]) %>%\n  group_by(primary_fur_color) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count)) %>%\n  as.data.frame()\n\nkable(tt) %>% kable_styling()\n\n\n\nprimary_fur_color\n\n\ncount\n\n\nGray\n\n\n2468\n\n\nCinnamon\n\n\n392\n\n\nBlack\n\n\n103\n\n\nNA\n\n\n55\n\n\ntt <- sq %>%\n  group_by(unique_squirrel_id) %>%\n  summarise(primary_fur_color = primary_fur_color[1], highlight_fur_color = highlight_fur_color[1]) %>%\n  group_by(primary_fur_color, highlight_fur_color) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count)) %>%\n  as.data.frame()\n\nkable(tt) %>% kable_styling()\n\n\n\nprimary_fur_color\n\n\nhighlight_fur_color\n\n\ncount\n\n\nGray\n\n\nNA\n\n\n894\n\n\nGray\n\n\nCinnamon\n\n\n750\n\n\nGray\n\n\nWhite\n\n\n487\n\n\nGray\n\n\nCinnamon, White\n\n\n265\n\n\nCinnamon\n\n\nGray\n\n\n162\n\n\nCinnamon\n\n\nWhite\n\n\n94\n\n\nBlack\n\n\nNA\n\n\n74\n\n\nCinnamon\n\n\nNA\n\n\n62\n\n\nCinnamon\n\n\nGray, White\n\n\n58\n\n\nNA\n\n\nNA\n\n\n55\n\n\nGray\n\n\nBlack, Cinnamon, White\n\n\n32\n\n\nGray\n\n\nBlack\n\n\n24\n\n\nBlack\n\n\nCinnamon\n\n\n15\n\n\nCinnamon\n\n\nBlack\n\n\n10\n\n\nGray\n\n\nBlack, Cinnamon\n\n\n9\n\n\nBlack\n\n\nGray\n\n\n8\n\n\nGray\n\n\nBlack, White\n\n\n7\n\n\nBlack\n\n\nCinnamon, White\n\n\n3\n\n\nCinnamon\n\n\nBlack, White\n\n\n3\n\n\nCinnamon\n\n\nGray, Black\n\n\n3\n\n\nBlack\n\n\nWhite\n\n\n2\n\n\nBlack\n\n\nGray, White\n\n\n1\n\n\nI’m kind of surprised that gray with black highlights is so uncommon. But to be fair, that surprise is based on absolutely no knowlege of NYC squirrels. Enlighten me!\nThe next weird thing I wanted to dig in was the other_activities field. Look at some of these gems.\n\n\nunique(sq$other_activities)[3:12]\n\n\n [1] \"wrestling with mother\"                        \n [2] \"grooming\"                                     \n [3] \"walking\"                                      \n [4] \"moving slowly\"                                \n [5] \"sitting\"                                      \n [6] \"eating (ate upside down on a tree — #jealous)\"\n [7] \"running (with nut)\"                           \n [8] \"playing with #5\"                              \n [9] \"hiding nut\"                                   \n[10] \"drank from a pond of rain water\"              \n\nI’m interested in the squirrel interactions.\n\n\ninteractions <- sq[str_which(sq$other_activities, \"#\"), ]\n\n# interactions[1, ]\n\ninteractions <- interactions[-1, ] ## get rid of #jealous\n\n\n\nThe first thing I want to know is whether this level of detail is concentrated in one pocket (due to a few really intense data collectors). Actually, no!\n\n\nggplot(sq, aes(long, lat)) +\n  geom_point() +\n  geom_point(data = interactions, aes(long, lat), col = \"red\") +\n  theme_minimal()\n\n\n\n\nFor each interaction, who is the other squirrel?\n\n\nnums <- unlist(map(str_split(interactions$other_activities, \"#\"), 2)) ## grab the number\njustNums <- str_replace_all(nums, \"[^0-9]\", \"\") ## get rid of the extra junk\ninteractions$otherSquirrel <- justNums\n\n\n\nOk, now for some data munging. If we look at the unique_squirrel_id field, it isn’t a perfect paste of “Hectare ID” + “Shift” + “Date” + “Hectare Squirrel Number.” The hectare ID has to be un-padded, the date has to be striped of the year, and the squirrel number has to be padded. Here we go!\n\n\ngetBuddy <- function(x) {\n  unpadHectare <- str_sub(interactions$hectare[x], str_locate(interactions$hectare[x], \"[^0]\")[1], str_length(interactions$hectare[x]))\n  # https://stat.ethz.ch/pipermail/r-help/2010-October/255450.html\n  newDate <- str_sub(interactions$date[x], 1, 4)\n  paddedSquirrel <- str_pad(interactions$otherSquirrel[x], width = 2, side = \"left\", pad = \"0\")\n  id <- paste(unpadHectare, interactions$shift[x], newDate, paddedSquirrel, sep = \"-\")\n  subset(sq, unique_squirrel_id == id)\n}\n\n\n\nPause for some fun.\n\n\ninteractions$other_activities[31]\n\n\n[1] \"canoodling w/ #9\"\n\nBack to business.\n\n\nbuddies <- map(1:nrow(interactions), getBuddy)\n\nmap(buddies, nrow) %>%\n  unlist() %>%\n  summary() ## check that everyone got a buddy\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1 \n\n\n\ninteractions %<>% mutate(otherSquirrelAge = unlist(map(buddies, \"age\"))) ## fancy pipe!\ninteractions %<>% mutate(otherSquirrelPrimaryFurColor = unlist(map(buddies, \"primary_fur_color\")))\ninteractions %<>% mutate(otherSquirrelHighlightFurColor = unlist(map(buddies, \"highlight_fur_color\")))\n\n\n\nIt looks like squirrels interact with squirrels like them in terms of age and coloring. But of course there is a big disparity in age and color overall, so I don’t want to read too much into this.\n\n\ntt <- interactions %>%\n  group_by(age, otherSquirrelAge) %>%\n  summarise(count = n())\n\nkable(tt) %>% kable_styling()\n\n\n\nage\n\n\notherSquirrelAge\n\n\ncount\n\n\nAdult\n\n\nAdult\n\n\n52\n\n\nAdult\n\n\nJuvenile\n\n\n6\n\n\nJuvenile\n\n\nAdult\n\n\n2\n\n\nJuvenile\n\n\nJuvenile\n\n\n5\n\n\nNA\n\n\nNA\n\n\n6\n\n\ntt <- interactions %>%\n  group_by(primary_fur_color, otherSquirrelPrimaryFurColor) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\nkable(tt) %>% kable_styling()\n\n\n\nprimary_fur_color\n\n\notherSquirrelPrimaryFurColor\n\n\ncount\n\n\nGray\n\n\nGray\n\n\n51\n\n\nCinnamon\n\n\nGray\n\n\n6\n\n\nGray\n\n\nCinnamon\n\n\n6\n\n\nCinnamon\n\n\nCinnamon\n\n\n4\n\n\nGray\n\n\nNA\n\n\n2\n\n\nBlack\n\n\nGray\n\n\n1\n\n\nGray\n\n\nBlack\n\n\n1\n\n\ntt <- interactions %>%\n  group_by(primary_fur_color, highlight_fur_color, otherSquirrelPrimaryFurColor, otherSquirrelHighlightFurColor) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\nkable(tt) %>% kable_styling()\n\n\n\nprimary_fur_color\n\n\nhighlight_fur_color\n\n\notherSquirrelPrimaryFurColor\n\n\notherSquirrelHighlightFurColor\n\n\ncount\n\n\nGray\n\n\nNA\n\n\nGray\n\n\nNA\n\n\n20\n\n\nGray\n\n\nCinnamon\n\n\nGray\n\n\nCinnamon\n\n\n11\n\n\nGray\n\n\nCinnamon, White\n\n\nGray\n\n\nCinnamon, White\n\n\n4\n\n\nGray\n\n\nNA\n\n\nGray\n\n\nCinnamon\n\n\n4\n\n\nCinnamon\n\n\nGray\n\n\nGray\n\n\nCinnamon\n\n\n2\n\n\nCinnamon\n\n\nGray\n\n\nGray\n\n\nNA\n\n\n2\n\n\nCinnamon\n\n\nGray, White\n\n\nCinnamon\n\n\nGray, White\n\n\n2\n\n\nCinnamon\n\n\nWhite\n\n\nCinnamon\n\n\nWhite\n\n\n2\n\n\nGray\n\n\nCinnamon\n\n\nGray\n\n\nWhite\n\n\n2\n\n\nGray\n\n\nCinnamon\n\n\nGray\n\n\nNA\n\n\n2\n\n\nGray\n\n\nWhite\n\n\nGray\n\n\nCinnamon\n\n\n2\n\n\nGray\n\n\nNA\n\n\nCinnamon\n\n\nGray\n\n\n2\n\n\nBlack\n\n\nNA\n\n\nGray\n\n\nNA\n\n\n1\n\n\nCinnamon\n\n\nNA\n\n\nGray\n\n\nCinnamon, White\n\n\n1\n\n\nCinnamon\n\n\nNA\n\n\nGray\n\n\nWhite\n\n\n1\n\n\nGray\n\n\nCinnamon\n\n\nCinnamon\n\n\nGray\n\n\n1\n\n\nGray\n\n\nCinnamon\n\n\nNA\n\n\nNA\n\n\n1\n\n\nGray\n\n\nCinnamon, White\n\n\nCinnamon\n\n\nNA\n\n\n1\n\n\nGray\n\n\nCinnamon, White\n\n\nGray\n\n\nWhite\n\n\n1\n\n\nGray\n\n\nCinnamon, White\n\n\nGray\n\n\nNA\n\n\n1\n\n\nGray\n\n\nWhite\n\n\nCinnamon\n\n\nGray\n\n\n1\n\n\nGray\n\n\nWhite\n\n\nCinnamon\n\n\nNA\n\n\n1\n\n\nGray\n\n\nWhite\n\n\nGray\n\n\nWhite\n\n\n1\n\n\nGray\n\n\nWhite\n\n\nGray\n\n\nNA\n\n\n1\n\n\nGray\n\n\nNA\n\n\nBlack\n\n\nNA\n\n\n1\n\n\nGray\n\n\nNA\n\n\nGray\n\n\nCinnamon, White\n\n\n1\n\n\nGray\n\n\nNA\n\n\nGray\n\n\nWhite\n\n\n1\n\n\nGray\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n1\n\n\nSo what have we learned? Not much, but there was some fun data wrangling + squirrels. Full disclosure, I did some not-tidy stuff in the exploratory phase, but in this post I took some extra time to switch back to the tidyverse. I admit, I had to refer back to this multiple times despite writing it.\nComments, suggestions, etc. are welcome –> @sastoudt\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-squirrel-census/squirrel-census_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-05T15:07:09-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-thanksgiving-dinner/",
    "title": "Thanksgiving Dinner",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-11-20",
    "categories": [],
    "contents": "\nThis is super tardy for Thanksgiving, but since Christmas is around the corner, and often there is a similar food vibe, here we go anyway…\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(tidyr)\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-11-20\")\ntg <- read.csv(\"thanksgiving_meals.csv\")\n\n\n\nWho travels most?\nTrying to break down by community type, age, and gender, leaves bins too sparse.\n\n\nlook <- tg %>%\n  filter(celebrate == \"Yes\") %>%\n  group_by(travel, community_type, age, gender) %>%\n  summarise(count = n())\n\nsummary(look$count) ## too sparse\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   4.500   8.000   9.899  13.000  38.000 \n\nLet’s first just look at travel by community type.\n\n\ntravelCommunity <- tg %>%\n  filter(celebrate == \"Yes\") %>%\n  group_by(travel, community_type) %>%\n  summarise(count = n()) %>%\n  filter(!is.na(community_type) & !is.na(travel)) %>%\n  left_join(., tg %>% filter(celebrate == \"Yes\") %>% group_by(community_type) %>% summarise(total = n())) %>%\n  mutate(percent = count / total)\n\nggplot(data = travelCommunity, aes(x = travel, y = percent, fill = community_type)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip()\n\n\n\n\nAh, labels are a mess! Luckily, there is a natural breakpoint (—).\n\n\ntravelCommunity$travel2 <- unlist(lapply(strsplit(as.character(travelCommunity$travel), \"--\"), function(x) {\n  paste(x[1], \"\\n\", x[2], collapse = \"\")\n}))\n\nggplot(data = travelCommunity, aes(x = travel2, y = percent, fill = community_type)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip() +\n  xlab(\"\")\n\n\n\n\nHaving to specify the xlab as empty instead of the ylab that it ends up with due to the coord_flip was unexpected.\nNow let’s look at travel by age.\n\n\ntravelAge <- tg %>%\n  filter(celebrate == \"Yes\") %>%\n  group_by(travel, age) %>%\n  summarise(count = n()) %>%\n  filter(!is.na(age) & !is.na(travel)) %>%\n  left_join(., tg %>% filter(celebrate == \"Yes\") %>% group_by(age) %>% summarise(total = n())) %>%\n  mutate(percent = count / total)\n\ntravelAge$travel2 <- unlist(lapply(strsplit(as.character(travelAge$travel), \"--\"), function(x) {\n  paste(x[1], \"\\n\", x[2], collapse = \"\")\n})) ## similarly break up label\n\n\n\nHere we need to reverse the order of age because of the coord_flip.\n\n\nggplot(data = travelAge, aes(x = travel2, y = percent, fill = age)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip() ## need to reverse order of age\n\n\n\n\nAll the things that don’t work.\n\n\nggplot(data = travelAge, aes(x = travel2, y = percent, fill = age)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip() +\n  scale_x_discrete(limits = rev(levels(travelAge$age))) ## nope\n\nggplot(data = travelAge, aes(x = travel2, y = percent, fill = age)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip() +\n  scale_y_discrete(limits = rev(levels(travelAge$age))) ## nope\n\n## set limits before flipping?\nggplot(data = travelAge, aes(x = travel2, y = percent, fill = age)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_y_discrete(limits = rev(levels(travelAge$age))) +\n  coord_flip() ## nope\n\nggplot(data = travelAge, aes(x = travel2, y = percent, fill = age)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_x_discrete(limits = rev(levels(travelAge$age))) +\n  coord_flip() ## nope\n\n\n\nFinally, the winner!\n\n\n\nggplot(data = travelAge %>% mutate(age = fct_rev(age)), aes(x = travel2, y = percent, fill = age)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip() +\n  scale_fill_discrete(guide = guide_legend(reverse = TRUE))\n\n\n\n\nHow about travel by gender?\n\n\ntravelGender <- tg %>%\n  filter(celebrate == \"Yes\") %>%\n  group_by(travel, gender) %>%\n  summarise(count = n()) %>%\n  filter(!is.na(gender) & !is.na(travel)) %>%\n  left_join(., tg %>% filter(celebrate == \"Yes\") %>% group_by(gender) %>% summarise(total = n())) %>%\n  mutate(percent = count / total)\n\ntravelGender$travel2 <- unlist(lapply(strsplit(as.character(travelGender$travel), \"--\"), function(x) {\n  paste(x[1], \"\\n\", x[2], collapse = \"\")\n})) ## similarly break up label\n\nggplot(data = travelGender, aes(x = travel2, y = percent, fill = gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  coord_flip() +\n  xlab(\"\")\n\n\n\n\nWho is shopping and who is working on Black Friday?\nThere is a small sample size here, so I’m just going to report some simple summary tables.\n\n\nincome <- tg %>%\n  filter(celebrate == \"Yes\") %>%\n  group_by(family_income) %>%\n  summarise(work = length(which(work_retail == \"Yes\")), shop = length(which(black_friday == \"Yes\"))) %>%\n  left_join(., tg %>% filter(celebrate == \"Yes\") %>% group_by(family_income) %>% summarise(total = n())) %>%\n  mutate(propW = work / total, propS = shop / total) %>%\n  filter(!is.na(family_income))\n\n\n\n\n\ntg %>%\n  filter(work_retail == \"Yes\") %>%\n  group_by(work_black_friday) %>%\n  summarise(count = n())\n\n\n# A tibble: 3 x 2\n  work_black_friday count\n* <fct>             <int>\n1 Doesn't apply         7\n2 No                   20\n3 Yes                  43\n\n## small sample size, can't do much\n\ntg %>%\n  filter(work_retail == \"Yes\") %>%\n  group_by(gender) %>%\n  summarise(count = n())\n\n\n# A tibble: 3 x 2\n  gender count\n* <fct>  <int>\n1 Female    31\n2 Male      38\n3 <NA>       1\n\ntg %>%\n  filter(work_retail == \"Yes\") %>%\n  group_by(age) %>%\n  summarise(count = n())\n\n\n# A tibble: 5 x 2\n  age     count\n* <fct>   <int>\n1 18 - 29    26\n2 30 - 44    21\n3 45 - 59    15\n4 60+         7\n5 <NA>        1\n\ntg %>%\n  filter(work_retail == \"Yes\") %>%\n  group_by(work_black_friday) %>%\n  summarise(count = n())\n\n\n# A tibble: 3 x 2\n  work_black_friday count\n* <fct>             <int>\n1 Doesn't apply         7\n2 No                   20\n3 Yes                  43\n\nbyIncome <- tg %>%\n  filter(work_retail == \"Yes\") %>%\n  group_by(family_income) %>%\n  summarise(count = n())\n\n\n\nI want to reorder based on money not alphabetical order.\n\n\nlevels(income$family_income) ## want to reorder\n\n\n [1] \"$0 to $9,999\"         \"$10,000 to $24,999\"  \n [3] \"$100,000 to $124,999\" \"$125,000 to $149,999\"\n [5] \"$150,000 to $174,999\" \"$175,000 to $199,999\"\n [7] \"$200,000 and up\"      \"$25,000 to $49,999\"  \n [9] \"$50,000 to $74,999\"   \"$75,000 to $99,999\"  \n[11] \"Prefer not to answer\"\n\nintstep <- levels(income$family_income)[order(nchar(levels(income$family_income)))]\nintstep ## by number of characters, works except for and up\n\n\n [1] \"$0 to $9,999\"         \"$200,000 and up\"     \n [3] \"$10,000 to $24,999\"   \"$25,000 to $49,999\"  \n [5] \"$50,000 to $74,999\"   \"$75,000 to $99,999\"  \n [7] \"$100,000 to $124,999\" \"$125,000 to $149,999\"\n [9] \"$150,000 to $174,999\" \"$175,000 to $199,999\"\n[11] \"Prefer not to answer\"\n\ntest <- fct_relevel(income$family_income, c(intstep[1], intstep[3:10], intstep[2], intstep[11]))\nlevels(test)\n\n\n [1] \"$0 to $9,999\"         \"$10,000 to $24,999\"  \n [3] \"$25,000 to $49,999\"   \"$50,000 to $74,999\"  \n [5] \"$75,000 to $99,999\"   \"$100,000 to $124,999\"\n [7] \"$125,000 to $149,999\" \"$150,000 to $174,999\"\n [9] \"$175,000 to $199,999\" \"$200,000 and up\"     \n[11] \"Prefer not to answer\"\n\nincome$family_income <- fct_relevel(income$family_income, c(intstep[1], intstep[3:10], intstep[2], intstep[11]))\nlevels(test)\n\n\n [1] \"$0 to $9,999\"         \"$10,000 to $24,999\"  \n [3] \"$25,000 to $49,999\"   \"$50,000 to $74,999\"  \n [5] \"$75,000 to $99,999\"   \"$100,000 to $124,999\"\n [7] \"$125,000 to $149,999\" \"$150,000 to $174,999\"\n [9] \"$175,000 to $199,999\" \"$200,000 and up\"     \n[11] \"Prefer not to answer\"\n\n\n\nbyIncome$family_income <- fct_relevel(byIncome$family_income, c(intstep[1], intstep[3:10], intstep[2], intstep[11]))\nbyIncome\n\n\n# A tibble: 12 x 2\n   family_income        count\n   <fct>                <int>\n 1 $0 to $9,999            10\n 2 $10,000 to $24,999       8\n 3 $100,000 to $124,999     4\n 4 $125,000 to $149,999     1\n 5 $150,000 to $174,999     2\n 6 $175,000 to $199,999     1\n 7 $200,000 and up          5\n 8 $25,000 to $49,999      17\n 9 $50,000 to $74,999       5\n10 $75,000 to $99,999       9\n11 Prefer not to answer     7\n12 <NA>                     1\n\nPie!\n\nThe question asks “Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply.” Each column represents a type of pie.\n\n\nhead(tg %>% select(contains(\"pie\")))\n\n\n   pie1 pie2   pie3      pie4 pie5 pie6  pie7  pie8    pie9\n1 Apple <NA>   <NA>      <NA> <NA> <NA>  <NA>  <NA>    <NA>\n2 Apple <NA>   <NA> Chocolate <NA> <NA>  <NA>  <NA> Pumpkin\n3 Apple <NA> Cherry      <NA> <NA> <NA> Peach Pecan Pumpkin\n4  <NA> <NA>   <NA>      <NA> <NA> <NA>  <NA> Pecan Pumpkin\n5 Apple <NA>   <NA>      <NA> <NA> <NA>  <NA>  <NA> Pumpkin\n6  <NA> <NA>   <NA>      <NA> <NA> <NA>  <NA>  <NA>    <NA>\n         pie10 pie11                  pie12                 pie13\n1         <NA>  <NA>                   <NA>                  <NA>\n2         <NA>  <NA> Other (please specify) Derby, Japanese fruit\n3 Sweet Potato  <NA>                   <NA>                  <NA>\n4         <NA>  <NA>                   <NA>                  <NA>\n5         <NA>  <NA>                   <NA>                  <NA>\n6 Sweet Potato  <NA> Other (please specify)         Blueberry pie\n\ncbind.data.frame(\n  type = (tg %>% select(contains(\"pie\")) %>% apply(., 2, function(x) {\n    x[which(!is.na(x))[1]]\n  })),\n  count = (tg %>% select(contains(\"pie\")) %>% apply(., 2, function(x) {\n    length(which(!is.na(x)))\n  }))\n) %>% arrange(desc(count))\n\n\n                        type count\npie9                 Pumpkin   729\npie1                   Apple   514\npie8                   Pecan   342\npie10           Sweet Potato   152\npie4               Chocolate   133\npie3                  Cherry   113\npie12 Other (please specify)    71\npie13  Derby, Japanese fruit    71\npie11                   None    40\npie6                Key lime    39\npie5           Coconut cream    36\npie2              Buttermilk    35\npie7                   Peach    34\n\nSince “Other (please specify)” has the same count as “Derby, Japanese fruit”, I’m going to assume this type of pie is where they specified. This will impact the count of pies per household.\n\n\ntg %>%\n  select(contains(\"pie\")) %>%\n  mutate(numPies = apply(., 1, function(x) {\n    length(which(!is.na(x)))\n  })) %>%\n  mutate(isOther = apply(., 1, function(x) {\n    as.numeric(!is.na(x[12]))\n  })) %>%\n  mutate(numPiesAdj = numPies - isOther) %>%\n  ggplot(., aes(numPiesAdj)) +\n  geom_histogram(binwidth = 1, col = \"black\", fill = \"white\")\n\n\n\n\nPlease invite me to the households that serve over five different types of pie per holiday (but perhaps people rotate among types of pies each year and don’t actually have them all at one time).\nVeggies\n\nSides are reported similarly to pie. How do veggies fare in the side dish line up?\n\n\nsideNum <- tg %>%\n  select(contains(\"side\")) %>%\n  apply(., 2, function(x) {\n    length(which(!is.na(x)))\n  }) %>%\n  as.data.frame() %>%\n  t() %>%\n  as.data.frame()\n\nsideNames <- tg %>%\n  select(contains(\"side\")) %>%\n  apply(., 2, function(x) {\n    x[which(!is.na(x))[1]]\n  }) %>%\n  as.data.frame()\n\nnames(sideNum) <- sideNames[, 1]\n\nsideNames[, 1]\n\n\n [1] Brussel sprouts                  Carrots                         \n [3] Cauliflower                      Corn                            \n [5] Cornbread                        Fruit salad                     \n [7] Green beans/green bean casserole Macaroni and cheese             \n [9] Mashed potatoes                  Rolls/biscuits                  \n[11] Squash                           Vegetable salad                 \n[13] Yams/sweet potato casserole      Other (please specify)          \n[15] Asian vinagrette salad          \n15 Levels: Asian vinagrette salad Brussel sprouts ... Yams/sweet potato casserole\n\nveggie <- c(1:4, 7, 11, 12, 13, 14)\n\nsideNum <- sideNum %>%\n  gather(side, num) %>%\n  filter(side != \"Other (please specify)\") %>%\n  mutate(isVeggie = rep(0, nrow(.)))\n## other is same as asian vinagrette salad\n\nsideNum$isVeggie[veggie] <- rep(1, length(veggie))\n\n\n\n\n\nggplot(data = sideNum, aes(x = reorder(side, -num), y = num, fill = as.factor(isVeggie))) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  ylab(\"number\") +\n  xlab(\"side\") +\n  scale_fill_manual(values = c(\"yellow\", \"green\"), name = \"is veggie?\")\n\n\n\n# https://stackoverflow.com/questions/25664007/reorder-bars-in-geom-bar-ggplot2\n\n\n\nWho can compete with potatoes and carbs?\nAnd the most controversial topic: the kids’ table\nYes, I’m a little salty for being at the kids’ table too many times.\n\nHere the question answered was “What’s the age cutoff at your”kids’ table\" at Thanksgiving?\". It appears that there is a non-neglible number of people who have a 21+ rule for the kids’ table.\n\n\nlevels(tg$kids_table_age)\n\n\n [1] \"10 or younger\" \"11\"            \"12\"            \"13\"           \n [5] \"14\"            \"15\"            \"16\"            \"17\"           \n [9] \"18\"            \"19\"            \"20\"            \"21 or older\"  \n\ntg$kids_table_age <- as.numeric(str_replace_all(as.character(tg$kids_table_age), \"or .*\", \"\"))\n\nggplot(tg, aes(kids_table_age)) +\n  geom_histogram(binwidth = 1, col = \"black\", fill = \"white\") +\n  annotate(\"text\", x = 21, y = 200, label = \"Why?!\", col = \"red\", size = 5)\n\n\n\n\nHappy Holidays!\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-thanksgiving-dinner/thanksgiving-dinner_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-06-05T15:12:21-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-wind-farms/",
    "title": "US Wind Farm Locations",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-11-12",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, this code was revealed to be deprecated. I am using eval = F to preserve the post, but code will not run as is. I will try to update at some point (or if you are reading this and now what to do to fix it, let me know).\nTardy as usual…\n\n\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(geofacet)\nlibrary(RColorBrewer)\nlibrary(statebins)\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-11-06\")\nwind= read.csv(\"us_wind.csv\")\n\n\n\nDeal with the missing data: I used na_if for the first time here.\n\n\nwind=wind %>% mutate(faa_ors=na_if(faa_ors,\"missing\"),faa_asn=na_if(faa_asn,\"missing\"),usgs_pr_id=na_if(usgs_pr_id,-9999), t_state=na_if(t_state,\"missing\"),t_county=na_if(t_county,\"missing\"),t_fips=na_if(t_fips,\"missing\"),p_name=na_if(p_name,\"missing\"),p_year=na_if(p_year,-9999), p_tnum=na_if(p_tnum,-9999), p_cap=na_if(p_cap,-9999), t_manu=na_if(t_manu,\"missing\"), t_model=na_if(t_model,\"missing\"), t_cap=na_if(t_cap,-9999),t_hh=na_if(t_hh,-9999), t_rd=na_if(t_rd,-9999), t_rsa=na_if(t_rsa,-9999),t_ttlh=na_if(t_ttlh,-9999),t_img_date=na_if(t_img_date,\"missing\"), t_img_srce=na_if(t_img_srce,\"missing\"))\n\n\n\nWhen windmills became operational by state over time:\n\n\ntoP=wind %>% group_by(t_state,p_year) %>% summarise(count=n())\nggplot(toP,aes(p_year,count))+geom_point()+geom_line()+facet_geo(~t_state) \n\n\n\nIt’s too hard to see what is going on because of big windmill states like CA. Let’s try letting the y-axes be different per state.\n\n\n\nggplot(toP,aes(p_year,count))+geom_point()+geom_line()+facet_geo(~t_state,scales=\"free_y\")\n\n\n\nIt’s still hard to see what is going on here. Let’s simplify the question.\ntotal number of windmills per state\nUsing the version of statebins on CRAN…\n\n\ntoP2=toP %>% group_by(t_state) %>% summarise(total=sum(count))\ntoP2=as.data.frame(toP2)\ntoP2$t_state=as.character(toP2$t_state)\n\nlibrary(statebins)\nstatebins_continuous(toP2,state_col=\"t_state\", value_col=\"total\",brewer_pal=\"YlOrRd\",legend_title=\"total wind turbines\")+guides(fill=guide_colorbar(label.theme=element_text(angle=45,size=10,hjust=1))) \n\n\n\nThe nice thing about this being built in ggplot is that it was easy to add a guides statement to rotate the legend labels. I originally took the source code and modified it (in the call to scale_fill_gradientn) to create a new function that rotates the legend text. Then I found this that I used below and realized I could just add on to the output of statebins_continuous instead of creating a whole new function.\nSkewness makes it hard to see what is really going on. How about a transform?\n\n\ntoP2$logTotal=log(toP2$total)\nstatebins_continuous(toP2,state_col=\"t_state\", value_col=\"logTotal\",brewer_pal=\"YlOrRd\",legend_title=\"log(total wind turbines)\")+guides(fill=guide_colorbar(label.theme=element_text(angle=45,size=10,hjust=1))) \n\n\n\nWhat year did each state gain the most windmills?\n\n\ntoP3=toP %>% group_by(t_state) %>% summarise(maxYear=p_year[which.max(p_year)])\ntoP3=as.data.frame(toP3)\ntoP3$t_state=as.character(toP3$t_state)\n\n\n\nWe can’t do the following because we are only allowed up to 9 different categories.\n\n\nstatebins(toP3,state_col=\"t_state\", value_col=\"maxYear\",brewer_pal=\"YlOrRd\",legend_title=\"year w/ most new \\n wind turbines\",breaks=length(unique(toP3$maxYear))) \n\n\n\nWe can use the manual version, but first we have to fix a bug in statebins_manual where it assumes color_col=\"color\" even though it allows us to pass in something else.\n\n\ncolors=c(brewer.pal(9,\"YlOrRd\"),\"black\")\n\nkey=cbind.data.frame(year=sort(unique(toP3$maxYear)),colz=colors)\n\ntoP3=merge(toP3,key,by.x=\"maxYear\",by.y=\"year\")\ntoP3$colz=as.character(toP3$colz)\n\n\n\nNote: we can’t call the variable for color col because in the statebins_manual function there is a merge to the state/abbreviation key that has a column named col. This results in a dataframe with col.x and col.y as columns, and then when we look to grab color from col, we no longer have a column with that name.\n\n\n## get some source files for helper functions\nsetwd(\"~/Desktop/statebins/R\")\nsource(\"aaa.R\")\nsource(\"util.R\")\n\nstatebins_manualFix=function(state_data, state_col=\"state\", color_col=\"color\",\n         text_color=\"black\", font_size=3,\n         state_border_col=\"white\", labels=NULL,\n         legend_title=\"Legend\", legend_position=\"top\",\n         plot_title=\"\", title_position=\"bottom\") {\n  \n  if (!title_position %in% c(\"\", \"top\", \"bottom\")) {\n    stop(\"'title_position' must be either blank, 'top' or 'bottom'\")\n  }\n  \n  state_data <- data.frame(state_data, stringsAsFactors=FALSE)\n  \n  if (max(nchar(state_data[,state_col])) == 2) {\n    merge.x <- \"abbrev\"\n  } else {\n    merge.x <- \"state\"\n  }\n  \n  state_data <- validate_states(state_data, state_col, merge.x)\n  \n  st.dat <- merge(state_coords, state_data, by.x=merge.x, by.y=state_col, all.y=TRUE)\n  gg <- ggplot(st.dat, aes_string(x=\"col\", y=\"row\", label=\"abbrev\"))\n  gg <- gg + geom_tile(aes_string(fill=color_col)) ## fixed from fill=\"color\"\n  gg <- gg + geom_tile(color=state_border_col, aes_string(fill=color_col), size=2, show_guide=FALSE) ## fixed from fill=\"color\"\n  \n  ## allows for a switch to white if block is black and text color is black\n  if(text_color==\"black\"){\n text_color2=ifelse(st.dat[,color_col]==\"black\",\"white\",\"black\")\n   gg <- gg + geom_text(color=text_color2, size=rep(font_size,nrow(st.dat)))\n\n  }else{\n      gg <- gg + geom_text(color=text_color, size=font_size)\n\n  }\n  gg <- gg + scale_y_reverse()\n  if (is.null(labels)) {\n    gg <- gg + scale_fill_manual(values=unique(st.dat[,color_col]))\n    legend_position = \"none\"\n  } else {\n    gg <- gg + scale_fill_manual(values=unique(state_data[,color_col]), labels=labels, name=legend_title) \n    ## switch to unique values from original data set so it matches the order of labels\n  }\n  gg <- gg + coord_equal()\n  gg <- gg + labs(x=NULL, y=NULL, title=NULL)\n  gg <- gg + theme_bw()\n  gg <- gg + theme(legend.position=legend_position)\n  gg <- gg + theme(panel.border=element_blank())\n  gg <- gg + theme(panel.grid=element_blank())\n  gg <- gg + theme(panel.background=element_blank())\n  gg <- gg + theme(axis.ticks=element_blank())\n  gg <- gg + theme(axis.text=element_blank())\n  \n  if (plot_title != \"\") {\n    \n    if (title_position == \"bottom\") {\n      gg <- arrangeGrob(gg, sub=textGrob(plot_title, gp=gpar(cex=1)))\n    } else {\n      gg <- gg + ggtitle(plot_title)\n    }\n    \n  }\n  \n  return(gg)\n  \n}\n\nstatebins_manualFix(toP3,state_col=\"t_state\",color_col=\"colz\",legend_title = \"year w/ most new \\n wind turbines\",legend_position = \"top\",labels=key$year,text_color=\"grey\",font_size=5)\n\n## black text unless block is black, then white\nstatebins_manualFix(toP3,state_col=\"t_state\",color_col=\"colz\",legend_title = \"year w/ most new \\n wind turbines\",legend_position = \"top\",labels=key$year,text_color=\"black\",font_size=5)\n\n\n\nIf we use the updated version that is on GitHub…\nFirst I tried to install both versions and switch between them using this advice, but it didn’t seem to update the statebins function.\n\n\ndevtools::install_github(\"hrbrmstr/statebins\",lib=\"/Library/Frameworks/R.framework/Versions/3.5/Resources/library/bonus\")\nlibrary(statebins, lib.loc=\"/Library/Frameworks/R.framework/Versions/3.5/Resources/library/bonus\")\n\n\n\nSo I did a hack.\n\n\n\nsetwd(\"~/Desktop/statebins/R\")\nsource(\"statebins.R\")\nsource(\"theme-statebin.R\")\nsource(\"geom-statebins.r\")\n\n\n\n\n\ntoP3$maxYear=as.factor(toP3$maxYear)\n#https://stackoverflow.com/questions/6919025/how-to-assign-colors-to-categorical-variables-in-ggplot2-that-have-stable-mappin\nnames(colors)= levels(toP3$maxYear)\n\ntoP3 %>% statebins(state_col=\"t_state\",\n    value_col = \"maxYear\",\n    dark_label = \"white\", light_label = \"black\", ## this is built in now\n    ggplot2_scale_function = scale_fill_manual,\n    values=colors,name = \"year w/ most new \\n wind turbines\",font_size=5)+ theme_statebins()\n\ntoP2 %>% statebins(state_col=\"t_state\",value_col=\"total\",palette=\"YlOrRd\",name = \"log(total wind turbines)\")+ theme_statebins() + guides(fill=guide_colorbar(label.theme=element_text(angle=45,size=10,hjust=1))) \n\n\n\nMy next challenge is to plot points on the map. Because there are wind turbines in Hawaii and Alaska, typical maps in ggplot are incomplete.\nIf we wanted to create a chloropleth, the fiftystater package makes this easy.\n\n\n#devtools::install_github(\"wmurphyrd/fiftystater\")\n## couldn't get from CRAN with my R version (3.5.1)\n\nlibrary(fiftystater)\n\nbyState <- wind %>% group_by(t_state) %>% summarise(total=n()) %>% mutate(logTotal=log(total))\n\n## Need map_id to be lowercase names\ntoMatch=cbind.data.frame(abb=state.abb,name=state.name)\nbyState2=merge(byState,toMatch,by.x=\"t_state\",by.y=\"abb\",all.x=T,all.y=T)\nbyState2$name=tolower(byState2$name)\n\n ggplot(byState2, aes(map_id = name)) + \n  geom_map(aes(fill = logTotal), map = fifty_states) + \n  expand_limits(x = fifty_states$long, y = fifty_states$lat) +\n  coord_map() +\n  scale_x_continuous(breaks = NULL) + \n  scale_y_continuous(breaks = NULL) +\n  labs(x = \"\", y = \"\") +\n  theme(legend.position = \"bottom\", \n        panel.background = element_blank())+guides(fill=guide_colorbar(label.theme=element_text(angle=45,size=10,hjust=1))) \n\n\n\n```\nHowever, if we want to plot points, the scaling for Alaska and Hawaii distorts the rest of the US.\n\n\nlibrary(maps)\nall_states <- map_data(\"state\")\np <- ggplot()+ geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour=\"black\", fill=\"white\" )\n\np+ geom_point(data=wind,aes(x=xlong,y=ylat))+coord_map(xlim=c(-180,-60), ylim=c(10,70))\n\n\n\nTo get something that looks like the chloropleth but with points, we need to transform the Alaska and Hawaii data.\n\n\nlibrary(maptools)\n#https://stackoverflow.com/questions/28421353/how-to-add-hawaii-and-alaska-to-spatial-polygons-in-r\ntmp_dl <- tempfile()\ndownload.file(\"http://www2.census.gov/geo/tiger/GENZ2013/cb_2013_us_state_20m.zip\", tmp_dl)\nunzip(tmp_dl, exdir=tempdir())\nlibrary(rgdal)\nST <- readOGR(tempdir(), \"cb_2013_us_state_20m\",verbose=F)\n\n\n#https://stackoverflow.com/questions/11052544/convert-map-data-to-data-frame-using-fortify-ggplot2-for-spatial-objects-in-r\nus_aea <- spTransform(ST, CRS(\"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"))\nus_aea@data$id <- rownames(us_aea@data)\n\n# extract, then rotate, shrink & move alaska (and reset projection)\n# need to use state IDs via # https://www.census.gov/geo/reference/ansi_statetables.html\nalaska <- us_aea[us_aea$STATEFP==\"02\",]\nalaska <- elide(alaska, rotate=-50)\nscaleSave = max(apply(bbox(alaska), 1, diff)) / 2.3\nalaska <- elide(alaska, scale=scaleSave)\nalaska <- elide(alaska, shift=c(-2100000, -2500000))\nproj4string(alaska) <- proj4string(us_aea)\n\n# extract, then rotate & shift hawaii\nhawaii <- us_aea[us_aea$STATEFP==\"15\",]\nhawaii <- elide(hawaii, rotate=-35)\nhawaii <- elide(hawaii, shift=c(5400000, -1400000))\nproj4string(hawaii) <- proj4string(us_aea)\n\n# remove old states and put new ones back in; note the different order\n# we're also removing puerto rico in this example but you can move it\n# between texas and florida via similar methods to the ones we just used\nus_aea <- us_aea[!us_aea$STATEFP %in% c(\"02\", \"15\", \"72\"),]\nus_aea <- rbind(us_aea, alaska, hawaii)\nus50 <- fortify(us_aea, region=\"STUSPS\")\n\nggplot(data=us50) + geom_map(map=us50, aes(x=long, y=lat, map_id=id, group=group), ,fill=\"white\", color=\"dark grey\", size=0.15) \n\n\n\nNow need to transform the data to add to this plot.\n\n\nwindT=wind\ncoordinates(windT) <- ~ xlong + ylat\nproj4string(windT) <-CRS(proj4string(ST)) ## this assumes the original\n\ntrf=spTransform(windT,CRS(proj4string(us_aea)))\n\n\ntrfRest=subset(trf,!(t_state %in% c(\"HI\",\"AK\") ))\nproj4string(trfRest) <- proj4string(trfRest)\nhi=subset(trf,t_state ==\"HI\")\nak=subset(trf,t_state==\"AK\")\n\n## adjust hawaii and alaska\nak <- elide(ak, rotate=-50)\nak <- elide(ak, scale=scaleSave)\nak  <- elide(ak, shift=c(-2100000, -2500000))\nproj4string(ak) <- proj4string(us_aea)\n\n# extract, then rotate & shift hawaii\nhi <- elide(hi, rotate=-35)\nhi <- elide(hi, shift=c(5400000, -1400000))\nproj4string(hi) <- proj4string(us_aea)\n\ntransformed <- rbind(trfRest, ak, hi)\n\nwindT <- as.data.frame(transformed)\n  \nggplot(data=us50) + \n  geom_map(map=us50, aes(x=long, y=lat, map_id=id, group=group), ,fill=\"white\", color=\"dark grey\", size=0.15) +\n  geom_point(data=windT,aes(x=xlong,y=ylat))+ylim(-2.5e6, 1e6)+xlim(-2.5e6,3e6)\n\n\n\nAre the wind turbines supposed to be in the ocean around Alaska, or did something go wrong when we were moving Alaska around?\nCommon sense check: are some wind turbines in the ocean for other states?\n\n\nca=subset(windT,t_state==\"CA\")\n\n## after transform\nggplot(data=us50) + \n  geom_map(map=us50, aes(x=long, y=lat, map_id=id, group=group), ,fill=\"white\", color=\"dark grey\", size=0.15) +\n  geom_point(data=ca,aes(x=xlong,y=ylat))+ylim(-2.5e6, 1e6)+xlim(-2.5e6,3e6)\n\n## before transform\nggplot()+ geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour=\"black\", fill=\"white\" )+ geom_point(data=wind,aes(x=xlong,y=ylat))+coord_map(xlim=c(-180,-60), ylim=c(10,70))\n\n\n\nYup, one off the coast of California in both the original and transformed data.\nBut I’m still not sure if Alaska is right. Base map to the rescue! It looks like Alaska’s wind turbines are definitely on the coast, but not as far away from Alaska as the ggplot version makes it look. Hm….\n\n\n\nlibrary(maps)\nmap(\"world\", c(\"USA\",\"hawaii\"), xlim=c(-180,-65), ylim=c(19,72),interior = FALSE)\npoints(wind$xlong,wind$ylat)\n\n\n\nDid I assume the wrong original projection for the data? Nope!\n\n\n## Original shapefile\n## https://eerscmap.usgs.gov/uswtdb/data/\nsetwd(\"~/Desktop/tidytuesday/data/2018-11-06/uswtdbSHP\")\nog <- readOGR(\".\",\"uswtdb_v1_2_20181001\",verbose=F)\nproj4string(og)\nproj4string(ST) ## matches what I assumed the wind data was\n\n\n\nThis remains a mystery for now. If anyone has any advice, please let me know [@sastoudt].\nEven if this would have worked, there is a lot of processing to just get a nice map of the full US with points on it. Has anyone pre-packaged all of this to be available in ggplot like fiftystater does for chloropleths?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T15:15:28-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-r-package-downloads/",
    "title": "R and R package downloads",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-10-30",
    "categories": [],
    "contents": "\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(cranlogs)\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-10-30\")\nrd <- read.csv(\"r_downloads_year.csv\")\nrd$date <- as.Date(as.character(rd$date))\n\n\n\nFirst download of each R version\n\n\n\nrd %>%\n  drop_na(os) %>%\n  group_by(version, os) %>%\n  summarise(first = min(date)) %>%\n  ggplot(., aes(fct_rev(version), as.Date(first), col = os)) +\n  geom_point() +\n  coord_flip() +\n  xlab(\"date of first download\") +\n  ylab(\"R version\")\n\n\n\n\n\nTake-Aways\nWindows often lags.\nI suspect “devel” and “latest” are relative to the current version since they appear early on.\nTidyverse and its components\n\n\ntidyverse <- cran_downloads(package = \"tidyverse\", from = min(rd$date), to = max(rd$date))\nggpl <- cran_downloads(package = \"ggplot2\", from = min(rd$date), to = max(rd$date))\ndp <- cran_downloads(package = \"dplyr\", from = min(rd$date), to = max(rd$date))\ntid <- cran_downloads(package = \"tidyr\", from = min(rd$date), to = max(rd$date))\nre <- cran_downloads(package = \"readr\", from = min(rd$date), to = max(rd$date))\npr <- cran_downloads(package = \"purrr\", from = min(rd$date), to = max(rd$date))\ntib <- cran_downloads(package = \"tibble\", from = min(rd$date), to = max(rd$date))\nst <- cran_downloads(package = \"stringr\", from = min(rd$date), to = max(rd$date))\nfc <- cran_downloads(package = \"forcats\", from = min(rd$date), to = max(rd$date))\n\n\n\n\nallTy <- rbind.data.frame(tidyverse, ggpl, dp, tid, re, pr, tib, st, fc)\n\n## add a horizontal line representing mean, help us pick out different colors\norderV <- allTy %>%\n  group_by(package) %>%\n  summarise(mV = mean(count)) %>%\n  arrange(mV)\n\n\n\n\n\n\nggplot(allTy, aes(date, count, col = package)) +\n  geom_point(size = 2, alpha = .75) +\n  geom_hline(data = orderV, aes(yintercept = mV, col = package), lwd = 2, lty = c(rep(1, 7), 2, 1)) +\n  geom_vline(aes(xintercept = as.Date(\"2017-12-25\")))\n\n\n\n\n\nNote: stringr and tibble are on top of one another.\nTake-Aways\nThe average number of downloads of tidyverse is less than the average number of downloads for each of its components. This makes sense because we can either download all packages at once via tidyverse or download the components separately.\nggplot2 has the highest average number of downloads.\nstringr and tibble have similar average number of downloads.\nThere is some evidence of a bit of work life balance as there is a noticeable drop off in downloads on Christmas.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-r-package-downloads/r-package-downloads_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-06-05T15:16:42-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-fast-food-calories/",
    "title": "Fast Food Calories",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-09-04",
    "categories": [],
    "contents": "\nData\nArticle\nData Source\n\n\nrequire(ggplot2)\nrequire(dplyr)\nrequire(gridExtra)\nrequire(tidyr)\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-09-04\")\n\nff <- read.csv(\"fastfood_calories.csv\", stringsAsFactors = F)\n\nff <- ff[, -c(1, ncol(ff))] ## remove salad (all the same) and X\n\n\n\nI wanted a simple way to group items across restaurants, so I’m going to follow this example and use hierarchical clustering via the Levenshtein Distance (typically used for string distances).\n\n\nd <- adist(ff$item)\nrownames(d) <- ff$Item\nhc <- hclust(as.dist(d))\ndf <- data.frame(ff, cut = cutree(hc, k = 10)) ## 10 is a totally arbitrary choice\n\n\n\nLet’s see what groups we end up with.\n\n\nunlist(lapply(split(df, df$cut), nrow))\n\n\n  1   2   3   4   5   6   7   8   9  10 \n 57 368  39   5  15  12   4   4   2   9 \n\nlapply(split(df[, c(\"cut\", \"item\", \"restaurant\")], df$cut), head, 15)\n\n\n$`1`\n    cut                                 item  restaurant\n1     1     Artisan Grilled Chicken Sandwich   Mcdonalds\n58    1    Chargrilled Chicken Club Sandwich Chick Fil-A\n59    1         Chargrilled Chicken Sandwich Chick Fil-A\n73    1      4 Piece Grilled Chicken Nuggets Chick Fil-A\n74    1      6 Piece Grilled Chicken Nuggets Chick Fil-A\n75    1      8 piece Grilled Chicken Nuggets Chick Fil-A\n76    1     12 Piece Grilled Chicken Nuggets Chick Fil-A\n77    1   Spicy Grilled Chicken Sub Sandwich Chick Fil-A\n78    1 Regular Grilled Chicken Sub Sandwich Chick Fil-A\n79    1        Smokehouse BBQ Bacon Sandwich Chick Fil-A\n82    1        Chargrilled Chicken Cool Wrap Chick Fil-A\n112   1 3 Piece Crispy Chicken Tender Dinner       Sonic\n113   1 5 Piece Crispy Chicken Tender Dinner       Sonic\n117   1          Small Jumbo Popcorn Chicken       Sonic\n118   1          Large Jumbo Popcorn Chicken       Sonic\n\n$`2`\n   cut                                item restaurant\n2    2      Single Bacon Smokehouse Burger  Mcdonalds\n3    2      Double Bacon Smokehouse Burger  Mcdonalds\n6    2                             Big Mac  Mcdonalds\n7    2                        Cheeseburger  Mcdonalds\n8    2            Classic Chicken Sandwich  Mcdonalds\n9    2                 Double Cheeseburger  Mcdonalds\n10   2 Double Quarter Pounder® with Cheese  Mcdonalds\n11   2                       Filet-O-Fish®  Mcdonalds\n12   2         Garlic White Cheddar Burger  Mcdonalds\n15   2                           Hamburger  Mcdonalds\n16   2                        Lobster Roll  Mcdonalds\n17   2     Maple Bacon Dijon 1/4 lb Burger  Mcdonalds\n20   2                           McChicken  Mcdonalds\n21   2                            McDouble  Mcdonalds\n22   2                               McRib  Mcdonalds\n\n$`3`\n   cut                                              item restaurant\n4    3         Grilled Bacon Smokehouse Chicken Sandwich  Mcdonalds\n5    3          Crispy Bacon Smokehouse Chicken Sandwich  Mcdonalds\n13   3     Grilled Garlic White Cheddar Chicken Sandwich  Mcdonalds\n14   3      Crispy Garlic White Cheddar Chicken Sandwich  Mcdonalds\n18   3        Grilled Maple Bacon Dijon Chicken Sandwich  Mcdonalds\n19   3         Crispy Maple Bacon Dijon Chicken Sandwich  Mcdonalds\n24   3           Grilled Pico Guacamole Chicken Sandwich  Mcdonalds\n25   3            Crispy Pico Guacamole Chicken Sandwich  Mcdonalds\n26   3 Premium Buttermilk Crispy Chicken Deluxe Sandwich  Mcdonalds\n27   3            Premium Crispy Chicken Deluxe Sandwich  Mcdonalds\n30   3       Grilled Signature Sriracha Chicken Sandwich  Mcdonalds\n31   3        Crispy Signature Sriracha Chicken Sandwich  Mcdonalds\n33   3          Grilled Sweet BBQ Bacon Chicken Sandwich  Mcdonalds\n34   3           Crispy Sweet BBQ Bacon Chicken Sandwich  Mcdonalds\n35   3         3 piece Buttermilk Crispy Chicken Tenders  Mcdonalds\n\n$`4`\n    cut                                             item  restaurant\n46    4  4 piece Sweet N' Spicy Honey BBQ Glazed Tenders   Mcdonalds\n47    4  6 piece Sweet N' Spicy Honey BBQ Glazed Tenders   Mcdonalds\n48    4 10 piece Sweet N' Spicy Honey BBQ Glazed Tenders   Mcdonalds\n270   4    4 Piece Chicken Strip Basket w/ Country Gravy Dairy Queen\n271   4    6 Piece Chicken Strip Basket w/ Country Gravy Dairy Queen\n\n$`5`\n    cut                                                item\n49    5                     Premium Asian Salad w/o Chicken\n50    5              Premium Asian Salad w/ Grilled Chicken\n51    5               Premium Asian Salad w/ Crispy Chicken\n52    5               Premium Bacon Ranch Salad w/o Chicken\n53    5        Premium Bacon Ranch Salad w/ Grilled Chicken\n54    5         Premium Bacon Ranch Salad w/ Crispy Chicken\n55    5                 Premium Southwest Salad w/o Chicken\n56    5          Premium Southwest Salad w/ Grilled Chicken\n57    5           Premium Southwest Salad w/ Crispy Chicken\n225   5                Chicken BLT Salad w/ Grilled Chicken\n226   5                 Chicken BLT Salad w/ Crispy Chicken\n227   5             Chicken Caesar Salad w/ Grilled Chicken\n228   5              Chicken Caesar Salad w/ Crispy Chicken\n229   5 Chicken, Apple & Cranberry Salad w/ Grilled Chicken\n230   5  Chicken, Apple & Cranberry Salad w/ Crispy Chicken\n     restaurant\n49    Mcdonalds\n50    Mcdonalds\n51    Mcdonalds\n52    Mcdonalds\n53    Mcdonalds\n54    Mcdonalds\n55    Mcdonalds\n56    Mcdonalds\n57    Mcdonalds\n225 Burger King\n226 Burger King\n227 Burger King\n228 Burger King\n229 Burger King\n230 Burger King\n\n$`6`\n    cut                                           item restaurant\n92    6              Sonic Bacon Cheeseburger (w/mayo)      Sonic\n99    6 Super Sonic Bacon Double Cheeseburger (w/mayo)      Sonic\n100   6     Super Sonic Double Cheeseburger W/ Mustard      Sonic\n101   6     Super Sonic Double Cheeseburger W/ Ketchup      Sonic\n102   6        Super Sonic Double Cheeseburger W/ Mayo      Sonic\n103   6       Super Sonic Jalapeno Double Cheeseburger      Sonic\n435   6       Cool Ranch® Doritos® Double Decker® Taco  Taco Bell\n442   6                Spicy Sweet Double Stacked Taco  Taco Bell\n443   6        Cool Ranch Habanero Double Stacked Taco  Taco Bell\n444   6               Nacho Crunch Double Stacked Taco  Taco Bell\n445   6             Fiery Doritos® Double Decker® Taco  Taco Bell\n449   6      Nacho Cheese Doritos® Double Decker® Taco  Taco Bell\n\n$`7`\n    cut\n223   7\n224   7\n231   7\n232   7\n                                                               item\n223 Bacon Cheddar Ranch Chicken Salad w/ grilled Chicken & Dressing\n224  Bacon Cheddar Ranch Chicken Salad w/ crispy Chicken & Dressing\n231    Garden Grilled Chicken Salad w/ Grilled Chicken, no dressing\n232     Garden Grilled Chicken Salad w/ Crispy Chicken, no dressing\n     restaurant\n223 Burger King\n224 Burger King\n231 Burger King\n232 Burger King\n\n$`8`\n    cut                                                       item\n233   8                            Side Caesar Salad with dressing\n234   8               Side Garden Salad and Avocado Ranch Dressing\n374   8                Buffalo Chicken Salad (with Ranch dressing)\n377   8 Chicken & Bacon Ranch Melt Salad (includes Ranch dressing)\n     restaurant\n233 Burger King\n234 Burger King\n374      Subway\n377      Subway\n\n$`9`\n    cut                                           item restaurant\n362   9       6\" Turkey Italiano Melt (with Provolone)     Subway\n363   9 Footlong Turkey Italiano Melt (with Provolone)     Subway\n\n$`10`\n    cut                                          item restaurant\n436  10               Cool Ranch® Doritos® Locos Taco  Taco Bell\n437  10       Cool Ranch® Doritos® Locos Taco Supreme  Taco Bell\n446  10                     Fiery Doritos® Locos Taco  Taco Bell\n447  10             Fiery Doritos® Locos Taco Supreme  Taco Bell\n450  10             Nacho Cheese Doritos® Locos Tacos  Taco Bell\n451  10     Nacho Cheese Doritos® Locos Tacos Supreme  Taco Bell\n470  10   Doritos® Cheesy Gordita Crunch - Cool Ranch  Taco Bell\n471  10        Doritos® Cheesy Gordita Crunch - Fiery  Taco Bell\n472  10 Doritos® Cheesy Gordita Crunch - Nacho Cheese  Taco Bell\n\n## should be purrr-ing\n\n\n\nFirst things first, the Doritos + Tacobell pairing is in a league of its own.\n\nJust from looking at the top 15:\nGroup 1: chicken\nGroup 2: sandwiches (no chicken) [but chicken items are in here, just not in head]\nGroup 3: chicken sandwiches\nGroup 4: chicken\nGroup 5: chicken salads\nGroup 6: doubles\nGroup 7: chicken salads with dressing\nGroup 8: salad with dressing\nGroup 9: Subway\nGroup 10: Doritos + Tacobell\nWhere are the other Subway items? Where are the Arby’s items hiding?\n\n\nlapply(\n  split(df, df$restaurant),\n  function(x) {\n    x %>%\n      group_by(cut) %>%\n      summarise(count = n()) %>%\n      mutate(prop = count / sum(count))\n  }\n)\n\n\n$Arbys\n# A tibble: 3 x 3\n    cut count   prop\n* <int> <int>  <dbl>\n1     1    16 0.291 \n2     2    37 0.673 \n3     3     2 0.0364\n\n$`Burger King`\n# A tibble: 6 x 3\n    cut count   prop\n* <int> <int>  <dbl>\n1     1     2 0.0286\n2     2    51 0.729 \n3     3     5 0.0714\n4     5     6 0.0857\n5     7     4 0.0571\n6     8     2 0.0286\n\n$`Chick Fil-A`\n# A tibble: 2 x 3\n    cut count  prop\n* <int> <int> <dbl>\n1     1    10 0.370\n2     2    17 0.630\n\n$`Dairy Queen`\n# A tibble: 3 x 3\n    cut count   prop\n* <int> <int>  <dbl>\n1     1     1 0.0238\n2     2    39 0.929 \n3     4     2 0.0476\n\n$Mcdonalds\n# A tibble: 5 x 3\n    cut count   prop\n* <int> <int>  <dbl>\n1     1     1 0.0175\n2     2    24 0.421 \n3     3    20 0.351 \n4     4     3 0.0526\n5     5     9 0.158 \n\n$Sonic\n# A tibble: 4 x 3\n    cut count  prop\n* <int> <int> <dbl>\n1     1     6 0.113\n2     2    29 0.547\n3     3    12 0.226\n4     6     6 0.113\n\n$Subway\n# A tibble: 4 x 3\n    cut count   prop\n* <int> <int>  <dbl>\n1     1    14 0.146 \n2     2    78 0.812 \n3     8     2 0.0208\n4     9     2 0.0208\n\n$`Taco Bell`\n# A tibble: 4 x 3\n    cut count   prop\n* <int> <int>  <dbl>\n1     1     7 0.0609\n2     2    93 0.809 \n3     6     6 0.0522\n4    10     9 0.0783\n\nMost of the Subway and Arby’s items are in the biggest group. Increasing the number of clusters would eventually break this group up further.\nWhat makes some restaraunts more spread out across groups than others? Since the distance metric counts additions, deletions, and substitutions, there is some dependence on the length of the item name.\n\n\nff$len <- nchar(ff$item)\nggplot(ff, aes(len, col = restaurant, group = restaurant)) +\n  geom_density()\n\n\n\n\nHm… nothing is jumping out at me here.\nAre there differences in nutrient levels across clusters? I’m going to color the clusters that I preliminarily marked as “salad” groups to see if they are healthier. Remember, that these groups have all salads, but there are salads in other groups.\n\n\ntoP <- df %>%\n  group_by(cut) %>%\n  summarise_if(is.numeric, mean, na.rm = T) %>%\n  gather(nutrient, value, -cut)\n\ntoP$isSalad <- as.factor(ifelse(toP$cut %in% c(5, 7, 8), 1, 0))\n\nggplot(toP, aes(as.factor(cut), value, col = isSalad)) +\n  geom_col() +\n  facet_wrap(~nutrient, scales = \"free_y\") +\n  xlab(\"cluster\") +\n  ylab(\"mean value\")\n\n\n\n\nThe purely salad groups still have fairly high cholesterol and saturated fat levels. They also have high values of vitamin A and C though, so it’s not all bad.\nWhy are chicken items so spread out among groups? Again, is this due to a difference in the length of item description? We can also see the distribution of distances between an item and all other items by whether or not the item has “chicken” in it to see if that gives us any clues.\n\n\ndim(d) ## distance between each item and every other item\n\n\n[1] 515 515\n\ndim(ff)\n\n\n[1] 515  17\n\nff$med <- apply(d, 1, median)\nff$max <- apply(d, 1, max)\nff$sd <- apply(d, 1, sd)\n\nff$isChicken <- as.factor(ifelse(grepl(\"Chicken\", ff$item), 1, 0))\nff$len <- nchar(ff$item)\n\ng1 <- ggplot(ff, aes(med, col = isChicken, group = isChicken)) +\n  geom_density() +\n  xlab(\"median string distance between other items\")\ng2 <- ggplot(ff, aes(max, col = isChicken, group = isChicken)) +\n  geom_density() +\n  xlab(\"max string distance between other items\")\ng3 <- ggplot(ff, aes(sd, col = isChicken, group = isChicken)) +\n  geom_density() +\n  xlab(\"sd string distance between other items\")\ng4 <- ggplot(ff, aes(len, col = isChicken, group = isChicken)) +\n  geom_density() +\n  xlab(\"length of item name\")\n\ngrid.arrange(g1, g2, g3, g4, ncol = 2)\n\n\n\n\nChicken items do seem to be further in string distance from other items than non-chicken items. They also have slightly longer item name lengths. However, non-chicken items actually have higher maximum distances from other items and a wider range of variability in distances.\nDisclaimer: This is not real text analysis; I’m just using some fancy-ish tools to explore. If anyone has ideas about how to make this more rigorous, I’d be curious to learn more.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-fast-food-calories/fast-food-calories_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-05T15:17:39-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-exercise-usa/",
    "title": "Exercise USA",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-07-17",
    "categories": [],
    "contents": "\nWeek 16\nCDC\nCDC - National Health Statistics Reports|\n\n\nrequire(readxl)\nrequire(dplyr)\nrequire(ggplot2)\nrequire(stringr)\nrequire(tidyr)\nrequire(geofacet)\nrequire(viridis)\n\n\n\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-07-17\")\nexercise <- read_excel(\"week16_exercise.xlsx\", sheet = 1)\n\nexercise <- exercise[, -1] ## remove count\nexercise <- exercise[-1, ] ## remove \"all states\"\"\n\nexerciseT <- exercise %>% gather(type, value, -state)\nexerciseT$value <- as.numeric(exerciseT$value)\n\n\n\nYou Better Work\nI expected that working men and women would have less time to exercise, but it looks like those who work meet the federal guidelines for exercise more across the US.\nSome potential reasons:\nphysical and financial access to gyms for workers\ncaretaking responsibilities for those “non-workers” prevent exercise\njobs that involve physical labor count as exercise? nope, CDC says “leisure-time physical activity”\nseeing coworkers on a daily basis motivates you to exercise\n\n\nggplot(subset(exerciseT, type %in% c(\"men_nonworking\", \"men_working\", \"women_nonworking\", \"women_working\")), aes(x = type, y = value, fill = type)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  facet_geo(~state) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\n\nWhere are the largest/smallest disparities?\nIn honor of the theme of sweating, I mean exercise, I’m switching to the inferno palette.\n\n\nexercise[2:ncol(exercise)] <- apply(exercise[2:ncol(exercise)], 2, function(x) {\n  as.numeric(x)\n})\n\ncounties <- map_data(\"county\")\nstate <- map_data(\"state\")\n\nexercise$state <- tolower(exercise$state)\n\nall_state <- inner_join(state, exercise, by = c(\"region\" = \"state\"))\n\n\n\nWhat’s striking here is that there is only one negative value. Only in New Hampshire do women meet the guidelines more than men. In DC, the difference between genders is largest, while Montana has the smallest difference.\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = men - women), color = \"grey\") +\n  labs(fill = \"men - women\") +\n  scale_fill_viridis(option = \"inferno\") +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\") +\n  ggtitle(\"Battle of the Sexes\")\n\n\n\nexercise$state[which(exercise$men - exercise$women < 0)]\n\n\n[1] \"new hampshire\"\n\nexercise$state[which.max(abs(exercise$men - exercise$women))]\n\n\n[1] \"district of columbia\"\n\nexercise$state[which.min(abs(exercise$men - exercise$women))]\n\n\n[1] \"montana\"\n\nIn South Dakota and Nebraska nonworking men meet the guidelines more than working men. Vermont has the biggest disparity while Iowa has the smallest.\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = men_working - men_nonworking), color = \"grey\") +\n  labs(fill = \"menWork-menNonwork\") +\n  scale_fill_viridis(option = \"inferno\") +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\") +\n  ggtitle(\"Working(?) Men\")\n\n\n\nexercise$state[which.max(abs(exercise$men_working - exercise$men_nonworking))]\n\n\n[1] \"vermont\"\n\nexercise$state[which.min(abs(exercise$men_working - exercise$men_nonworking))]\n\n\n[1] \"iowa\"\n\nIn Idaho and Utah nonworking women meet the guidelines more than working women. Wyoming has the biggest disparity while Oklahoma has the smallest.\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = women_working - women_nonworking), color = \"grey\") +\n  labs(fill = \"womenWork-womenNonwork\") +\n  scale_fill_viridis(option = \"inferno\") +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\") +\n  ggtitle(\"Working(?) Women\")\n\n\n\nexercise$state[which(exercise$women_working - exercise$women_nonworking < 0)]\n\n\n[1] \"idaho\" \"utah\" \n\nexercise$state[which.max(abs(exercise$women_working - exercise$women_nonworking))]\n\n\n[1] \"wyoming\"\n\nexercise$state[which.min(abs(exercise$women_working - exercise$women_nonworking))]\n\n\n[1] \"oklahoma\"\n\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-exercise-usa/exercise-usa_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-06-05T15:20:15-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-craft-beer/",
    "title": "Craft Beer USA",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-07-10",
    "categories": [],
    "contents": "\nWeek 15\nCraft Beer USA\ndata.world\nthrillist.com\n\n\nrequire(readxl)\nrequire(dplyr)\nrequire(ggplot2)\nrequire(stringr)\n\n\n\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-07-10\")\nbeers <- read_excel(\"week15_beers.xlsx\", sheet = 1)\nbrewer <- read_excel(\"week15_beers.xlsx\", sheet = 2)\nbeer <- inner_join(beers, brewer, by = c(\"brewery_id\" = \"id\"))\n\n\n\n\n\nbyState <- beer %>%\n  group_by(state) %>%\n  summarise(numBrewer = length(unique(brewery_id)), count = n(), mabv = mean(abv, na.rm = T))\ncounties <- map_data(\"county\")\nstate <- map_data(\"state\")\n\nstateInfo <- cbind.data.frame(abb = state.abb, name = tolower(state.name))\n\nstate <- inner_join(state, stateInfo, by = c(\"region\" = \"name\"))\n\nall_state <- inner_join(state, byState, by = c(\"abb\" = \"state\"))\n\n\n\nThis palette isn’t very visually appealing, but in the spirit of beer, I’ll use it anyway.\n\n\n# https://www.reddit.com/r/beer/comments/4gd24e/the_hex_colour_palette_of_beer/\nbeerPal <- c(\"#F3F993\", \"#F5F75C\", \"#F6F513\", \"#EAE615\", \"#E0D01B\", \"#D5BC26\", \"#CDAA37\", \"#C1963C\", \"#BE8C3A\", \"#BE823A\", \"#C17A37\", \"#BF7138\", \"#BC6733\", \"#B26033\", \"#A85839\", \"#985336\", \"#8D4C32\", \"#7C452D\", \"#6B3A1E\", \"#5D341A\", \"#4E2A0C\", \"#4A2727\", \"#361F1B\", \"#261716\", \"#231716\", \"#19100F\", \"#16100F\", \"#120D0C\", \"#100B0A\", \"#050B0A\")\n\n\n\nWhere to Bar Crawl?\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = mabv), color = \"grey\") +\n  labs(fill = \"mabv\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\") +\n  ggtitle(\"Where is the beer strongest?\")\n\n\n\n\nA stark (and believable) difference between Nevada and Utah.\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = numBrewer), color = \"grey\") +\n  labs(fill = \"numBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\") +\n  ggtitle(\"Where are the most breweries?\")\n\n\n\n\nColorado maintains it’s reputation.\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = count / numBrewer), color = \"grey\") +\n  labs(fill = \"beerPerBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\") +\n  ggtitle(\"Where is the largest variety?\")\n\n\n\n\nSurprisingly Kansas is where it is at!\n\n\nbeer %>%\n  group_by(city, state) %>%\n  summarise(count = n(), numBrewer = length(unique(brewery_id))) %>%\n  arrange(desc(count))\n\n\n# A tibble: 401 x 4\n# Groups:   city [384]\n   city         state count numBrewer\n   <chr>        <chr> <int>     <int>\n 1 Grand Rapids MI       66         3\n 2 Chicago      IL       55         9\n 3 Portland     OR       52        11\n 4 Indianapolis IN       43         4\n 5 San Diego    CA       42         8\n 6 Boulder      CO       41         9\n 7 Denver       CO       40         8\n 8 Brooklyn     NY       38         4\n 9 Seattle      WA       35         9\n10 Longmont     CO       33         1\n# … with 391 more rows\n\nbeer %>%\n  group_by(city, state) %>%\n  summarise(count = n(), numBrewer = length(unique(brewery_id))) %>%\n  arrange(desc(numBrewer))\n\n\n# A tibble: 401 x 4\n# Groups:   city [384]\n   city          state count numBrewer\n   <chr>         <chr> <int>     <int>\n 1 Portland      OR       52        11\n 2 Boulder       CO       41         9\n 3 Chicago       IL       55         9\n 4 Seattle       WA       35         9\n 5 Austin        TX       25         8\n 6 Denver        CO       40         8\n 7 San Diego     CA       42         8\n 8 Bend          OR       11         6\n 9 Portland      ME       12         6\n10 San Francisco CA       32         5\n# … with 391 more rows\n\nSomebody please tell me about the hidden gem of Grand Rapids. Apparently, it is Beer City, USA.\nStyles\nThere are too many styles, so I pick some major ones and investigate them.\n\n\nstout <- beer[str_detect(beer$style, \"Stout\"), ]\namerican <- beer[str_detect(beer$style, \"American\"), ]\nipa <- beer[str_detect(beer$style, \"IPA\"), ]\n\n\n\nTrue American?\n\n\nbyStateA <- american %>%\n  group_by(state) %>%\n  summarise(numBrewer = length(unique(brewery_id)), count = n(), mabv = mean(abv, na.rm = T))\n\nall_state <- inner_join(state, byStateA, by = c(\"abb\" = \"state\"))\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = numBrewer), color = \"grey\") +\n  labs(fill = \"numBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\")\n\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = count / numBrewer), color = \"grey\") +\n  labs(fill = \"beerPerBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\")\n\n\n\n\nMississippi: American Beer’s Hearland\nRepresenting my namesake\n\n\nbyStateS <- stout %>%\n  group_by(state) %>%\n  summarise(numBrewer = length(unique(brewery_id)), count = n(), mabv = mean(abv, na.rm = T))\n\nall_state <- inner_join(state, byStateS, by = c(\"abb\" = \"state\"))\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = numBrewer), color = \"grey\") +\n  labs(fill = \"numBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\")\n\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = count / numBrewer), color = \"grey\") +\n  labs(fill = \"beerPerBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\")\n\n\n\n\nWhat’s up with some states having no stouts?!\nThe controversial IPA\n\n\nbyStateI <- ipa %>%\n  group_by(state) %>%\n  summarise(numBrewer = length(unique(brewery_id)), count = n(), mabv = mean(abv, na.rm = T))\n\nall_state <- inner_join(state, byStateI, by = c(\"abb\" = \"state\"))\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = numBrewer), color = \"grey\") +\n  labs(fill = \"numBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\")\n\n\n\nggplot(data = state, mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(data = all_state, aes(fill = count / numBrewer), color = \"grey\") +\n  labs(fill = \"beerPerBrewer\") +\n  scale_fill_gradientn(colors = beerPal) +\n  theme_void() +\n  geom_path(data = state, aes(x = long, y = lat, group = group), color = \"black\")\n\n\n\n## what's up with Utah?\nut <- beer[which(beer$state == \"UT\"), ]\nut[str_detect(ut$style, \"IPA\"), ] ## double counting\n\n\n# A tibble: 8 x 12\n  count.x   abv   ibu    id name.x style brewery_id ounces count.y\n    <dbl> <dbl> <dbl> <dbl> <chr>  <chr>      <dbl>  <dbl>   <dbl>\n1    1382 0.04     NA   644 Johnn… Amer…        399     16     400\n2    2254 0.04     42  1925 Trade… Amer…        159     12     160\n3    2255 0.073    83  1723 Hop N… Amer…        159     12     160\n4    2258 0.073    82  1089 Hop N… Amer…        159     12     160\n5    2300 0.09     75  1825 Squat… Amer…        302     12     303\n6    2302 0.06     NA  1823 Wasat… Amer…        302     12     303\n7    2303 0.06     NA  1682 Wasat… Amer…        302     12     303\n8    2305 0.09     75  1680 Squat… Amer…        302     12     303\n# … with 3 more variables: name.y <chr>, city <chr>, state <chr>\n\nWest Virginia and Arkansas are not into IPAs.\nVariation in ABV\nWhich styles have the most variation in alcohol content (of the top 20 most prevalent styles) given their average value?\n\n\nbeer %>%\n  group_by(style) %>%\n  summarise(count = n(), coeffVarabv = mean(abv, na.rm = T) / sd(abv, na.rm = T)) %>%\n  arrange(desc(count)) %>%\n  head(20) %>%\n  arrange(desc(coeffVarabv))\n\n\n# A tibble: 20 x 3\n   style                          count coeffVarabv\n   <chr>                          <int>       <dbl>\n 1 American Double / Imperial IPA   105       12.5 \n 2 KÃ¶lsch                           42       11.4 \n 3 American Amber / Red Lager        29       10.7 \n 4 American Blonde Ale              108       10.0 \n 5 MÃ¤rzen / Oktoberfest             30        9.72\n 6 Hefeweizen                        40        9.44\n 7 American Pale Ale (APA)          245        8.63\n 8 Cider                             37        7.58\n 9 American Pale Wheat Ale           97        7.55\n10 American IPA                     424        7.33\n11 American Stout                    39        7.17\n12 German Pilsener                   36        7.01\n13 American Porter                   68        6.96\n14 American Amber / Red Ale         133        6.62\n15 American Pale Lager               39        6.27\n16 American Brown Ale                70        5.97\n17 American Black Ale                36        5.49\n18 Fruit / Vegetable Beer            49        5.44\n19 Saison / Farmhouse Ale            52        5.41\n20 Witbier                           51        4.79\n\nFancy string matching for another time: match the beer style to the colors listed here.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-craft-beer/craft-beer_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-05T15:25:10-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-bikeshare/",
    "title": "Biketown Bikeshare",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-06-05",
    "categories": [],
    "contents": "\n\n\nrequire(dplyr)\nrequire(readr)\nrequire(lubridate)\nrequire(ggplot2)\n\n\n\nTidy the Raw Data.\nLuckily, each file has the same header, so we can easily stack them.\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018-06-05/PublicTripData\")\n\ntoRead <- list.files()\n\nindivid <- lapply(toRead, read.csv)\n\nfull <- do.call(\"rbind\", individ)\nhead(full, 1)\n\n\n  RouteID PaymentPlan         StartHub StartLatitude StartLongitude\n1 1282087      Casual NE Sandy at 16th      45.52441      -122.6498\n  StartDate StartTime EndHub EndLatitude EndLongitude   EndDate\n1 7/19/2016     10:22           45.53506    -122.6546 7/19/2016\n  EndTime TripType BikeID      BikeName Distance_Miles Duration\n1   10:48            6083 0468 BIKETOWN           1.19  0:25:46\n  RentalAccessPath MultipleRental\n1           keypad          FALSE\n\nrm(individ)\n\n\n\nMake Date and Time Information More Useful\n\n\nfull$StartDate = as.character(full$StartDate)\nfull$StartDate <- parse_date(full$StartDate, format = \"%m/%d/%Y\")\nfull$StartTime = as.character(full$StartTime)\nfull$StartTime <- parse_time(full$StartTime)\nfull$EndTime = as.character(full$EndTime)\nfull$EndTime <- parse_time(full$EndTime)\nfull$durationHr <- as.vector((full$EndTime - full$StartTime) / 60) ## in minutes\nfull$StartHour <- hour(full$StartTime)\n\n\n\nDifference by Payment Plan\nHypothesis: Subscribers will travel further because they will be bike enthusiasts.\n\n\nfull %>%\n  group_by(PaymentPlan) %>%\n  summarise(mDur = mean(durationHr, na.rm = T), mDist = mean(Distance_Miles), missDu = sum(is.na(durationHr)), missD = sum(is.na(Distance_Miles)), maxDur = max(durationHr, na.rm = T))\n\n\n# A tibble: 3 x 6\n  PaymentPlan    mDur mDist missDu missD maxDur\n* <fct>         <dbl> <dbl>  <int> <int>  <dbl>\n1 \"Casual\"      20.2   2.54    138     0   1090\n2 \"Subscriber\"   9.58  1.55    168     0   1071\n3 \"\"           NaN    NA      4088  4088   -Inf\n\nNegative duration? Weird! I just assumed the start and end date would be the same. They are, so we’re just going to throw out the negative duration values.\n\n\ntocheck <- full[which(full$durationHr < 0), ]\ntable(tocheck$StartDate == tocheck$EndDate)\n\n\n\nFALSE \n 5034 \n\nfullR <- full[-which(full$durationHr < 0), ]\n\n\n\nI’m wrong! The casual users travel further on average than the subscribers.\n\n\nfullR %>%\n  group_by(PaymentPlan) %>%\n  summarise(mDur = mean(durationHr, na.rm = T), mDist = mean(Distance_Miles), missDu = sum(is.na(durationHr)), missD = sum(is.na(Distance_Miles)), maxDur = max(durationHr, na.rm = T), sdDur = sd(durationHr, na.rm = T), sdDist = sd(Distance_Miles)) ##\n\n\n# A tibble: 3 x 8\n  PaymentPlan   mDur mDist missDu missD maxDur sdDur sdDist\n* <fct>        <dbl> <dbl>  <int> <int>  <dbl> <dbl>  <dbl>\n1 \"Casual\"      33.7  2.51    138     0   1090  35.6   21.7\n2 \"Subscriber\"  15.8  1.54    168     0   1071  25.2   25.0\n3 \"\"           NaN   NA      4088  4088   -Inf  NA     NA  \n\nSubscribers do not seem to be more “hardcore” in terms of resilience to bad weather. Both groups have peak usage in the warmer months.\n\n\nfullR <- fullR[-which(fullR$PaymentPlan == \"\"), ]\nfullR$month <- month(fullR$StartDate)\n\ntoPlot <- fullR %>%\n  group_by(PaymentPlan, month) %>%\n  summarise(mDur = mean(durationHr, na.rm = T), mDist = mean(Distance_Miles), count = n()) ## this seems surprising\n\nggplot(toPlot, aes(x = month, y = mDur, col = PaymentPlan, group = PaymentPlan)) +\n  geom_point() +\n  geom_line() +\n  ggtitle(\"Seasonal Usage\")\n\n\n\nggplot(toPlot, aes(x = month, y = mDist, col = PaymentPlan, group = PaymentPlan)) +\n  geom_point() +\n  geom_line() +\n  ggtitle(\"Seasonal Usage\")\n\n\n\nggplot(toPlot, aes(x = month, y = count, col = PaymentPlan, group = PaymentPlan)) +\n  geom_point() +\n  geom_line() +\n  ggtitle(\"Seasonal Usage\")\n\n\n\n\nAha! It seems like subscribers are commuters. There are spikes in the morning and evening when people would be traveling to and from work. This makes sense now. Casual users may be biking around to explore while subscribers have a fixed and reasonable distance to work.\n\n\ntoPlot <- fullR %>%\n  group_by(PaymentPlan, StartHour) %>%\n  summarise(mDur = mean(durationHr, na.rm = T), mDist = mean(Distance_Miles), count = n())\n\nggplot(toPlot, aes(x = StartHour, y = mDur, col = PaymentPlan, group = PaymentPlan)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Start Hour\") +\n  ylab(\"Mean Duration (min.)\") +\n  ggtitle(\"Evidence for Commuting Behavior\")\n\n\n\nggplot(toPlot, aes(x = StartHour, y = mDist, col = PaymentPlan, group = PaymentPlan)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Start Hour\") +\n  ylab(\"Mean Distance\") +\n  ggtitle(\"Evidence for Commuting Behavior\")\n\n\n\nggplot(toPlot, aes(x = StartHour, y = count, col = PaymentPlan, group = PaymentPlan)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Start Hour\") +\n  ylab(\"Cont\") +\n  ggtitle(\"Evidence for Commuting Behavior\")\n\n\n\n\nIdeas for Future Exploration\nIf we had user ids we could test my commuter hypothesis to see if the same user was travelling the same distance every morning and evening. (I understand that having user IDs would be a bit creepy given the latitude/longitude information.) I suppose I could try to use the latitude and longitude to figure out “home” and “work”, but that seems tricky. Maybe for another time.\nIt seems like the durations and distances are a bit higher in the morning. Some hypotheses:\nDuration: If there are hills it may be easier to go one way over the other.\nDuration: There could me more traffic in the morning.\nDuration and Distance: Users could stop somewhere on their way home (dinner, bar, gym, etc.) and then have two shorter rides.\nAgain, we would need user ids or to do something clever with the lat/lons if they were precise enough.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-bikeshare/bikeshare_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-06-05T15:29:10-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-comic-books/",
    "title": "Comic Book Characters",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-29",
    "categories": [],
    "contents": "\nData: Comic book characters\nData Source: FiveThirtyEight package\nArticle: FiveThirtyEight.com\n\n\nrequire(dplyr)\nrequire(ggplot2)\nrequire(tidyr)\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-05-29/\")\ncb <- read.csv(\"week9_comic_characters.csv\")\n\n\n\nNames: Boy v. Man, Girl v. Woman\n\n\ncb$isBoy <- unlist(lapply(cb$name, function(x) {\n  grepl(\"boy\\\\>\", x, ignore.case = T)\n})) ## nothing after boy\n\ncb$isGirl <- unlist(lapply(cb$name, function(x) {\n  grepl(\"girl\", x, ignore.case = T)\n})) ##\n\ncb$isMan <- unlist(lapply(cb$name, function(x) {\n  grepl(\"man\\\\>\", x, ignore.case = T)\n})) ## nothing after man\n\ncb$isWoman <- unlist(lapply(cb$name, function(x) {\n  grepl(\"woman\", x, ignore.case = T)\n})) ##\n\ncb$isMan[which(cb$isMan == 1 & cb$isWoman == 1)] <- 0 ## don't want to double count woman\n\nbyYear <- cb %>%\n  group_by(year) %>%\n  summarise(isGirl = sum(isGirl), count = n(), isWoman = sum(isWoman), isBoy = sum(isBoy), isMan = sum(isMan)) %>%\n  mutate(percentG = isGirl / count, percentW = isWoman / count)\n\n\n\nTangent: Just for the record: characters identified as another’s girlfriend exist, but no boyfriends.\n\n\ngf <- cb[which(unlist(lapply(cb$name, function(x) {\n  grepl(\"girlfriend\", x, ignore.case = T)\n})) == T), ]\ngf$name\n\n\n[1] Ruby (Thug's girlfriend) (Earth-616)     \n[2] Annie (Noh-Varr's Girlfriend) (Earth-616)\n[3] Karen (Hijack's girlfriend) (Earth-616)  \n23272 Levels: 'Spinner (Earth-616) ...\n\nbf <- cb[which(unlist(lapply(cb$name, function(x) {\n  grepl(\"boyfriend\", x, ignore.case = T)\n})) == T), ]\nnrow(bf)\n\n\n[1] 0\n\n\n\nsub <- byYear[, c(\"year\", \"isGirl\", \"isWoman\")]\ntoPlotUpdated <- sub %>% gather(name, value, -year)\n\nggplot(toPlotUpdated, aes(x = year, y = value, col = name)) +\n  geom_point() +\n  geom_segment(aes(x = year, y = 0, xend = year, yend = value, col = name)) +\n  ylab(\"number of characters introduced with \\n particular name\") +\n  ggtitle(\"Where my girls at?\") +\n  geom_text(aes(x = 1940.1, y = 1.2, label = \"Wonder\\n Woman\"))\n\n\n\n\n\n\nsub <- byYear[, c(\"year\", \"isBoy\", \"isMan\")]\ntoPlotUpdated <- sub %>% gather(name, value, -year)\n\nggplot(toPlotUpdated, aes(x = year, y = value, col = name)) +\n  geom_point() +\n  geom_segment(aes(x = year, y = 0, xend = year, yend = value, col = name)) +\n  ylab(\"number of characters introduced with \\n particular name\") +\n  ggtitle(\"It's a man's (super) world\") +\n  geom_text(aes(x = 1986, y = 15, label = \"Superman\"))\n\n\n\n\nTake-Away: There are way more “men” than “boys” but “girl” is pervasive even after the Wonder Woman precedent.\nGood v. Bad\n\n\ntoPlot2 <- cb %>%\n  group_by(year) %>%\n  summarise(bad = length(which(align == \"Bad Characters\")), good = length(which(align == \"Good Characters\")))\n\ntoPlot2Update <- gather(toPlot2, align, count, -year)\n\nggplot(toPlot2Update, aes(x = year, y = count, col = align, group = align)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"number of characters introduced\")\n\n\n\nggplot(toPlot2, aes(x = year, y = bad / good)) +\n  geom_point() +\n  geom_hline(yintercept = 1) +\n  geom_text(aes(x = 1943, y = 10, label = \"WWII\")) +\n  ggtitle(\"A hero(ine) can punch above their weight\")\n\n\n\n\nTake-Away: More bad than good characters are introduced over time fairly consistently. Interestingly, there is a peak in bad characters during World War II. Overall, it looks like each hero can handle more than one villain.\nAppearance Variability\n\n\ntoPlot <- cb %>%\n  group_by(year, sex) %>%\n  summarise(count = n(), uniqueEye = length(unique(eye)) / n(), uniqueHair = length(unique(hair)) / n()) %>%\n  filter(!is.na(sex)) %>%\n  filter(sex %in% c(\"Male Characters\", \"Female Characters\"))\n\nggplot(toPlot, aes(x = year, y = uniqueEye, col = sex, group = sex)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"number of unique eye color \\n normalized by number of introductions\")\n\n\n\nggplot(toPlot, aes(x = year, y = uniqueHair, col = sex, group = sex)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"number of unique hair color \\n normalized by number of introductions\")\n\n\n\nggplot(toPlot, aes(x = year, y = count, col = sex, group = sex)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"number of introductions\")\n\n\n\n\nTake-Away: There is (proportionally) more variability in hair and eye color for female characters. There is a decline in variability in hair and eye color over time, but at least part of this is due to the rise in new characters (and a limit on the number of hair/eye colors).\nIdentity by Sex\n\n\nbySex <- cb %>%\n  group_by(sex) %>%\n  summarise(count = n())\n\nbySexID <- cb %>%\n  group_by(sex, id) %>%\n  summarise(count = n()) %>%\n  inner_join(bySex, by = c(\"sex\" = \"sex\")) %>%\n  mutate(percent = count.x / count.y)\n\ntoPlot <- bySexID %>%\n  filter(id %in% c(\"Public Identity\", \"Secret Identity\", \"No Dual Identity\")) %>%\n  filter(!is.na(sex))\n\ntoPlot2 <- as.data.frame(toPlot %>% arrange(sex))\n\nggplot(toPlot2, aes(id, y = percent, fill = sex)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nTake-Away: There doesn’t seem to be any real difference between female and men in terms of identity. I am reluctant to make any claims about the other categories because of their small sample size.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-comic-books/comic-books_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-06-05T15:30:15-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-honey/",
    "title": "US Honey Production",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\nData: US Honey Production\nData Source: USDA\nData Source:Kaggle.com\nArticle: Bee Culture\nFind my cleaning process for going from the three raw data files to my clean version here.\nIt’s a bit like… \n\n[1] \"state\"          \"numColonies\"    \"yieldPerColony\"\n[4] \"production\"     \"stocks\"         \"avgPricePerLb\" \n[7] \"valProd\"        \"year\"          \n\nBy Year\n\n\n\nSupply and Demand\n\n\n\nEfficiency\n\n\n\nTake-Aways\nVariability in number of colonies is increasing.\nPrice is increasing.\nEfficiency (yield/colony) is declining.\nBy State\n\n# A tibble: 6 x 11\n  state numColoniesT productionT avgPrice sdPrice avgYieldPerCol\n  <chr>        <int>       <int>    <dbl>   <dbl>          <dbl>\n1 CA            6100      347535     113.    42.6           55.8\n2 ND            5555      475085     113.    42.7           88.1\n3 SD            3544      266141     114     43.1           75.8\n4 FL            2928      247048     110.    41.7           83.1\n5 MT            2040      156562     115.    43.7           77.3\n6 MN            1934      144360     115.    42.8           74.3\n# … with 5 more variables: sdYieldPerCol <dbl>, mnumColonies <dbl>,\n#   mproduction <dbl>, sdnumColonies <dbl>, sdproduction <dbl>\n# A tibble: 6 x 11\n  state numColoniesT productionT avgPrice sdPrice avgYieldPerCol\n  <chr>        <int>       <int>    <dbl>   <dbl>          <dbl>\n1 ND            5555      475085     113.    42.7           88.1\n2 CA            6100      347535     113.    42.6           55.8\n3 SD            3544      266141     114     43.1           75.8\n4 FL            2928      247048     110.    41.7           83.1\n5 MT            2040      156562     115.    43.7           77.3\n6 MN            1934      144360     115.    42.8           74.3\n# … with 5 more variables: sdYieldPerCol <dbl>, mnumColonies <dbl>,\n#   mproduction <dbl>, sdnumColonies <dbl>, sdproduction <dbl>\n# A tibble: 6 x 11\n  state numColoniesT productionT avgPrice sdPrice avgYieldPerCol\n  <chr>        <int>       <int>    <dbl>   <dbl>          <dbl>\n1 VA              99        3998     232     93.4           40.2\n2 NV             108        4832     222.    76.2           45.2\n3 IL             121        7475     214.    83.8           61.7\n4 KY              66        3445     208.    64.5           52.1\n5 NC             171        8141     207.    67.7           47.8\n6 TN             108        6116     189.    56.1           56.3\n# … with 5 more variables: sdYieldPerCol <dbl>, mnumColonies <dbl>,\n#   mproduction <dbl>, sdnumColonies <dbl>, sdproduction <dbl>\n# A tibble: 6 x 11\n  state numColoniesT productionT avgPrice sdPrice avgYieldPerCol\n  <chr>        <int>       <int>    <dbl>   <dbl>          <dbl>\n1 HI             131       12647     171    103.            98  \n2 LA             565       54410     105.    43.5           95.7\n3 ND            5555      475085     113.    42.7           88.1\n4 MS             252       21853     106.    37.9           87.5\n5 FL            2928      247048     110.    41.7           83.1\n6 WI            1022       81833     127.    43.1           79.4\n# … with 5 more variables: sdYieldPerCol <dbl>, mnumColonies <dbl>,\n#   mproduction <dbl>, sdnumColonies <dbl>, sdproduction <dbl>\n\n\n\n\nTake-Aways\nI don’t know anything about honey/bees, so maybe these are obvious but I’m surprised by:\nthe high price in the mid east coast states.\nthe large number of colonies and production in North Dakota\nthe high efficiency in Louisiana and North Dakota.\nBy State Over Time\nTime for some fancy geofacets!\n\n\n\nTake-Aways\nNumber of Colonies and Production: North Dakota is on the rise; California is on the decline.\nLouisiana’s high efficiency is on the decline while Mississippi’s is on the rise. Florida and Hawaii are also suffering.\nPrice increases are fairly similar across all of the states.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-honey/honey_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-06-05T15:33:11-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-star-wars/",
    "title": "Star Wars Survey",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-14",
    "categories": [],
    "contents": "\nWeek 7 - Star Wars Survey (2014)\nRAW DATAArticleDataSource fivethirtyeight (fivethirtyeight package)\nHow do perceptions of female Star Wars characters differ across age and gender?\n\n\nrequire(data.table)\nrequire(dplyr)\nrequire(ggplot2)\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-05-14\")\nsw <- fread(\"week7_starwars.csv\")\n## read.csv didn't work for me\n\n\n\nBrute force manipulation.\n\n\nrealHeader <- sw[1, ]\nsw <- sw[-1, ]\n\nnames(sw)[c(36, 38)] <- c(\"householdIncome\", \"location\")\nnames(sw)[2] <- \"seenStarWars\"\n\n\n\nLet’s focus on those who have actually seen Star Wars.\n\n\nswYes <- subset(sw, seenStarWars == \"Yes\")\n\n\n\nPadme\n\n\n## complete data only\ntoPlot <- swYes[-which(swYes$Gender == \"\"), c(\"V28\", \"Gender\", \"Age\")]\ntoPlot <- toPlot[-which(toPlot$V28 == \"\"), ]\n\ntoPlot$V28 <- factor(toPlot$V28)\ntoPlot$V28 <- factor(toPlot$V28, levels = levels(toPlot$V28)[c(4, 6, 3, 1, 2, 5)]) ## GROSS!\n\nbyCatGen <- toPlot %>%\n  group_by(V28, Gender) %>%\n  summarise(count = n())\n\nbyGen <- toPlot %>%\n  group_by(Gender) %>%\n  summarise(count = n())\n\ntoPlot <- byCatGen %>%\n  inner_join(byGen, by = c(\"Gender\" = \"Gender\")) %>%\n  mutate(percent = count.x / count.y)\n\n\n\n\n\nggplot(toPlot, aes(V28, y = percent, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\")\n\n\n\n\n\n\n## complete data only\ntoPlot <- swYes[, c(\"V28\", \"Gender\", \"Age\")]\ntoPlot <- swYes[-which(swYes$Gender == \"\"), c(\"V28\", \"Gender\", \"Age\")]\ntoPlot <- toPlot[-which(toPlot$V28 == \"\"), ]\n\n## relevel\ntoPlot$V28 <- factor(toPlot$V28)\nlevels(toPlot$V28)\n\n\n[1] \"Neither favorably nor unfavorably (neutral)\"\n[2] \"Somewhat favorably\"                         \n[3] \"Somewhat unfavorably\"                       \n[4] \"Unfamiliar (N/A)\"                           \n[5] \"Very favorably\"                             \n[6] \"Very unfavorably\"                           \n\ntoPlot$V28 <- factor(toPlot$V28, levels = levels(toPlot$V28)[c(4, 6, 3, 1, 2, 5)]) ## GROSS!\n\n\n\nbyCatGenAge <- toPlot %>%\n  group_by(Gender, Age, V28) %>%\n  summarise(count = n())\n\nbyGenAge <- toPlot %>%\n  group_by(Gender, Age) %>%\n  summarise(count = n())\n\ntoPlot <- byCatGenAge %>%\n  inner_join(byGenAge, by = c(\"Gender\" = \"Gender\", \"Age\" = \"Age\")) %>%\n  mutate(percent = count.x / count.y)\n\n## get combo\ntoPlot$genderAge <- paste(toPlot$Gender, toPlot$Age) ## is there a less hacky way to do this?\ntoPlot$genderAge <- as.factor(toPlot$genderAge)\n\n## relevel\nlevels(toPlot$genderAge)\n\n\n[1] \"Female > 60\"  \"Female 18-29\" \"Female 30-44\" \"Female 45-60\"\n[5] \"Male > 60\"    \"Male 18-29\"   \"Male 30-44\"   \"Male 45-60\"  \n\ntoPlot$genderAge <- factor(toPlot$genderAge, levels = levels(toPlot$genderAge)[c(2, 6, 3, 7, 4, 8, 1, 5)]) ## GROSS!\n\n\n\nThanks for the help!!\nFrom @ibddoctor: https://t.co/193sOToMJB\n\n\nlevels(toPlot$genderAge)\n\n\n[1] \"Female 18-29\" \"Male 18-29\"   \"Female 30-44\" \"Male 30-44\"  \n[5] \"Female 45-60\" \"Male 45-60\"   \"Female > 60\"  \"Male > 60\"   \n\nggplot(toPlot, aes(V28, y = percent, fill = genderAge, order = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\n\n\n## NOPE, but this would be ideal\n\n\n\nFrom @hadleywickham: try arrange()ing\n\n\ntoPlot$Gender <- as.factor(toPlot$Gender)\ntoPlot$Age <- as.factor(toPlot$Age)\n\n\nlevels(toPlot$genderAge) ## fine\n\n\n[1] \"Female 18-29\" \"Male 18-29\"   \"Female 30-44\" \"Male 30-44\"  \n[5] \"Female 45-60\" \"Male 45-60\"   \"Female > 60\"  \"Male > 60\"   \n\nlevels(toPlot$Age) ## need to relevel\n\n\n[1] \"> 60\"  \"18-29\" \"30-44\" \"45-60\"\n\ntoPlot$Age <- factor(toPlot$Age, levels = levels(toPlot$Age)[c(2, 3, 4, 1)]) ## GROSS!\n\n\n# toPlot$genderAge=factor(toPlot$genderAge,levels=levels(toPlot$genderAge)[c(2,6,3,7,4,8,1,5)]) ## GROSS!\n\n\n## if it plots one level of V28 at a time, this would make sense\ntest <- toPlot %>% arrange(Age, V28)\nhead(test)\n\n\n# A tibble: 6 x 7\n# Groups:   Gender, Age [2]\n  Gender Age   V28                 count.x count.y percent genderAge  \n  <fct>  <fct> <fct>                 <int>   <int>   <dbl> <fct>      \n1 Female 18-29 Unfamiliar (N/A)         10      85  0.118  Female 18-…\n2 Male   18-29 Unfamiliar (N/A)          5      93  0.0538 Male 18-29 \n3 Female 18-29 Very unfavorably          3      85  0.0353 Female 18-…\n4 Male   18-29 Very unfavorably          8      93  0.0860 Male 18-29 \n5 Female 18-29 Somewhat unfavorab…       6      85  0.0706 Female 18-…\n6 Male   18-29 Somewhat unfavorab…      12      93  0.129  Male 18-29 \n\n\n\nggplot(test, aes(V28, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\n\n\n## NOPE but closer, now all the females are together\n\n\n\nTHIS IS THE ONE\n\nMembers of the older female generation are not (proportionally) Padme fans.\n\n\ntest <- as.data.frame(toPlot %>% arrange(Age, V28))\nhead(test)\n\n\n  Gender   Age                  V28 count.x count.y    percent\n1 Female 18-29     Unfamiliar (N/A)      10      85 0.11764706\n2   Male 18-29     Unfamiliar (N/A)       5      93 0.05376344\n3 Female 18-29     Very unfavorably       3      85 0.03529412\n4   Male 18-29     Very unfavorably       8      93 0.08602151\n5 Female 18-29 Somewhat unfavorably       6      85 0.07058824\n6   Male 18-29 Somewhat unfavorably      12      93 0.12903226\n     genderAge\n1 Female 18-29\n2   Male 18-29\n3 Female 18-29\n4   Male 18-29\n5 Female 18-29\n6   Male 18-29\n\nggplot(test, aes(V28, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Padme?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\n\n\n## MAGICALLY WORKS\n\n\n\nMystery: Why is as.data.frame needed?\nLeia\nEveryone loves Leia.\n\n\n## complete data only\ntoPlot <- swYes[-which(swYes$Gender == \"\"), c(\"V18\", \"Gender\", \"Age\")]\ntoPlot <- toPlot[-which(toPlot$V18 == \"\"), ]\n\n## relevel\ntoPlot$V18 <- factor(toPlot$V18)\ntoPlot$V18 <- factor(toPlot$V18, levels = levels(toPlot$V18)[c(4, 6, 3, 1, 2, 5)]) ## GROSS!\n\nbyCatGen <- toPlot %>%\n  group_by(V18, Gender) %>%\n  summarise(count = n())\n\nbyGen <- toPlot %>%\n  group_by(Gender) %>%\n  summarise(count = n())\n\ntoPlot <- byCatGen %>%\n  inner_join(byGen, by = c(\"Gender\" = \"Gender\")) %>%\n  mutate(percent = count.x / count.y)\n\n\n\n\n\nggplot(toPlot, aes(V18, y = percent, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Leia?\")\n\n\n\n\n\n\n## complete data only\ntoPlot <- swYes[, c(\"V18\", \"Gender\", \"Age\")]\ntoPlot <- swYes[-which(swYes$Gender == \"\"), c(\"V18\", \"Gender\", \"Age\")]\ntoPlot <- toPlot[-which(toPlot$V18 == \"\"), ]\n\nbyCatGenAge <- toPlot %>%\n  group_by(Gender, Age, V18) %>%\n  summarise(count = n())\n\nbyGenAge <- toPlot %>%\n  group_by(Gender, Age) %>%\n  summarise(count = n())\n\ntoPlot <- byCatGenAge %>%\n  inner_join(byGenAge, by = c(\"Gender\" = \"Gender\", \"Age\" = \"Age\")) %>%\n  mutate(percent = count.x / count.y)\n\n## get combo\ntoPlot$genderAge <- paste(toPlot$Gender, toPlot$Age) ## is there a less hacky way to do this?\ntoPlot$genderAge <- as.factor(toPlot$genderAge)\n\n## relevel\nlevels(toPlot$genderAge)\n\n\n[1] \"Female > 60\"  \"Female 18-29\" \"Female 30-44\" \"Female 45-60\"\n[5] \"Male > 60\"    \"Male 18-29\"   \"Male 30-44\"   \"Male 45-60\"  \n\ntoPlot$genderAge <- factor(toPlot$genderAge, levels = levels(toPlot$genderAge)[c(2, 6, 3, 7, 4, 8, 1, 5)]) ## GROSS!\n# toPlot$genderAge=ordered(toPlot$genderAge,levels=levels(toPlot$genderAge)[c(2,6,3,7,4,8,1,5)]) ## GROSS!\ntoPlot$V18 <- as.factor(toPlot$V18)\ntoPlot$V18 <- factor(toPlot$V18, levels = levels(toPlot$V18)[c(4, 6, 3, 1, 2, 5)]) ## GROSS!\n\n\n\nWhat’s up with the female youths here?!\n\n\ntoPlot$Gender <- as.factor(toPlot$Gender)\ntoPlot$Age <- as.factor(toPlot$Age)\n\n\nlevels(toPlot$genderAge) ## fine\n\n\n[1] \"Female 18-29\" \"Male 18-29\"   \"Female 30-44\" \"Male 30-44\"  \n[5] \"Female 45-60\" \"Male 45-60\"   \"Female > 60\"  \"Male > 60\"   \n\nlevels(toPlot$Age) ## need to relevel\n\n\n[1] \"> 60\"  \"18-29\" \"30-44\" \"45-60\"\n\ntoPlot$Age <- factor(toPlot$Age, levels = levels(toPlot$Age)[c(2, 3, 4, 1)]) ## GROSS!\n\n\n## if it plots one level of V18 at a time, this would make sense\ntest <- as.data.frame(toPlot %>% arrange(Age, V18))\n\n\n\nggplot(test, aes(V18, y = percent, fill = genderAge)) +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"total\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"\") +\n  ggtitle(\"How do you feel about Leia?\") +\n  scale_fill_manual(\"legend\", values = c(\"Female 18-29\" = \"indianred\", \"Male 18-29\" = \"red\", \"Female 30-44\" = \"dodgerblue\", \"Male 30-44\" = \"blue\", \"Female 45-60\" = \"green\", \"Male 45-60\" = \"forestgreen\", \"Female > 60\" = \"grey\", \"Male > 60\" = \"black\"))\n\n\n\n## make colors more informative (light and dark of a color for male and female same age), rearrange levels so easier to compare\n\n\n\nChallenges\nSide by side instead of stacked (geom_bar documentation)\nChange barchart colors.\nRotate axis labels.\nReleveling. In haste, I did not do this well. Read this to see why I’m wrong and how I could do better.\nNormalizing by the number per category. Letting stat=\"count\" was a red herring. It would be nice if there was a way to input values to normalize the fill variable by, but instead I ended up manually calculating the percentages and using stat=\"identity\".\nCorrect and consistent ordering of colors (to match the legend) Thanks for the pointers @ibddoctor and @hadleywickham!\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-star-wars/star-wars_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-05T15:33:42-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-coffee-chains/",
    "title": "Global Coffee Chains",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-05-07",
    "categories": [],
    "contents": "\nFueled By Dunkin\nWeek 6 - Global coffee-chain locations (as of 2017 or 2018)\nRAW DATAArticleDataSource (Starbucks): kaggle.comDataSource (Tim Horton): timhortons.comDataSource (Dunkin Donuts): odditysoftware.com\n\n\nrequire(readxl)\nrequire(dplyr)\nrequire(maps)\nrequire(ggmap)\nrequire(fields)\nrequire(sf)\n\n\n\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-05-07\")\n\ncoffee1 <- read_excel(\"week6_coffee_chains.xlsx\", sheet = 1)\ncoffee2 <- read_excel(\"week6_coffee_chains.xlsx\", sheet = 2)\ncoffee3 <- read_excel(\"week6_coffee_chains.xlsx\", sheet = 3)\n\n## all have different columns\n\nstarbucksUS <- subset(coffee1, Country == \"US\")\ndunkinUS <- subset(coffee3, e_country == \"USA\") ## all in US so not really needed\n\n\n\nHow far do I need to walk/drive to get my fix?\nI’m putting my personal biases into this, so I\"m only looking at Dunkin and Starbucks within the US.\n\n\ngetMeToCoffee <- function(myPoint, modePrefer, coffeePrefer) {\n  if (is.character(myPoint)) {\n    myPoint2 <- geocode(myPoint, output = \"latlon\")\n  }\n\n  if (coffeePrefer == \"dunkin\") {\n    toDunkin <- rdist(as.matrix(myPoint2), dunkinUS[, c(\"loc_LONG_centroid\", \"loc_LAT_centroid\")])\n    closestDunkin <- which.min(toDunkin)\n    tryThis <- paste(dunkinUS$\"e_address\"[closestDunkin], dunkinUS$e_city[closestDunkin], \",\", dunkinUS$e_state[closestDunkin], sep = \" \")\n    test <- mapdist(myPoint, tryThis,\n      mode = modePrefer,\n      output = \"simple\", messaging = FALSE, sensor = FALSE,\n      language = \"en-EN\", override_limit = FALSE\n    )\n    ## does better with character addresses than lat longs\n  } else if (coffeePrefer == \"starbucks\") {\n    toStarbucks <- rdist(myPoint2, starbucksUS[, c(\"Longitude\", \"Latitude\")])\n    closestStarbucks <- which.min(toStarbucks)\n\n    tryThis <- paste(starbucksUS[closestStarbucks, \"Street Address\"], starbucksUS$City[closestStarbucks], \",\", starbucksUS[closestStarbucks, \"State/Province\"], sep = \" \")\n    test <- mapdist(myPoint, tryThis,\n      mode = modePrefer,\n      output = \"simple\", messaging = FALSE, sensor = FALSE,\n      language = \"en-EN\", override_limit = FALSE\n    )\n  }\n  else {\n    test <- NA\n  }\n\n  return(test)\n}\n\n\n\n\n\ngetMeToCoffee(\"Indiana, PA\", \"driving\", \"dunkin\") ## home\ngetMeToCoffee(\"100 Bureau Drive, Gaithersburg MD\", \"driving\", \"dunkin\") ## NIST address\ngetMeToCoffee(\"Smith College, Northampton MA\", \"walking\", \"dunkin\")\ngetMeToCoffee(\"Evans Hall, Berkeley CA\", \"driving\", \"dunkin\")\n\n\n\n\n\n\n\nNote: We actually do have a Dunkin in good-ole Indiana, PA so all is not lost.\n\n\n# map(\"county\",\"pennsylvania\")\n# points(coffee3$loc_LONG_centroid,coffee3$loc_LAT_centroid,col=\"orange\")\n# https://ryanpeek.github.io/2017-11-05-mapping-with-sf-Part-2/\n# https://github.com/tidyverse/ggplot2/issues/2071\n\npaDunkin <- subset(coffee3, e_state == \"PA\")\nPA <- us_states(states = \"pennsylvania\")\nggplot() +\n  geom_sf(data = PA, color = \"black\", fill = NA) +\n  geom_point(data = paDunkin, aes(x = loc_LONG_centroid, y = loc_LAT_centroid)) +\n  theme_void() +\n  theme(panel.grid.major = element_line(colour = \"white\"))\n\n\n\n\nPlenty\n\n\n# map(\"county\",\"maryland\")\n# points(coffee3$loc_LONG_centroid,coffee3$loc_LAT_centroid,col=\"orange\")\n\nmdDunkin <- subset(coffee3, e_state == \"MD\")\nMD <- us_states(states = \"maryland\")\nggplot() +\n  geom_sf(data = MD, color = \"black\", fill = NA) +\n  geom_point(data = mdDunkin, aes(x = loc_LONG_centroid, y = loc_LAT_centroid)) +\n  theme_void() +\n  theme(panel.grid.major = element_line(colour = \"white\"))\n\n\n\n\nPlenty\n\n\n# map(\"county\",\"massachusetts\")\n# points(coffee3$loc_LONG_centroid,coffee3$loc_LAT_centroid,col=\"orange\")\n\nmaDunkin <- subset(coffee3, e_state == \"MA\")\nMA <- us_states(states = \"massachusetts\")\nggplot() +\n  geom_sf(data = MA, color = \"black\", fill = NA) +\n  geom_point(data = maDunkin, aes(x = loc_LONG_centroid, y = loc_LAT_centroid)) +\n  theme_void() +\n  theme(panel.grid.major = element_line(colour = \"white\"))\n\n\n\n\nBook me a plane ticket!\n\n\n# map(\"county\",\"california\")\n# points(coffee3$loc_LONG_centroid,coffee3$loc_LAT_centroid,col=\"orange\")\n\ncaDunkin <- subset(coffee3, e_state == \"CA\")\nCA <- us_states(states = \"california\")\nggplot() +\n  geom_sf(data = CA, color = \"black\", fill = NA) +\n  geom_point(data = caDunkin, aes(x = loc_LONG_centroid, y = loc_LAT_centroid)) +\n  theme_void() +\n  theme(panel.grid.major = element_line(colour = \"white\"))\n\n\n\n\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-coffee-chains/coffee-chains_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-06-05T15:34:43-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-acs-2015/",
    "title": "ACS Census Data (2015)",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-04-30",
    "categories": [],
    "contents": "\nNOTE: When updating my website in June 2021, this code was revealed to be deprecated. I am using eval = F to preserve the post, but code will not run as is. I will try to update at some point (or if you are reading this and now what to do to fix it, let me know).\nSetup\n\n\nrequire(ggplot2)\nrequire(maps)\nrequire(dplyr)\nrequire(plotly)\nrequire(spotifyr)\n\n\n\nWeek 5 - County-level American Community Survey (5-year estimates) 2015\nRAW DATADataSource: census.govKaggle source\nThis week I am taking inspiration from the Tidy Tuesday submissions of @AidoBo and @jakekaupp.\nI’m slightly tweaking @AidoBo’s function to plot continuous variables on a map to help me explore.\n\n\nFor #TidyTuesday I created simple function which allows you to plot any continuous variable in the data on a map #rstats #r4ds pic.twitter.com/6Q1I121VqI\n\n— Aidan Boland (@AidoBo) May 1, 2018\n\nAnd inspired by @jakekaupp’s work showing commute time in terms of number of Despacito listens\n\n\nA blog post catching up on week 4 and week 5 of #TidyTuesday https://t.co/AoXuNI5s0j Code available at https://t.co/kuJdBQG4pn #rstats #r4ds pic.twitter.com/IXjONQ0LXs\n\n— Jake Kaupp (@jakekaupp) May 3, 2018\n\nI wanted to adapt the function from above (thanks @AidoBo) to make a commuting map for any song. We can use the spotifyr package to access the length of a given song.\n\n\ncounties= map_data(\"county\")\nstate=map_data(\"state\")\n\n  county_plot <-function(x){\n  ## adapted from\n  \n  ##https://twitter.com/AidoBo/status/991338257391804416\n  \n  all_county$x<-all_county[,x] ## a different fix for this? something like aes_string?\n  \n  ggplot(data=counties,mapping=aes(x=long,y=lat,group=group))+\n    geom_polygon(data=all_county, aes(fill=x),color=\"grey\")+labs(fill=x)+scale_fill_distiller(palette=\"Spectral\")+theme_void()+\n    geom_path(data=state, aes(x=long,y=lat,group=group),color=\"black\") ## add state boundaries\n  \n}\n\n\n\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-04-30\")\nacs<-read.csv(\"week5_acs2015_county_data.csv\")\n\nhead(acs)\nnames(acs)\n\n## from @AidoBo too\nall_county<-inner_join(counties,acs %>% mutate(County=tolower(County),State=tolower(State)),by=c(\"subregion\"=\"County\",\"region\"=\"State\"))\n\n\n\nGet your own Client ID and Client Secret here.\n\n\nspotify_client_id=\"\" ## put yours here\nspotify_client_secret=\"\" ## put yours here\naccess_token <- get_spotify_access_token(client_id=spotify_client_id,client_secret=spoitfy_client_secret)\n\n\n\n\n\n\n\n\ncounty_commute_plot_tunes <-function(artist,song,access_token){\n  artists <- get_artists(artist,authorization=access_token)\n  albums <- get_albums(artists$artist_uri[1],authorization=access_token)\n  tracks<-get_album_tracks(albums,authorization=access_token)\n  \n\n  track= tracks[which(grepl(song,tracks$track_name)),][1,\"track_uri\"]\n  \n  audio_features <- get_track_audio_features(track,authorization=access_token)\n  \n  songLength=audio_features$duration_ms/1000/60\n  \n  all_county$commuteTune=all_county$MeanCommute/songLength\n  \n  ggplot(data=counties,mapping=aes(x=long,y=lat,group=group))+\n    geom_polygon(data=all_county, aes(fill=commuteTune),color=\"grey\")+labs(fill=paste(\"Song Plays Per \\n Average Commute\"))+scale_fill_distiller(palette=\"Spectral\")+theme_void()+ggtitle(paste(artist,song,sep=\" - \"))+\n    geom_path(data=state, aes(x=long,y=lat,group=group),color=\"black\") ## add state boundaries\n  \n}\n\n\n\nCommuting\nWhat are these hot spots?\n\n\np=county_plot(\"MeanCommute\") \np\n\n\n\nWe can use ggplotly to use hover information to identify counties of interest.\n\n\ncounty_plotly <-function(x){\n  ## adapted from\n  \n  ##https://twitter.com/AidoBo/status/991338257391804416\n  \n  all_county$x<-all_county[,x] ## a different fix for this? something like aes_string?\n  \n  ggplot(data=counties,mapping=aes(x=long,y=lat,group=group))+\n    geom_polygon(data=all_county, aes(fill=x,region=region,subregion=subregion),color=\"grey\")+labs(fill=x)+scale_fill_distiller(palette=\"Spectral\")+theme_void()+\n    geom_path(data=state, aes(x=long,y=lat,group=group),color=\"black\") ## add state boundaries\n  \n}\n\ntest=county_plotly(\"MeanCommute\")\nggplotly(test,tooltip=c(\"region\",\"subregion\"))\n\n\n\nMust be the money?\n\n\ncounty_commute_plot_tunes(\"Nelly\",\"Ride Wit Me\",access_token)\n\n\n\nWhere can commuting make you make more money?\nCaveat: I’m not really answering this question because we don’t have the data at the individual level, but as an exploratory exercise…\n\n\np <- ggplot(acs, aes(x = MeanCommute, y = IncomePerCap, text =paste(County,State,sep=\"-\"))) +\n  geom_point() +xlab(\"mean commute\")+\n  ylab(\"income per cap\")+ggtitle(\"Where Does/Doesn't Commuting Pay Off?\")\np ## static for GitHub\n#ggplotly(p)\n\n\n\nCan we get a rough idea of where it does and doesn’t pay to commute on a map instead of relying on hovering?\n\n\n## which counties have above average income and below average commute time per state (averages within a state)\naveragesByState=group_by(acs,State)%>% summarize(avgMeanCommute=mean(MeanCommute),avgIncomePerCap=mean(IncomePerCap))\n\n\nacsM=merge(acs,averagesByState,by.x=\"State\",by.y=\"State\",all.x=T)\n\nacs$goodCommuteIncomeLevels=rep(0, nrow(acs))\nacs$goodCommuteIncomeLevels[which(acsM$IncomePerCap>acsM$avgIncomePerCap & acsM$MeanCommute < acsM$avgMeanCommute)]=1\nacs$goodCommuteIncomeLevels=as.factor(acs$goodCommuteIncomeLevels)\n\nall_county<-inner_join(counties,acs %>% mutate(County=tolower(County),State=tolower(State)),by=c(\"subregion\"=\"County\",\"region\"=\"State\"))\n\n## need a discrete version of the map\nggplot(data=counties,mapping=aes(x=long,y=lat,group=group))+\n    geom_polygon(data=all_county, aes(fill=goodCommuteIncomeLevels),color=\"grey\")+labs(fill=\"Good Commuter Given Income\")+scale_fill_discrete()+theme_void()+\n    geom_path(data=state, aes(x=long,y=lat,group=group),color=\"black\") ## add state boundaries\n\n\n\nWork At Home\n\n\np=county_plot(\"WorkAtHome\") \np\n\n\n\nAgain we could use ggplotly to identify hot spots?\n\n\ntest=county_plotly(\"WorkAtHome\")\nggplotly(test,tooltip=c(\"region\",\"subregion\"))\n\n\n\nA lot could be going on here, so again I don’t want to read too much into this plot. Since we don’t have income per person we don’t know if those working from home make more or less than those in other jobs within their county. However, there are some interesting patterns here that it would be interesting to look into with data at the individual level.\n\n\nggplot(acs,aes(x=WorkAtHome,y=acs$IncomePerCap))+geom_point() +xlab(\"percentage working from home\")+\n  ylab(\"income per cap\")+ggtitle(\"Does Working From Home Pay?\")\n\n\n\n\n\ncounty_commute_plot_tunes(\"Fifth Harmony\",\"Work from Home\",access_token)\n\n\n\nNote: My county_commute_plot_tunes is not robust to capitalization. There was some trial and error involved.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-05T15:50:50-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-australian-salaries/",
    "title": "Australian Salaries by Gender",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-04-24",
    "categories": [],
    "contents": "\nWeek 4 - Gender differences in Australian Average Taxable Income\nRAW DATAArticleDataSource: data.gov.au\nDisparities in STEM\nTake-aways\nAbout equal number of indivuals in scientist jobs.\nMany more males in engineering jobs.\n(to be fair, should look into proportion of work force)\nRough OLS interpretation: For every dollar a woman makes in science, a man makes $1.52.\nRough OLS interpretation: For every dollar a woman makes in engineering, a man makes $1.26.\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-04-23\")\naus <- read.csv(\"week4_australian_salary.csv\")\n\nrequire(ggplot2)\nrequire(plotly) ## use to hover and see the job names\n\n\n\nLook for STEM jobs.\n\n\naus[grep(\"stat\", aus$occupation), ] ## looking for statistics\n\n\n        X gender_rank                                  occupation\n1131 1131         907 Garage attendant; Service station attendant\n1132 1132         979 Garage attendant; Service station attendant\n1786 1786         170                     Railway station manager\n1787 1787         174                     Railway station manager\n1792 1792         250                  Real estate agency manager\n1793 1793         111                  Real estate agency manager\n1794 1794         305                           Real estate agent\n1795 1795         239                           Real estate agent\n1796 1796         538                Real estate property manager\n1797 1797         210                Real estate property manager\n1994 1994         385                     Stock and station agent\n1995 1995         457                     Stock and station agent\n     gender individuals average_taxable_income\n1131 Female        2434                  31906\n1132   Male        2678                  34126\n1786 Female         196                  74737\n1787   Male        1220                  97952\n1792 Female        2326                  66271\n1793   Male        2437                 110559\n1794 Female        6997                  62056\n1795   Male       10983                  88045\n1796 Female       18088                  49080\n1797   Male        6708                  92500\n1994 Female         108                  57899\n1995   Male        1204                  67675\n\naus[grep(\"math\", aus$occupation), ] ## nope\n\n\n[1] X                      gender_rank           \n[3] occupation             gender                \n[5] individuals            average_taxable_income\n<0 rows> (or 0-length row.names)\n\nscientist <- aus[grep(\"scien\", aus$occupation), ] ## bingo\nengineer <- aus[grep(\"engineer\", aus$occupation), ]\n\n\n\nGet things organized. Not particularly tidy, but bear with me.\n\n\nscientistG <- split(scientist, scientist$gender)\nengineerG <- split(engineer, engineer$gender)\n\nnames(scientistG[[1]]) <- paste(\"F\", names(scientistG[[1]]), sep = \"\")\nnames(scientistG[[2]]) <- paste(\"M\", names(scientistG[[2]]), sep = \"\")\n\nnames(engineerG[[1]]) <- paste(\"F\", names(engineerG[[1]]), sep = \"\")\nnames(engineerG[[2]]) <- paste(\"M\", names(engineerG[[2]]), sep = \"\")\n\nscientistFull <- cbind(scientistG[[1]], scientistG[[2]])\nengineerFull <- cbind(engineerG[[1]], engineerG[[2]])\n\n\n\nLook at number of individuals in each job\nThe line is y=x. If there was gender parity, we would see points lying around this line. You can hover to see the job titles.\n\n\np <- ggplot(scientistFull, aes(x = Findividuals, y = Mindividuals, text = Moccupation)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"number of individuals\") +\n  ylab(\"average taxable income for males ($)\") +\n  ggtitle(\"Science Jobs\")\np ## for static version on github\n\n\n\np <- ggplotly(p)\np\n\n\n\n{\"x\":{\"data\":[{\"x\":[1301,3365,1033,1902,3435,813,1280,8580,93,240],\"y\":[3351,2546,917,3967,4260,440,690,3953,299,1613],\"text\":[\"Findividuals: 1301<br />Mindividuals: 3351<br />Agricultural scientist; Agronomist\",\"Findividuals: 3365<br />Mindividuals: 2546<br />Biologist; Life scientist\",\"Findividuals: 1033<br />Mindividuals:  917<br />Ceramics scientist; Exercise physiologist; Polymer scientist; Sports scientist\",\"Findividuals: 1902<br />Mindividuals: 3967<br />Earth science technician; Soil technician\",\"Findividuals: 3435<br />Mindividuals: 4260<br />Environmental scientist\",\"Findividuals:  813<br />Mindividuals:  440<br />Geographer; Social scientist\",\"Findividuals: 1280<br />Mindividuals:  690<br />Life science technician\",\"Findividuals: 8580<br />Mindividuals: 3953<br />Medical laboratory scientist\",\"Findividuals:   93<br />Mindividuals:  299<br />Soil scientist\",\"Findividuals:  240<br />Mindividuals: 1613<br />Spatial science technician; Surveying or spatial science technician; Surveying technician\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-331.35,9004.35],\"y\":[-331.35,9004.35],\"text\":\"intercept: 0<br />slope: 1\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":48.9497716894977},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Science Jobs\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-331.35,9004.35],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"2500\",\"5000\",\"7500\"],\"tickvals\":[0,2500,5000,7500],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"2500\",\"5000\",\"7500\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"number of individuals\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[100.95,4458.05],\"tickmode\":\"array\",\"ticktext\":[\"1000\",\"2000\",\"3000\",\"4000\"],\"tickvals\":[1000,2000,3000,4000],\"categoryorder\":\"array\",\"categoryarray\":[\"1000\",\"2000\",\"3000\",\"4000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"average taxable income for males ($)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"e391162818e4\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"scatter\"},\"e391227a2c19\":{\"intercept\":{},\"slope\":{}}},\"cur_data\":\"e391162818e4\",\"visdat\":{\"e391162818e4\":[\"function (y) \",\"x\"],\"e391227a2c19\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\n\n\np <- ggplot(engineerFull, aes(x = Findividuals, y = Mindividuals, text = Moccupation)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"number of individuals\") +\n  ylab(\"average taxable income for males ($)\") +\n  ggtitle(\"Engineer Jobs\")\np ## for static version on github\n\n\n\np <- ggplotly(p)\np\n\n\n\n{\"x\":{\"data\":[{\"x\":[182,36,236,258,242,1663,3568,1059,166,773,1456,312,345,47,217,382,1002,282,252,493,1363,735,92,24,202,1378,236,83,554,319,559,4691,525,378,20,179,252],\"y\":[2280,348,1634,1357,884,4751,29314,5785,2135,10687,21488,1330,8451,286,5378,6539,1881,1879,2502,1121,12071,2896,4683,630,1416,28026,3239,5475,6571,2027,5788,28921,4173,3520,632,1746,1564],\"text\":[\"Findividuals:  182<br />Mindividuals:  2280<br />Aeronautical engineer\",\"Findividuals:   36<br />Mindividuals:   348<br />Agricultural engineer\",\"Findividuals:  236<br />Mindividuals:  1634<br />Air safety inspector or investigator; Aircraft navigator; Airways surveyor; Airworthiness surveyor; Aviation safety inspector; Flight engineer\",\"Findividuals:  258<br />Mindividuals:  1357<br />Aircraft draftsperson; Biomedical engineering technician or associate; Chemical engineering technician; Mining draftsperson; Shipbuilding draftsperson\",\"Findividuals:  242<br />Mindividuals:   884<br />Biomedical engineer\",\"Findividuals: 1663<br />Mindividuals:  4751<br />Chemical engineer\",\"Findividuals: 3568<br />Mindividuals: 29314<br />Civil engineer\",\"Findividuals: 1059<br />Mindividuals:  5785<br />Civil engineering draftsperson; Structural draftsperson\",\"Findividuals:  166<br />Mindividuals:  2135<br />Civil engineering technician or associate\",\"Findividuals:  773<br />Mindividuals: 10687<br />Computer network and systems engineer\",\"Findividuals: 1456<br />Mindividuals: 21488<br />Electrical engineer\",\"Findividuals:  312<br />Mindividuals:  1330<br />Electrical engineering draftsperson\",\"Findividuals:  345<br />Mindividuals:  8451<br />Electrical engineering technician; Electrical tester; Electronics tester\",\"Findividuals:   47<br />Mindividuals:   286<br />Electronic engineering draftsperson\",\"Findividuals:  217<br />Mindividuals:  5378<br />Electronic engineering technician\",\"Findividuals:  382<br />Mindividuals:  6539<br />Electronics engineer\",\"Findividuals: 1002<br />Mindividuals:  1881<br />Environmental engineer\",\"Findividuals:  282<br />Mindividuals:  1879<br />Geotechnical engineer\",\"Findividuals:  252<br />Mindividuals:  2502<br />Industrial engineer\",\"Findividuals:  493<br />Mindividuals:  1121<br />IT quality assurance engineer\",\"Findividuals: 1363<br />Mindividuals: 12071<br />IT support engineer\",\"Findividuals:  735<br />Mindividuals:  2896<br />IT systems test engineer\",\"Findividuals:   92<br />Mindividuals:  4683<br />Marine engineer\",\"Findividuals:   24<br />Mindividuals:   630<br />Marine engineer surveyor\",\"Findividuals:  202<br />Mindividuals:  1416<br />Materials engineer\",\"Findividuals: 1378<br />Mindividuals: 28026<br />Mechanical engineer\",\"Findividuals:  236<br />Mindividuals:  3239<br />Mechanical engineering draftsperson\",\"Findividuals:   83<br />Mindividuals:  5475<br />Mechanical engineering technician or associate\",\"Findividuals:  554<br />Mindividuals:  6571<br />Mining engineer\",\"Findividuals:  319<br />Mindividuals:  2027<br />Petroleum engineer\",\"Findividuals:  559<br />Mindividuals:  5788<br />Production or plant engineer\",\"Findividuals: 4691<br />Mindividuals: 28921<br />Software engineer\",\"Findividuals:  525<br />Mindividuals:  4173<br />Structural engineer\",\"Findividuals:  378<br />Mindividuals:  3520<br />Telecommunications engineer\",\"Findividuals:   20<br />Mindividuals:   632<br />Telecommunications field engineer\",\"Findividuals:  179<br />Mindividuals:  1746<br />Telecommunications network engineer\",\"Findividuals:  252<br />Mindividuals:  1564<br />Transport engineer\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-213.55,4924.55],\"y\":[-213.55,4924.55],\"text\":\"intercept: 0<br />slope: 1\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":54.7945205479452},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Engineer Jobs\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-213.55,4924.55],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"1000\",\"2000\",\"3000\",\"4000\"],\"tickvals\":[0,1000,2000,3000,4000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"1000\",\"2000\",\"3000\",\"4000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"number of individuals\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-1165.4,30765.4],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10000\",\"20000\",\"30000\"],\"tickvals\":[0,10000,20000,30000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10000\",\"20000\",\"30000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"average taxable income for males ($)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"e3914fa74c68\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"scatter\"},\"e391708968b2\":{\"intercept\":{},\"slope\":{}}},\"cur_data\":\"e3914fa74c68\",\"visdat\":{\"e3914fa74c68\":[\"function (y) \",\"x\"],\"e391708968b2\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\nLook at salary\nAgain the line is y=x. If there was gender parity, we would see points lying around this line. You can hover to see the job titles.\n\n\np <- ggplot(scientistFull, aes(x = Faverage_taxable_income, y = Maverage_taxable_income, text = Moccupation)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"average taxable income for females ($)\") +\n  ylab(\"average taxable income for males ($)\") +\n  ggtitle(\"Science Jobs\")\np ## for static version on github\n\n\n\n# p <- ggplotly(p) ## to look at job titles\n# p\n\n\n\n\n\np <- ggplot(engineerFull, aes(x = Faverage_taxable_income, y = Maverage_taxable_income, text = Moccupation)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  xlab(\"average taxable income for females ($)\") +\n  ylab(\"average taxable income for males ($)\") +\n  ggtitle(\"Engineer Jobs\")\np ## for static version on github\n\n\n\n# p <- ggplotly(p) ## to look at job titles\n# p\n\n\n\nRough Modeling\n\n\nlm(scientistG[[2]]$Maverage_taxable_income ~ scientistG[[1]]$Faverage_taxable_income)\n\n\n\nCall:\nlm(formula = scientistG[[2]]$Maverage_taxable_income ~ scientistG[[1]]$Faverage_taxable_income)\n\nCoefficients:\n                            (Intercept)  \n                             -14063.862  \nscientistG[[1]]$Faverage_taxable_income  \n                                  1.521  \n\n\n\nlm(engineerG[[2]]$Maverage_taxable_income ~ engineerG[[1]]$Faverage_taxable_income)\n\n\n\nCall:\nlm(formula = engineerG[[2]]$Maverage_taxable_income ~ engineerG[[1]]$Faverage_taxable_income)\n\nCoefficients:\n                           (Intercept)  \n                              6543.508  \nengineerG[[1]]$Faverage_taxable_income  \n                                 1.261  \n\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-australian-salaries/australian-salaries_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-06-05T15:36:41-04:00",
    "input_file": {}
  },
  {
    "path": "TidyTuesday/2021-06-05-global-mortality/",
    "title": "Global Mortality",
    "description": {},
    "author": [
      {
        "name": "Sara Stoudt",
        "url": {}
      }
    ],
    "date": "2018-04-16",
    "categories": [],
    "contents": "\nSetup\n\n\nrequire(readxl)\nrequire(dplyr)\nrequire(ggplot2)\nrequire(gridExtra)\nrequire(tidyr)\nrequire(RColorBrewer)\n\n\n\nWeek 3 - Global causes of mortality\nRAW DATAArticleDatSource: ourworldindata.orgOriginal Graphic\nRead and Clean Data\n\n\nsetwd(\"~/Desktop/tidytuesday/data/2018/2018-04-16\")\ngm <- read_excel(\"global_mortality.xlsx\")\n\ngm.gathered <- gather(gm, cause, percent, -country, -country_code, -year) ## want a single column for cause of death\ngm.gathered$cause <- as.vector(gsub(\" \\\\(\\\\%\\\\)\", \"\", gm.gathered$cause)) ## remove (%) in causes of death\n\n\n\nGet Colors Ready\nI will want the color per cause to be the same across plots.\nThe colors I use are still not perfectly distinguishable. Any suggestions?\n\n\ncolorOrder <- colorRampPalette(c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"))(length(unique(gm.gathered$cause)))\ncolorOrderShuffle <- colorOrder[sample(1:length(colorOrder), length(colorOrder))]\n## don't want causes close in alphabetical order to be near the same color mainly because of prevalence of\n## cancer and cardiovascular diseases\n\n\ncolorMap <- cbind.data.frame(\n  colorOrderShuffle,\n  unique(gm.gathered$cause)\n)\nnames(colorMap) <- c(\"color\", \"cause\")\n\n\n\nPlot Function\nHere is a function to, given a country, plot the causes of death that have the top N highest average percentage and coefficients of variation across the time span.\n\n\nmakePlotTopN <- function(data, Country, N) {\n  dataToUse <- subset(data, country == Country) ## get country of interest\n\n\n  byCause <- group_by(dataToUse, cause) %>% summarise(avgPercent = mean(percent), sdPercent = sd(percent), sdPercentNorm = sd(percent) / avgPercent)\n  ## find average and standard deviation of percentages of causes of death across the time frame\n\n  byCauseM <- byCause %>% arrange(desc(avgPercent))\n  byCauseSD <- byCause %>% arrange(desc(sdPercentNorm))\n\n  toPlotM <- subset(dataToUse, cause %in% byCauseM$cause[1:N]) ## get top N average\n  toPlotSD <- subset(dataToUse, cause %in% byCauseSD$cause[1:N]) ## get top N variability\n\n\n  ## want colors to be the same across plots\n  ## is there an easier way?\n  toPlotM$cause <- as.factor(toPlotM$cause)\n  toPlotSD$cause <- as.factor(toPlotSD$cause)\n\n  toMergeM <- as.data.frame(toPlotM$cause)\n  toMergeSD <- as.data.frame(toPlotSD$cause)\n  names(toMergeM) <- names(toMergeSD) <- \"cause\"\n\n  col1 <- unique(merge(toMergeM, colorMap, by.x = \"cause\", by.y = \"cause\"))\n  col2 <- unique(merge(toMergeSD, colorMap, by.x = \"cause\", by.y = \"cause\"))\n\n  ## plots\n  g1 <- ggplot(toPlotM, aes(x = year, y = percent, color = cause)) +\n    geom_line(size = 2) +\n    ggtitle(paste(Country, \": Top \", N, \" Highest Average Cause of Death\")) +\n    ylab(\"Percentage of Deaths\") +\n    xlab(\"Year\") +\n    scale_colour_manual(values = as.character(col1$color))\n\n  g2 <- ggplot(toPlotSD, aes(x = year, y = percent, color = cause)) +\n    geom_line(size = 2) +\n    ggtitle(paste(Country, \": Top \", N, \" Highest Coeff of Var Cause of Death\")) +\n    ylab(\"Percentage of Deaths\") +\n    xlab(\"Year\") +\n    scale_colour_manual(values = as.character(col2$color))\n\n\n  grid.arrange(g1, g2, ncol = 2)\n}\n\n\n\nFollowing the article to choose sample countries.\n\n\nmakePlotTopN(gm.gathered, \"United States\", 10)\n\n\n\n\n\n\nmakePlotTopN(gm.gathered, \"Germany\", 10)\n\n\n\n\n\n\nmakePlotTopN(gm.gathered, \"Brazil\", 10)\n\n\n\n\n\n\nmakePlotTopN(gm.gathered, \"South Africa\", 10)\n\n\n\n\n\n\nmakePlotTopN(gm.gathered, \"Kenya\", 10)\n\n\n\n\n\n\nmakePlotTopN(gm.gathered, \"Iraq\", 10)\n\n\n\n\nNote: I wondered how @dpseidel had her Tidy Tuesday plots displayed on Github and discovered this.\n\n\n\n",
    "preview": "TidyTuesday/2021-06-05-global-mortality/global-mortality_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-06-05T15:37:39-04:00",
    "input_file": {}
  }
]
