{
  "articles": [
    {
      "path": "blog.html",
      "title": "Blog",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2025-07-02T11:52:23-04:00"
    },
    {
      "path": "coding_projects.html",
      "title": "Coding Projects",
      "description": "Some additional details about Sara's side coding projects",
      "author": [],
      "contents": "\nData-Driven Narratives: Inspiring Eco-writing with an Interactive Data Exploration Applet: a collaboration with creative writers and students to design an interactive data exploration app to inspire creative writing about the environment\nReading for Humanness: Crowdsource data-related themes in science fiction to aid discussion of ethics in the classroom. Reach out if you are interested in contributing!\nCritical Literacy: Supporting Student Reflection on Social Justice in Data-Related Courses: developing reading guides to help students read about the intersection between social justice and data/algorithms\nAnalysis of the Carceral Sequences in Pennsylvania: sequence analysis of moves within the PA Department of Corrections system\nHow do public officials make Land Bank decisions? Artificial Intelligence may seek patterns I helped out with a code review for this investigation of which properties end up in land banks. The project ended up winning the Best Data Journalism award from the All Ohio Excellence in Journalism Awards this year!\nCensus Data Aggregator While at the Los Angeles Times Data Desk, I worked to add functionality to the Census data aggregator, an open source Python library that helps aggregate data from the census while still accounting for margin of error.\nstringr Vignette: I wrote a vignette to help users transition from performing string operations in base R to using the tidyverse stringr package.\nWealth Tax Shiny App: UC Berkeley economists Emmanuel Saez and Gabriel Zucman analyzed Senator Elizabeth Warren’s proposal for a wealth tax, and Fernando Hoces de la Guardia from the Berkeley Initiative for Transparency in the Social Sciences (BITSS) wanted to turn their work into an open policy analysis (OPA). I created the interactive visualization portion of this project.\nTravel Time to Greenspaces: I helped Riona Sheik crunch some numbers to supplement her story on access to public outdoor spaces via public transportation in Los Angeles (check out the plot).\nCabinet of Curiosity: I played a supporting role in Ciera Martinez’s natural history data project. She works with students to make natural history data acessible and visible to data scientists.\nMy Top Songs Spotify Analysis: Materials from R Ladies Lightning Talk on analyzing my top played songs over time\nBART Ridership Visualization: Before and after COVID shelter-in-place\niNaturalist Records in the time of COVID: Comparison of time trends in records and compare with shelter-in-place order timeline by state.\n\n\n\n",
      "last_modified": "2025-07-02T11:52:23-04:00"
    },
    {
      "path": "index.html",
      "title": "Sara A. Stoudt, PhD",
      "author": [],
      "contents": "\n\n          \n          \n          Sara A. Stoudt, PhD\n          \n          \n          Home\n          Blog\n          Research\n          Teaching\n          Writing\n          Coding Projects\n          Tidy Tuesday\n          ☰\n          \n          \n      \n        \n          \n            \n          \n            Sara A. Stoudt, PhD\n          \n          \n            \n          \n          \n            My name is Sara Stoudt, and I use she/her pronouns. I am\n            an Assistant Professor at Bucknell University in the\n            Mathematics Department. I received my PhD in statistics from\n            the University of California, Berkeley where I was also a Berkeley Institute for\n            Data Science Fellow. My research focus is on ecological\n            applications of statistics and statistics communication. At\n            Berkeley I was advised by Will Fithian and Perry de Valpine\n            and taught writing for statistics with Deb Nolan.\n            Previously, I received a B.A. in Mathematics from Smith\n            College with an emphasis on Statistics. Check out Deborah\n            Nolan and my book, “Communicating\n            with Data: The Art of Writing for Data Science” with\n            companion website here,\n            the Data\n            Science by Design “Our Environment” Anthology, and the\n            Data Science\n            by Design “Future of Data Science” Anthology.\n          \n          \n            \n                \n                  \n                    Twitter\n                  \n                \n              \n                            \n                \n                  \n                    GitHub\n                  \n                \n              \n                            \n                \n                  \n                    Google Scholar\n                  \n                \n              \n                            \n                \n                  \n                    CV\n                  \n                \n              \n                            \n                \n                  \n                    Email\n                  \n                \n              \n                          \n        \n      \n    \n\n    \n      \n        \n          \n            \n          \n            Sara A. Stoudt, PhD\n          \n          \n            \n          \n          \n            My name is Sara Stoudt, and I use she/her pronouns. I am\n            an Assistant Professor at Bucknell University in the\n            Mathematics Department. I received my PhD in statistics from\n            the University of California, Berkeley where I was also a Berkeley Institute for\n            Data Science Fellow. My research focus is on ecological\n            applications of statistics and statistics communication. At\n            Berkeley I was advised by Will Fithian and Perry de Valpine\n            and taught writing for statistics with Deb Nolan.\n            Previously, I received a B.A. in Mathematics from Smith\n            College with an emphasis on Statistics. Check out Deborah\n            Nolan and my book, “Communicating\n            with Data: The Art of Writing for Data Science” with\n            companion website here,\n            the Data\n            Science by Design “Our Environment” Anthology, and the\n            Data Science\n            by Design “Future of Data Science” Anthology.\n          \n        \n      \n      \n          \n              \n                                \n                    \n                    Twitter\n                    \n                \n                                \n                    \n                    GitHub\n                    \n                \n                                \n                    \n                    Google Scholar\n                    \n                \n                                \n                    \n                    CV\n                    \n                \n                                \n                    \n                    Email\n                    \n                \n                              \n          \n      \n    \n\n    \n    \n    ",
      "last_modified": "2025-07-02T11:52:25-04:00"
    },
    {
      "path": "research.html",
      "title": "Research",
      "description": "Some additional details about Sara's research",
      "author": [],
      "contents": "\nCurrent Research - Ecology\nThings I’m thinking about…\nconsequences of model mis-specification in species distribution and abundance models\ndata from participatory science efforts and the behavior of participatory scientists themselves\nblends of systematic and opportunistic data\nCurrent Research - Statistics Pedagogy\nThings I’m thinking about…\nteaching writing and data communication more generally in statistics courses\ncreative mediums and alternative research products\ncomputing in classes of all levels\nCurrent Research - Other Applied Work\nAre you a Bucknell faculty member or Lewisburg-area community member interested in working with me? I would love to hear from you!\nResearch with Undergraduate Students\nAre you a Bucknell student interested in working with me? Reach out if you would be interested in future opportunities!\nData-Driven Narratives: Inspiring Eco-writing with an Interactive Data Exploration Applet\nCo-advised with Elinam Agbo\nAdvisees: Caitlyn Hickey and Shaheryar Asghar (Bucknell University)\nFind out more here.\nCritical Literacy: Supporting Student Reflection on Social Justice in Data-Related Courses\nCo-advised with Nathan Ryan\nAdvisees: Marina Anglo, Julieanna Nelson-Saunders, Thao Nguyen (Bucknell University)\nFind out more here.\nAnalysis of the Carceral Sequences in Pennsylvania\nCo-advised with Nathan Ryan\nAdvisees: Marina Anglo, Thao Nguyen (Bucknell University)\nFind out more here.\nA Partial Least Squares Calibration Model for Predicting Biogenic Silica\nCo-advised with Greg de Wet\nAdvisee: Vivienne Maxwell (Smith College)\nGraduate Research\nClarifying Identifiability Debates in Species Distribution Modeling\nAdvisors: Will Fithian and Perry de Valpine\nEcologists commonly make strong parametric assumptions when formulating statistical models. Such assumptions have sparked repeated debates in the literature about statistical identifiability of species distribution and abundance models, among others. At issue is whether the assumption of a particular parametric form serves to impose artificial statistical identifiability that should not be relied upon or instead whether such an assumption is part and parcel of statistical modeling. We borrow from the econometrics literature to introduce a broader view of the identifiability problem than has been taken in ecological debates. In particular we review the concept of nonparametric identifiability and show what can go wrong when we lack this strong form of identifiability, e.g. extreme sensitivity to model misspecification.\nRead more in the Berkeley Science Review.\nOther ideas we are interested in thinking more about are:\nassessing the fit of more complicated joint species distribution models (in terms of community metrics that we care about for inference rather than just prediction metrics) under model mis-specification\naccounting for correlation between species’ occurrence and detection in joint species distribution and abundance models;\ncombining “good” data (that provides nonparametric identifiability) with “bad” data (that only provides parametric identifiability) to help recover robustness to model misspecification in species distribution and abundance models; and\nmaking recommendations for data collection for species distribution and abundance models.\nDS421 Research\nStreamlining Climate Model Accessibility for Integration into Site-Specific Climate Research\nProject for Capstone Project, with Jenna Baughman\nClimate change is tracked and measured on the global scale, making it hard to incorporate for more localized analysis. There are many different climate products that give different predictions for future climate under different scenarios. This data is stored at the global level per time step, so if you are interested in a time series for one location, you would have to download and process data for the whole globe at each time step, taking computational time and space that may be prohibitive. We aim to streamline this process by allowing users to explore different climate products and helping users access the time series in their location of interest more easily. See our work here.\nUS Drought Vulnerability\nProject for Class Taught by Gaston Sanchez, with Daniel Blaustein-Rejto, Ian Bolliger, Hal Gordon, Andrew Hultgren, Yang Ju, and Kate Pennington\nOur goal was to assess variation in vulnerability to drought across counties and census blocks in the continental United States using health, social, and agricultural indicators. See our work here and check out our Shiny app here.\nGeneralized Additive Models (GAMs) for Understanding Chlorophyll in the Bay Delta, Comparison of GAMs and Weighted Regression\nAdvisors: Perry de Valpine, David Senn, Erica Spotswood, Collaborator (comparisons) Marcus Beck\nThis was a Summer ’16 project in collaboration with the San Francisco Estuary Institute. Find more information here. A Shiny application for the GAM/weighted regression portion can be found here.\nNIST Research\nInterpolation of Atmospheric Greenhouse Gas Fluxes, Evaluation of the accuracy, consistency, and stability of measurements of the Planck constant, Shiny Application for Gas Standard Reference Material Analysis\nAdvisors: Antonio Possolo, Collaborators Ana Kerlo (greenhouse gas),Stephan Schlamminger, Jon R Pratt, and Carl J Williams (Planck constant), Christina Liaskos (Shiny app)\nSmall aircraft collect air quality data around cities by recording air samples as they fly horizontally. If they collect data from multiple horizontal “transects” at different altitudes, we can interpolate to understand the value of various greenhouse gases. The goals of this project are to choose which interpolation method gives the most accurate estimates of greenhouse gas flux and decide how many transects (and at what altitudes) we need for a desired level of precision.\nAre Planck constant measurements ready to help redefine the kilogram? We discuss the preconditions for the redefinition here.\nI built a Shiny applications (web applications based in R) to implement the value assignment and uncertainty evaluation for gas mixture standard reference materials This streamlines the analysis process, allowing scientists to harness the power of R (as opposed to something like Excel) to analyze their data without having to learn R themselves.\nImplementations for Easy Use by Scientists: Shiny Applications\nAdvisors: Antonio Possolo and Tom Bartel (EIV), Collaborator (nano) Bryan Jimenez\nI built Shiny applications to implement the Errors in Variables force calibrations calculations and calculations for the size measurement of nanoparticles. I also worked to finesse the optimization procedure for the EIV approach. This is challenging because we often are pushing the boundaries of how many parameters we need to estimate given the number of data points.\nErrors in Variables Modeling in Force Calibrations\nAdvisors: Antonio Possolo and Tom Bartel\nI worked on an alternative method for determining the calibration function for force measuring instruments as well as a Monte Carlo uncertainty evaluation for the calibration function. I implemented these methods and am helping to integrate them into the procedure employed at NIST for force calibration reporting. Force measuring instruments are calibrated by recording the readings of each instrument when “known” masses are applied. I use an errors in variables regression method instead of the ordinary least squares method in order to take into consideration the uncertainty in the forces applied. The Monte Carlo method gives a more accurate representation of the calibration uncertainty throughout the transducer’s force range than a single conservative value given by the current approach. Read our paper here.\nHomogenization of Surface Temperature Records\nAdvisors: Antonio Possolo and Adam Pintar\nI worked on improving and making available a homogenization algorithm developed by NIST statisticians. The process of homogenization finds and adjusts for biases unrelated to the climate in temperature records. I worked to improve the uncertainty quantification of the algorithm and to provide a way for the method to handle missing values. I attended the Statistical and Applied Mathematical Sciences Institute’s workshop on international surface temperatures to work on this homogenization algorithm. I am now working with another attendee to create an R package that includes the homogenization algorithm as well as resources for accessing and formatting portions of interest in the International Surface Temperature Initiative databank for use with the algorithm.\nMeasuring Optical Apertures for Solar Irradiance Monitoring\nAdvisors: Antonio Possolo and Maritoni Litorja\nI worked to improve the preciseness and accuracy of aperture measurements used in solar irradiance monitoring. My job was to check for possible biases in the data collection and analysis as well as test different methods and assess their degree of uncertainty. I implemented and compared various algorithms for fitting circles on both simulated data and data collected from my proposed sampling method experiments. My goal was to see which combination of algorithm and sampling method yielded the least uncertainty. I found a combination of fitting algorithm and sampling method that was more accurate than the pair that scientists at NIST were using. My recommendations are currently being implemented at NIST.\nUndergraduate Research\nGeostatistical Models for the Spatial Variability of the Abundance of Uranium in Soils and Sediments of the Continental United States\nUndergraduate Honors Thesis, Advisor: Ben Baumer\nIn my thesis I compared several different models for the spatial distribution of the mass fraction of uranium in soils and sediments across the continental United States, aiming to identify the model that predicts this quantity with smallest uncertainty. I am explored local regression, generalized additive models, and Gaussian processes to interpolate maps of uranium and used cross validation to pick the model that predicts uranium both accurately and precisely. I made interpolated maps and maps characterizing the uncertainty of the estimates that are compatible with Google Earth so that a user can interact with the data, ‘flying over’ the US or zooming in to a region of interest. Read an extended abstract here.\nModeling Internet Traffic Generation for Telecommunication Applications\nIndustrial Careers in Mathematical Sciences Program (PIC Math)\nWith: Pamela Badian-Pessot, Vera Bao, Erika Earley, Yadira Flores, Liza Maharjan, Blanche Ngo Mahop, Jordan Menter, Van Nguyen, Laura Rosenbauer, Danielle Williams, and Weijia Zhang\nAdvisors: Nessy Tania & Veena Mendiratta (Bell Labs)\nThe goal of this project was to develop a stochastic model that could predict and simulate traffic load on an internet network. The model takes as input number of users and proportion of internet application being used at a given time - namely web surfing, video streaming, online gaming - and outputs simulated traffic data. Using individual user data, we produced models for web surfing, video streaming, and gaming which were combined to form the simulator. The first method fit known theoretical distributions to the data to simulate individual packets; the second used an empirical copula to simulate packets per second. Read our paper here.\nMarch Machine Learning Madness\nWith: Lauren Santana, Advisor: Ben Baumer\nIn pursuit of the perfect March Madness bracket we aimed to determine the most effective model and the most relevant data to predict match-up probabilities. We decided to use an ensemble method of machine learning techniques. The predictions made by a support vector machine, a Naive Bayes classifier, a k nearest neighbors method, a decision tree, random forests, and an artificial neural network were combined using logistic regression to determine final matchup probabilities. These machine learning techniques were trained on historical team and tournament data. We tested the performance of our method in the two stages of Kaggle’s March Machine Learning Madness competition. After the 2014 tournament, we assessed our performance using a generalizable simulation-based technique. Read more here.\nRoadless America: An Activity for Introduction to Sample Proportions and Their Confidence Intervals\nWith: Yue Cao and Dana Udwin\nAdvisor: Nicholas J. Horton\nOur goal was to have an accessible classroom activity that uses random sampling of locations in coordination with Google Maps to determine what proportion of the United States is within one mile of a road and visualize where roadless areas tend to be. The challenge was to implement a sophisticated geographic sampling that could be kept invisible to the user so that students could focus on the big picture ideas. My job involved brainstorming ways to appropriately collect and display this data as well as conveying these complex ideas in a more straightforward way to both students, who were seeing the material for the first time, and to instructors, who were seeing this technology for the first time. We worked on different sets of code and accompanying instructions for different thresholds of experience with the technology. We tested our most simplistic activity on an introductory statistics class at Smith, and used the feedback from the experience to motivate supplementary versions. You can find the classroom activity here.\nFactors Associated With Changes in Academic Performance During the Transition from Elementary to Middle School\nWith: Dana Hsu and Anna Rockower\nAdvisor: Katherine Halvorsen\nWe were asked by a local school district to look at student, teacher, and school variables to try to explain the drop in standardized test scores on the math section between 5th and 6th grade using de-identified data. My job was to clean the data as well as perform univariate and bivariate analysis to see what associations occur with the change in standardized testing score between 5th and 6th grade. I then worked to model the change in scores.\n\n\n\n",
      "last_modified": "2025-07-02T11:52:26-04:00"
    },
    {
      "path": "teaching.html",
      "title": "Teaching",
      "description": "Some additional details about Sara's teaching",
      "author": [],
      "contents": "\nStudents Requesting Letters of Recommendation\nThings I will need from you:\nLogistical information: the schools you are applying to, the instructions for submitting, and the deadlines (please give me at least a 2 week notice)\nInformation about your goals and trajectory so far: what you are interested in related to the program or job and how does the material in the class(es) that you have had me for fit into that vision for your future.\nProject information: If you completed a project in one of my classes, please remind me of your role, the research question and findings.\nFall 2025\nFOUN - Storytelling with Data\nMATH 407 - Statistical Modeling\nRESC Dinner Seminar - Data Tales: Exploring Speculative Thinking as Statistical Thinking\nRead along here.\nSpring 2025\nMATH 216 - Stat I\nMATH 217 - Stat II\nFall 2024\nFOUN - Storytelling with Data\nMATH 217 - Stat II\nFall 2023/Spring 2024 - I am on teaching leave.\nSpring 2023 - Bucknell University\nMATH 216 - Stat I\nMATH 407 - Statistical Design of Scientific Studies\nFall 2022 - Bucknell University\nMATH 217 - Stat II\nFOUN 098-53- Storytelling with Data\nSpring 2022 - Bucknell University\nMATH 217 - Stat II\nMATH 304 - Statistical Inference Theory\nInteractive learnr tutorials and R Markdown templates\nFall 2021 - Bucknell University\nMATH 216 - Stat I\nMATH 304 - Statistical Inference Theory\nSpring 2021 - Smith College\nSDS 220 - Introduction to Probability and Statistics\nInteractive learnr tutorials\nFall 2020 - Smith College\nSDS 220 - Introduction to Probability and Statistics\nSDS/CSC 109 - Communicating with Data\nClass newsletter\nRead more about the physical data visualization unit here.\nNICAR 2020\nIntroduction to statistics and R for journalists\nSpring 2018 - UC Berkeley\nSTAT 198 - Blogging for Data Science, Graduate Student Instructor with Professor Deb Nolan\nSee the blogs here\nFall 2017 - UC Berkeley\nSTAT 157- Communicating with Data: The art of writing for data science, Graduate Student Instructor with Professor Deb Nolan\nFor a snapshot of what we are doing in class, refer to the tweet-log for the class.\n\n\n\n",
      "last_modified": "2025-07-02T11:52:26-04:00"
    },
    {
      "path": "TidyTuesday.html",
      "title": "Tidy Tuesday",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2025-07-02T11:52:26-04:00"
    },
    {
      "path": "writing.html",
      "title": "Writing",
      "description": "Some additional details about Sara's writing for broader audiences",
      "author": [],
      "contents": "\nFootnote Finding: When Correlation and Regression Slopes Tell (Slightly) Different Stories\nIs this p-hacking?\nMastermind Constrained Prose\nSampled Poems Contain Multitudes\nA Summer set.seed() Sestina\nStatistical Concepts and Intersectionality\nThe Origins of Ordinary Least Squares Assumptions: Some Are More Breakable Than Others\nWhy Do We Plot Data Explainer Zine\nCOVID-19 Case Fatality Rate Bias Visual Explainer\nEcology for the Masses Stat Corner\nscripts for 15 episodes of Study Hall: Data Literacy (produced by Arizona State University and the Crash Course team at Complexly\nContributions to The Pudding: analysis of censorship in Kidz Bop lyrics and past and future “Karen”s\nBIDS Best Practices Working Group Write-Ups\nBlogging Beyond: based on Deb Nolan and my experience teaching writing for statistics and data science, a belated response to @KelseyAHE’s request for a blog about student blogs\nGender Issues Roundtable Discussion: A Case Study in Uncomfortable Conversations: with Kellie Ottoboni, Rebecca Barter, and Ryan Giordano\nFixed, Mixed, and Random Effects as part of Rebecca Barter’s Practical Statistics group\nWhat does probability mean anyway?: Statsbites\nFour Hours in Manhattan Beach: Check out the accompanying map here.\nCA Governor Gavin Newsom’s Social Media\nTag Yourself: Logic Magazine article about community science.\nBerkeley Science Review\nCovermesongs: I am a Features writer for the music blog Covermesongs that focuses on, you guessed it, covers.\n\n\n\n",
      "last_modified": "2025-07-02T11:52:27-04:00"
    }
  ],
  "collections": ["posts/posts.json", "TidyTuesday/TidyTuesday.json"]
}
